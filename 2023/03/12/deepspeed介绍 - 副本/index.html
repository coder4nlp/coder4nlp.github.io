<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Noto+Serif+SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.14.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.json","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="并行训练技术近年来，模型规模迅速增加，动辄几B甚至几百B。但是GPU显存大小根本无法支撑训练推理。首先模型参数过多，导致单机内存放不下，即使能放得下，算力也跟不上。同时，硬件算力的增长远远比不上模型增长的速度，单机训练变得不再可行，需要并行化分布式训练加速。比如Megatron-Turing NLG有 530B 的参数，训练需要超过 10T 的内存来存储权重、梯度和状态。  同时，模型是一个有机的">
<meta property="og:type" content="article">
<meta property="og:title" content="deepspeed介绍">
<meta property="og:url" content="http://example.com/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D%20-%20%E5%89%AF%E6%9C%AC/index.html">
<meta property="og:site_name" content="Coder4nlp&#39;s Blog">
<meta property="og:description" content="并行训练技术近年来，模型规模迅速增加，动辄几B甚至几百B。但是GPU显存大小根本无法支撑训练推理。首先模型参数过多，导致单机内存放不下，即使能放得下，算力也跟不上。同时，硬件算力的增长远远比不上模型增长的速度，单机训练变得不再可行，需要并行化分布式训练加速。比如Megatron-Turing NLG有 530B 的参数，训练需要超过 10T 的内存来存储权重、梯度和状态。  同时，模型是一个有机的">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D%20-%20%E5%89%AF%E6%9C%AC/trendofnlpmodelsize.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/24c2ee1e7328450e940702291ef551f2.png">
<meta property="og:image" content="http://example.com/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D%20-%20%E5%89%AF%E6%9C%AC/2.jpg">
<meta property="og:image" content="http://example.com/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D%20-%20%E5%89%AF%E6%9C%AC/1.png">
<meta property="article:published_time" content="2023-03-12T13:56:26.000Z">
<meta property="article:modified_time" content="2023-03-15T11:35:07.158Z">
<meta property="article:author" content="Coder4nlp">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D%20-%20%E5%89%AF%E6%9C%AC/trendofnlpmodelsize.png">


<link rel="canonical" href="http://example.com/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D%20-%20%E5%89%AF%E6%9C%AC/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D%20-%20%E5%89%AF%E6%9C%AC/","path":"2023/03/12/deepspeed介绍 - 副本/","title":"deepspeed介绍"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>deepspeed介绍 | Coder4nlp's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Coder4nlp's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-following"><a href="/following/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>following</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF"><span class="nav-number">1.</span> <span class="nav-text">并行训练技术</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#DeepSpeed%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">1.1.</span> <span class="nav-text">DeepSpeed是什么？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Deepspeed%E5%AE%89%E8%A3%85"><span class="nav-number">1.2.</span> <span class="nav-text">Deepspeed安装</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DeepSpeed%E4%BD%BF%E7%94%A8"><span class="nav-number">1.3.</span> <span class="nav-text">DeepSpeed使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DeepSpeed%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">1.3.1.</span> <span class="nav-text">DeepSpeed初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#batch-size%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">batch_size相关参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">优化器相关参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Scheduler-%E5%8F%82%E6%95%B0"><span class="nav-number">1.3.1.3.</span> <span class="nav-text">Scheduler 参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#FP16-%E9%80%89%E9%A1%B9"><span class="nav-number">1.3.1.4.</span> <span class="nav-text">FP16 选项</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ZeRO-Optimizations-for-FP16-Training"><span class="nav-number">1.3.1.5.</span> <span class="nav-text">ZeRO Optimizations for FP16 Training</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%BF%80%E6%B4%BBCheckpointing"><span class="nav-number">1.3.1.6.</span> <span class="nav-text">激活Checkpointing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%97%A5%E5%BF%97"><span class="nav-number">1.3.1.7.</span> <span class="nav-text">日志</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GLM-10B%E4%BD%BF%E7%94%A8%E7%9A%84%E5%8F%82%E6%95%B0"><span class="nav-number">1.3.1.8.</span> <span class="nav-text">GLM 10B使用的参数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%8F%8A%E4%BC%98%E5%8C%96%E5%99%A8%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">1.3.2.</span> <span class="nav-text">模型及优化器初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DeepSpeed%E8%AE%AD%E7%BB%83"><span class="nav-number">1.3.3.</span> <span class="nav-text">DeepSpeed训练</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ZeRO%E6%A6%82%E8%BF%B0"><span class="nav-number">1.4.</span> <span class="nav-text">ZeRO概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-ZeRO-Stage-2"><span class="nav-number">1.4.1.</span> <span class="nav-text">3.2 ZeRO Stage 2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-ZeRO-Stage-3"><span class="nav-number">1.4.2.</span> <span class="nav-text">3.3 ZeRO Stage 3</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E5%86%85%E5%AD%98%E4%BC%B0%E8%AE%A1"><span class="nav-number">1.4.3.</span> <span class="nav-text">4.2 内存估计</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C"><span class="nav-number">1.5.</span> <span class="nav-text">数据并行</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%EF%BC%9A%E5%B0%86%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E5%88%86%E8%80%8C%E6%B2%BB%E4%B9%8B"><span class="nav-number">1.5.1.</span> <span class="nav-text">数据并行：将训练数据分而治之</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C%E4%B8%8E%E6%B5%81%E6%B0%B4%E7%BA%BF%E5%B9%B6%E8%A1%8C%EF%BC%9A%E5%B0%86%E6%A8%A1%E5%9E%8B-%E7%94%B1%E5%A4%A7%E5%8C%96%E5%B0%8F"><span class="nav-number">1.5.2.</span> <span class="nav-text">模型并行与流水线并行：将模型****由大化小</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">1.6.</span> <span class="nav-text">参考</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Coder4nlp</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D%20-%20%E5%89%AF%E6%9C%AC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Coder4nlp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="deepspeed介绍 | Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          deepspeed介绍
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-12 21:56:26" itemprop="dateCreated datePublished" datetime="2023-03-12T21:56:26+08:00">2023-03-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-15 19:35:07" itemprop="dateModified" datetime="2023-03-15T19:35:07+08:00">2023-03-15</time>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="并行训练技术"><a href="#并行训练技术" class="headerlink" title="并行训练技术"></a>并行训练技术</h1><p>近年来，模型规模迅速增加，动辄几B甚至几百B。但是GPU显存大小根本无法支撑训练推理。首先模型参数过多，导致单机内存放不下，即使能放得下，算力也跟不上。同时，硬件算力的增长远远比不上模型增长的速度，单机训练变得不再可行，需要并行化分布式训练加速。比如<code>Megatron-Turing NLG</code>有 530B 的参数，训练需要超过 10T 的内存来存储权重、梯度和状态。</p>
<p><img src="/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D%20-%20%E5%89%AF%E6%9C%AC/trendofnlpmodelsize.png"></p>
<p>同时，模型是一个有机的整体，简单增加机器数量并不能提升算力，需要有并行策略和通信设计，才能实现高效的并行训练。</p>
<p>Pytorch的分布式并行框架（Distributed Data Parallel，简称DDP），它也是仅仅能够将数据并行，放到各个GPU的模型上进行训练。</p>
<p>也就是说，DDP的应用场景在你的模型大小大于显卡显存大小时，它就很难继续使用了，除非你自己再将模型参数拆散分散到各个GPU上。</p>
<p>今天要给大家介绍的<a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a>，它就能实现这个拆散功能，它通过将模型参数拆散分布到各个GPU上，以实现大型模型的计算，弥补了DDP的缺点，非常方便，这也就意味着我们能用更少的GPU训练更大的模型，而且不受限于显存。</p>
<h2 id="DeepSpeed是什么？"><a href="#DeepSpeed是什么？" class="headerlink" title="DeepSpeed是什么？"></a>DeepSpeed是什么？</h2><p>DeepSpeed是一个开源深度学习训练优化库，其中包含的一个新的显存优化技术—— ZeRO（零冗余优化器），通过扩大规模，提升速度，控制成本，提升可用性，极大地推进了大模型训练能力。DeepSpeed的核心就在于：<strong>GPU显存不够，CPU内存来凑</strong>。DeepSpeed使用的一个核心要义是，<strong>时间开销和显存占用的权衡</strong>。</p>
<ul>
<li><p><strong>用 3D 并行化实现万亿参数模型训练：</strong> DeepSpeed 实现了三种并行方法的灵活组合：ZeRO 支持的数据并行，流水线并行和张量切片模型并行。3D 并行性适应了不同工作负载的需求，以支持具有<strong>万亿</strong>参数的<strong>超大型模型</strong>，同时实现了近乎完美的显存扩展性和吞吐量扩展效率。此外，其提高的通信效率使用户可以在网络带宽有限的常规群集上以 2-7 倍的速度训练有数十亿参数的模型。</p>
</li>
<li><p><strong>ZeRO-Offload 使 GPU 单卡能够训练 10 倍大的模型：</strong> 为了同时利用 CPU 和 GPU 内存来训练大型模型，我们扩展了 ZeRO-2。我们的用户在使用带有<strong>单张英伟达 V100 GPU</strong> 的机器时，可以在不耗尽显存的情况下运行<strong>多达 130 亿个参数的模型</strong>，模型规模扩展至现有方法的10倍，并保持有竞争力的吞吐量。此功能使数十亿参数的模型训练更加大众化，，并为许多深度学习从业人员打开了一扇探索更大更好的模型的窗户。</p>
</li>
</ul>
<h2 id="Deepspeed安装"><a href="#Deepspeed安装" class="headerlink" title="Deepspeed安装"></a>Deepspeed安装</h2><p>开始使用DeepSpeed的最快方法是通过pip，这将安装DeepSpeed的最新版本，它不绑定到特定的PyTorch或CUDA版本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install deepspeed</span><br></pre></td></tr></table></figure>

<ul>
<li>在安装DeepSpeed之前必须先安装PyTorch。</li>
<li>为了获得完整的特性支持，我们建议使用PyTorch&gt;&#x3D; 1.8版本，最好是最新的PyTorch稳定版本。</li>
</ul>
<p>安装后，您可以验证您的安装，并通过DeepSpeed环境报告查看您的机器与哪些扩展&#x2F;操作兼容。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ds_report</span><br></pre></td></tr></table></figure>

<h2 id="DeepSpeed使用"><a href="#DeepSpeed使用" class="headerlink" title="DeepSpeed使用"></a>DeepSpeed使用</h2><p>以GLM使用到的deepspeed配置为例</p>
<h3 id="DeepSpeed初始化"><a href="#DeepSpeed初始化" class="headerlink" title="DeepSpeed初始化"></a>DeepSpeed初始化</h3><p>DeepSpeed 通过输入参数来启动训练，因此需要使用<code>argparse</code>解析参数。完整的参数可以查看<a target="_blank" rel="noopener" href="https://www.deepspeed.ai/docs/config-json/">DeepSpeed文档</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser = deepspeed.add_config_arguments(parser)</span><br></pre></td></tr></table></figure>

<p>pytorch初始化的分布式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.distributed.init_process_group(</span><br><span class="line">           backend=args.distributed_backend,</span><br><span class="line">           world_size=args.world_size, rank=args.rank,</span><br><span class="line">           init_method=init_method)</span><br></pre></td></tr></table></figure>

<p>DeepSpeed初始化分布式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deepspeed.init_distributed(dist_backend=args.distributed_backend)</span><br></pre></td></tr></table></figure>

<p>这里介绍一些常用的参数，</p>
<h4 id="batch-size相关参数"><a href="#batch-size相关参数" class="headerlink" title="batch_size相关参数"></a>batch_size相关参数</h4><p><em><strong>train_batch_size</strong></em>: [integer]（注意:train_batch_size必须等于train_micro_batch_size_per_gpu * gradient_accumulation * gpu个数。为了简单起见，你可以选择只指定三个参数中的两个，最后一个参数将由DeepSpeed自动推断。）</p>
<table>
<thead>
<tr>
<th align="left">Value</th>
<th align="left">Example</th>
</tr>
</thead>
<tbody><tr>
<td align="left">The effective training batch size. This is the amount of data samples that leads to one step of model update. <em><strong>train_batch_size</strong></em> is aggregated by the batch size that a single GPU processes in one forward&#x2F;backward pass (a.k.a., <em><strong>train_micro_batch_size_per_gpu</strong></em>), the gradient accumulation steps (a.k.a., <em><strong>gradient_accumulation_steps</strong></em>), and the number of GPUs. Can be omitted if both <em><strong>train_micro_batch_size_per_gpu</strong></em> and <em><strong>gradient_accumulation_steps</strong></em> are provided.</td>
<td align="left"></td>
</tr>
</tbody></table>
<p><em><strong>train_micro_batch_size_per_gpu</strong></em>: [integer]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Batch size to be processed by one GPU in one step (without gradient accumulation). Can be omitted if both <em><strong>train_batch_size</strong></em> and <em><strong>gradient_accumulation_steps</strong></em> are provided.</td>
<td align="left"><em><strong>train_batch_size</strong></em> value</td>
</tr>
</tbody></table>
<p><em><strong>gradient_accumulation_steps</strong></em>: [integer]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Number of training steps to accumulate gradients before averaging and applying them. This feature is sometimes useful to improve scalability since it results in less frequent communication of gradients between steps. Another impact of this feature is the ability to train with larger batch sizes per GPU. Can be omitted if both <em><strong>train_batch_size</strong></em> and <em><strong>train_micro_batch_size_per_gpu</strong></em> are provided.</td>
<td align="left"></td>
</tr>
</tbody></table>
<h4 id="优化器相关参数"><a href="#优化器相关参数" class="headerlink" title="优化器相关参数"></a>优化器相关参数</h4><p><em><strong>optimizer</strong></em>: [dictionary]</p>
<table>
<thead>
<tr>
<th align="left">Fields</th>
<th align="left">Value</th>
<th align="left">Example</th>
</tr>
</thead>
<tbody><tr>
<td align="left">type</td>
<td align="left">The optimizer name. DeepSpeed natively supports <strong>Adam</strong>, <strong>AdamW</strong>, <strong>OneBitAdam</strong>, <strong>Lamb</strong>, and <strong>OneBitLamb</strong> optimizers (See <a target="_blank" rel="noopener" href="https://deepspeed.readthedocs.io/en/latest/optimizers.html">here</a> for details) and will import other optimizers from <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html">torch</a>.</td>
<td align="left"><code>&quot;Adam&quot;</code></td>
</tr>
<tr>
<td align="left">params</td>
<td align="left">Dictionary of parameters to instantiate optimizer. The parameter names must match the optimizer constructor signature (e.g., for <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Adam">Adam</a>).</td>
<td align="left"><code>&#123;&quot;lr&quot;: 0.001, &quot;eps&quot;: 1e-8&#125;</code></td>
</tr>
</tbody></table>
<h4 id="Scheduler-参数"><a href="#Scheduler-参数" class="headerlink" title="Scheduler 参数"></a>Scheduler 参数</h4><p>在执行model_engine.step()时，DeepSpeed在每个训练步骤中调用调度器的step()方法。</p>
<p>*<strong>scheduler*</strong>: [dictionary]</p>
<table>
<thead>
<tr>
<th align="left">Fields</th>
<th align="left">Value</th>
<th align="left">Example</th>
</tr>
</thead>
<tbody><tr>
<td align="left">type</td>
<td align="left">The scheduler name. See <a target="_blank" rel="noopener" href="https://deepspeed.readthedocs.io/en/latest/schedulers.html">here</a> for list of support schedulers.</td>
<td align="left"><code>&quot;WarmupLR&quot;</code></td>
</tr>
<tr>
<td align="left">params</td>
<td align="left">Dictionary of parameters to instantiate scheduler. The parameter names should match scheduler constructor signature.</td>
<td align="left"><code>&#123;&quot;warmup_min_lr&quot;: 0, &quot;warmup_max_lr&quot;: 0.001&#125;</code></td>
</tr>
</tbody></table>
<h4 id="FP16-选项"><a href="#FP16-选项" class="headerlink" title="FP16 选项"></a>FP16 选项</h4><p><strong>注意</strong>:此模式不能与下面描述的amp模式组合。</p>
<p><em><strong>fp16</strong></em>: [dictionary]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Configuration for using mixed precision&#x2F;FP16 training that leverages <a target="_blank" rel="noopener" href="https://nvidia.github.io/apex/">NVIDIA’s Apex package</a>. An example, including the available dictionary keys is illustrated below. NOTE: this does not use Apex’s AMP mode that allows for more flexibility in mixed precision training modes, this mode is similar to AMP’s O2 mode. Please see AMP support below if you want to use more complex mixed precision modes. If you want to use ZeRO (currently) you must use this mode.</td>
<td align="left">None</td>
</tr>
</tbody></table>
<h4 id="ZeRO-Optimizations-for-FP16-Training"><a href="#ZeRO-Optimizations-for-FP16-Training" class="headerlink" title="ZeRO Optimizations for FP16 Training"></a>ZeRO Optimizations for FP16 Training</h4><p>启用和配置ZeRO内存优化</p>
<p><em><strong>stage</strong></em>: [integer]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">选择ZeRO优化器的不同阶段。阶段0、阶段1、阶段2、阶段3分别为禁用、优化器状态分区、优化器+梯度状态分区、优化器+梯度+参数分区。</td>
<td align="left"><code>0</code></td>
</tr>
</tbody></table>
<p><em><strong>reduce_scatter</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">使用reduce或reduce scatter点代替all reduce to average gradient</td>
<td align="left"><code>true</code></td>
</tr>
</tbody></table>
<p>*<strong>reduce_bucket_size*</strong>: [integer]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Number of elements reduced&#x2F;allreduced at a time. Limits the memory required for the allgather for large model sizes</td>
<td align="left"><code>5e8</code></td>
</tr>
</tbody></table>
<p><em><strong>contiguous_gradients</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Copies the gradients to a contiguous buffer as they are produced. Avoids memory fragmentation during backward pass.</td>
<td align="left"><code>True</code></td>
</tr>
</tbody></table>
<p><em><strong>overlap_comm</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Attempts to overlap the reduction of the gradients with backward computation</td>
<td align="left"><code>false</code></td>
</tr>
</tbody></table>
<p>*<strong>allgather_bucket_size*</strong>: [integer]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Number of elements allgathered at a time. Limits the memory required for the allgather for large model sizes</td>
<td align="left"><code>5e8</code></td>
</tr>
</tbody></table>
<p>*<strong>cpu_offload*</strong>: [boolean]</p>
<p><strong>Deprecated:</strong> <strong>cpu_offload</strong> is deprecated and will be removed in future, please use <code>offload_optimizer</code> instead.</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Enable offloading of optimizer memory and computation to CPU. This frees up GPU memory for larger models or batch sizes. Valid with stage 1 and 2.</td>
<td align="left"><code>False</code></td>
</tr>
</tbody></table>
<h4 id="激活Checkpointing"><a href="#激活Checkpointing" class="headerlink" title="激活Checkpointing"></a>激活Checkpointing</h4><p><em><strong>partition_activations</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Enables partition activation when used with model parallelism</td>
<td align="left"><code>false</code></td>
</tr>
</tbody></table>
<p><em><strong>contiguous_memory_optimization</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Copies partitioned activations so that they are contiguous in memory</td>
<td align="left"><code>false</code></td>
</tr>
</tbody></table>
<h4 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h4><p><em><strong>wall_clock_breakdown</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Enable timing of the latency of forward&#x2F;backward&#x2F;update training phases</td>
<td align="left"><code>false</code></td>
</tr>
</tbody></table>
<h4 id="GLM-10B使用的参数"><a href="#GLM-10B使用的参数" class="headerlink" title="GLM 10B使用的参数"></a>GLM 10B使用的参数</h4><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="punctuation">:</span> <span class="number">4</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;gradient_accumulation_steps&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;steps_per_print&quot;</span><span class="punctuation">:</span> <span class="number">50</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;gradient_clipping&quot;</span><span class="punctuation">:</span> <span class="number">1.0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;zero_optimization&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;stage&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;contiguous_gradients&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;overlap_comm&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;reduce_scatter&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;reduce_bucket_size&quot;</span><span class="punctuation">:</span> <span class="number">5e7</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;allgather_bucket_size&quot;</span><span class="punctuation">:</span> <span class="number">5e7</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;cpu_offload&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;zero_allow_untested_optimizer&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;fp16&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;enabled&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;loss_scale&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;loss_scale_window&quot;</span><span class="punctuation">:</span> <span class="number">1000</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;hysteresis&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;min_loss_scale&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;optimizer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Adam&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;params&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;lr&quot;</span><span class="punctuation">:</span> <span class="number">5e-6</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;betas&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="number">0.9</span><span class="punctuation">,</span></span><br><span class="line">        <span class="number">0.95</span></span><br><span class="line">      <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;eps&quot;</span><span class="punctuation">:</span> <span class="number">1e-8</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;weight_decay&quot;</span><span class="punctuation">:</span> <span class="number">1e-2</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;activation_checkpointing&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;partition_activations&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;contiguous_memory_optimization&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;wall_clock_breakdown&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="模型及优化器初始化"><a href="#模型及优化器初始化" class="headerlink" title="模型及优化器初始化"></a>模型及优化器初始化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">model, optimizer, _, _ = deepspeed.initialize(</span><br><span class="line">    model=model,</span><br><span class="line">    model_parameters=param_groups,</span><br><span class="line">    args=args,</span><br><span class="line">    mpu=mpu,</span><br><span class="line">    dist_init_required=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="DeepSpeed训练"><a href="#DeepSpeed训练" class="headerlink" title="DeepSpeed训练"></a>DeepSpeed训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deepspeed --hostfile $&#123;HOST_FILE_PATH&#125; --master_port $&#123;MASTER_PORT&#125; --num_nodes $&#123;NUM_WORKERS&#125; --num_gpus $&#123;NUM_GPUS_PER_WORKER&#125;<span class="string">&quot; finetune_glm.py</span></span><br></pre></td></tr></table></figure>

<p>以GLM为例，使用自己的数据训练</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bash scripts/ds_finetune_seq2seq.sh \ </span><br><span class="line">   config_tasks/model_blocklm_10B.sh \ </span><br><span class="line">   config_tasks/seq_customization.sh</span><br></pre></td></tr></table></figure>

<h2 id="ZeRO概述"><a href="#ZeRO概述" class="headerlink" title="ZeRO概述"></a>ZeRO概述</h2><p>Zero Redundancy Optimizer (ZeRO)是DeepSpeed的workhorse. 用户可以提供不同的ZeRO config文件，来实现DeepSpeed的不同功能特性。</p>
<p>来看一下<a target="_blank" rel="noopener" href="https://deepspeed.readthedocs.io/en/latest/zero3.html#getting-started">官网教程</a>对ZeRO的描述：</p>
<blockquote>
<p>The Zero Redundancy Optimizer (ZeRO) removes the memory redundancies across data-parallel processes by partitioning the three model states (optimizer states, gradients, and parameters) across data-parallel processes instead of replicating them. By doing this, it boosts memory efficiency compared to classic data-parallelism while retaining its computational granularity and communication efficiency.</p>
</blockquote>
<p>一句话总结： <code>partitioning instead of replicating</code>，<strong>划分而不是复制</strong>。</p>
<p>即，传统的<a target="_blank" rel="noopener" href="https://aitechtogether.com/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">深度学习</a>，模型训练并行，是将模型参数复制多份到多张GPU上，只将数据拆分（如，torch的Dataparallel），这样就会有大量的显存冗余浪费。而ZeRO就是为了消除这种冗余，提高对memory的利用率。注意，这里的“memory”不仅指多张GPU memory，还包括CPU。</p>
<p>而ZeRO的实现方法，就是把参数占用，逻辑上分成三种类型。将这些类型的参数划分：</p>
<ul>
<li><code>optimizer states</code>：即优化器的参数状态。例如，Adam的动量参数。</li>
<li><code>gradients</code>：梯度缓存，对应于optimizer。</li>
<li><code>parameters</code>：模型参数。</li>
</ul>
<p>对应的，DeepSpeed的ZeRO config文件就可以分为如下几类：</p>
<ul>
<li><code>ZeRO Stage 1</code>: 划分optimizer states。优化器参数被划分到多个memory上，每个momoey上的进程只负责更新它自己那部分参数。</li>
<li><code>ZeRO Stage 2</code>: 划分gradient。每个memory，只保留它分配到的optimizer state所对应的梯度。这很合理，因为梯度和optimizer是紧密联系在一起的。只知道梯度，不知道optimizer state，是没有办法优化模型参数的。</li>
<li><code>ZeRO Stage 3</code>: 划分模型参数，或者说，不同的layer. ZeRO-3会在forward和backward的时候，自动将模型参数分配到多个memory。</li>
</ul>
<p>由于ZeRO-1只分配optimizer states(参数量很小)，实际使用的时候，<strong>我们一般只会考虑<code>ZeRO-2</code>和<code>ZeRO-3</code>。</strong></p>
<p>接下来介绍stage 2和3的常用config文件。</p>
<h3 id="3-2-ZeRO-Stage-2"><a href="#3-2-ZeRO-Stage-2" class="headerlink" title="3.2 ZeRO Stage 2"></a>3.2 ZeRO Stage 2</h3><p>结合官网的介绍，笔者提供一个常用的ZeRO-stage-2的config文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;bfloat16&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: &quot;auto&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;fp16&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: &quot;auto&quot;,</span><br><span class="line">        &quot;loss_scale&quot;: 0,</span><br><span class="line">        &quot;loss_scale_window&quot;: 1000,</span><br><span class="line">        &quot;initial_scale_power&quot;: 16,</span><br><span class="line">        &quot;hysteresis&quot;: 2,</span><br><span class="line">        &quot;min_loss_scale&quot;: 1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;optimizer&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;AdamW&quot;,</span><br><span class="line">        &quot;params&quot;: &#123;</span><br><span class="line">            &quot;lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;betas&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;eps&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;weight_decay&quot;: &quot;auto&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;scheduler&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;WarmupLR&quot;,</span><br><span class="line">        &quot;params&quot;: &#123;</span><br><span class="line">            &quot;warmup_min_lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;warmup_max_lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;warmup_num_steps&quot;: &quot;auto&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;zero_optimization&quot;: &#123;</span><br><span class="line">        &quot;stage&quot;: 2,</span><br><span class="line">        &quot;offload_optimizer&quot;: &#123;</span><br><span class="line">            &quot;device&quot;: &quot;cpu&quot;,</span><br><span class="line">            &quot;pin_memory&quot;: true</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;allgather_partitions&quot;: true,</span><br><span class="line">        &quot;allgather_bucket_size&quot;: 2e8,</span><br><span class="line">        &quot;overlap_comm&quot;: true,</span><br><span class="line">        &quot;reduce_scatter&quot;: true,</span><br><span class="line">        &quot;reduce_bucket_size&quot;: 2e8,</span><br><span class="line">        &quot;contiguous_gradients&quot;: true</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;gradient_accumulation_steps&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;gradient_clipping&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;train_batch_size&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;train_micro_batch_size_per_gpu&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;steps_per_print&quot;: 1e5</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>有关于<code>offload</code></li>
</ul>
<p>上述参数中，最重要的一个就是<code>&quot;offload_optimizer&quot;</code>。如上述所示，我们将其<code>”device“</code>设置成了cpu，DeepSpeed就会按照之前提到过的ZeRO操作，在训练过程中，将优化器状态分配到cpu上。从而降低单张GPU的memory占用。</p>
<ul>
<li>有关于<code>overlap_comm</code></li>
</ul>
<p>另外一个需要提到的参数是<code>overlap_comm</code>。简单地理解，它控制着多个memory上进程之间通信的buffer的大小。这个值越大，进程之间通信越快，模型训练速度也会提升，但相应的显存占用也会变大；反之亦然。</p>
<p>因此，<code>overlap_comm</code>也是一个需要进行一定权衡的参数。</p>
<h3 id="3-3-ZeRO-Stage-3"><a href="#3-3-ZeRO-Stage-3" class="headerlink" title="3.3 ZeRO Stage 3"></a>3.3 ZeRO Stage 3</h3><p>和Stage-2类似，笔者也提供一个stage-3的模板config</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;bfloat16&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: false</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;fp16&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: &quot;auto&quot;,</span><br><span class="line">        &quot;loss_scale&quot;: 0,</span><br><span class="line">        &quot;loss_scale_window&quot;: 1000,</span><br><span class="line">        &quot;initial_scale_power&quot;: 16,</span><br><span class="line">        &quot;hysteresis&quot;: 2,</span><br><span class="line">        &quot;min_loss_scale&quot;: 1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;optimizer&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;AdamW&quot;,</span><br><span class="line">        &quot;params&quot;: &#123;</span><br><span class="line">            &quot;lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;betas&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;eps&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;weight_decay&quot;: &quot;auto&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;scheduler&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;WarmupLR&quot;,</span><br><span class="line">        &quot;params&quot;: &#123;</span><br><span class="line">            &quot;warmup_min_lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;warmup_max_lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;warmup_num_steps&quot;: &quot;auto&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;zero_optimization&quot;: &#123;</span><br><span class="line">        &quot;stage&quot;: 3,</span><br><span class="line">        &quot;offload_optimizer&quot;: &#123;</span><br><span class="line">            &quot;device&quot;: &quot;cpu&quot;,</span><br><span class="line">            &quot;pin_memory&quot;: true</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;offload_param&quot;: &#123;</span><br><span class="line">            &quot;device&quot;: &quot;cpu&quot;,</span><br><span class="line">            &quot;pin_memory&quot;: true</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;overlap_comm&quot;: true,</span><br><span class="line">        &quot;contiguous_gradients&quot;: true,</span><br><span class="line">        &quot;sub_group_size&quot;: 1e9,</span><br><span class="line">        &quot;reduce_bucket_size&quot;: &quot;auto&quot;,</span><br><span class="line">        &quot;stage3_prefetch_bucket_size&quot;: &quot;auto&quot;,</span><br><span class="line">        &quot;stage3_param_persistence_threshold&quot;: &quot;auto&quot;,</span><br><span class="line">        &quot;stage3_max_live_parameters&quot;: 1e9,</span><br><span class="line">        &quot;stage3_max_reuse_distance&quot;: 1e9,</span><br><span class="line">        &quot;stage3_gather_fp16_weights_on_model_save&quot;: true</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;gradient_accumulation_steps&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;gradient_clipping&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;steps_per_print&quot;: 1e5,</span><br><span class="line">    &quot;train_batch_size&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;train_micro_batch_size_per_gpu&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;wall_clock_breakdown&quot;: false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>有关于<code>“offload_param”</code></li>
</ul>
<p>可以看到，除了和stage2一样，有<code>offload_optimizer</code>参数之外，stage3还有一个<code>offload_param</code>参数。即，将模型参数进行划分。</p>
<ul>
<li>stage-3相关的其他参数</li>
</ul>
<p>下面这些参数是stage-3-specific的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;sub_group_size&quot;: 1e9,</span><br><span class="line">&quot;reduce_bucket_size&quot;: &quot;auto&quot;,</span><br><span class="line">&quot;stage3_prefetch_bucket_size&quot;: &quot;auto&quot;,</span><br><span class="line">&quot;stage3_param_persistence_threshold&quot;: &quot;auto&quot;,</span><br><span class="line">&quot;stage3_max_live_parameters&quot;: 1e9,</span><br><span class="line">&quot;stage3_max_reuse_distance&quot;: 1e9,</span><br><span class="line">&quot;stage3_gather_fp16_weights_on_model_save&quot;: true</span><br></pre></td></tr></table></figure>

<h3 id="4-2-内存估计"><a href="#4-2-内存估计" class="headerlink" title="4.2 内存估计"></a>4.2 内存估计</h3><p>如之前多次强调的，DeepSpeed使用过程中的一个难点，就在于<code>时间和空间</code>的权衡。</p>
<p>分配更多参数到CPU上，虽然能够降低显存开销，但是也会极大地提升时间开销。</p>
<p>DeepSpeed提供了一段简单的memory估算代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from transformers import AutoModel</span><br><span class="line">from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live</span><br><span class="line"></span><br><span class="line">## specify the model you want to train on your device</span><br><span class="line">model = AutoModel.from_pretrained(&quot;t5-large&quot;)</span><br><span class="line">## estimate the memory cost (both CPU and GPU)</span><br><span class="line">estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=1, num_nodes=1)</span><br></pre></td></tr></table></figure>

<p>如上，如果不用stage2和stage3(最下面那两行)，训练T5-large需要一张显存至少为12.49GB的显卡(考虑到很多其他的缓存变量，还有你的batch_size，实际上可能需要24GB大小的卡)。而在相继使用了stage2和3之后，显存开销被极大地降低，转而CPU内存消耗显著提升，模型训练时间开销也相应地增大。</p>
<p><em><strong>建议:</strong></em><br>在使用DeepSpeed之前，先使用上述代码，大概估计一下显存消耗，决定使用的GPU数目，以及ZeRO-stage。</p>
<p>原则是，<strong>能直接多卡训练，就不要用ZeRO；能用ZeRO-2就不要用ZeRO-3.</strong></p>
<p>笔者尝试使用DeepSpeed进行模型的训练。</p>
<p>首先是stage 2，也就是只把optimizer放到cpu上。下面是使用前后的GPU显存占用和训练速度对比：</p>
<ul>
<li>GPU显存：<code>20513</code>MB &#x3D;&gt; <code>17349</code>MiB</li>
<li>训练速度 (由<code>tqdm</code>估计)：<code>1.3</code> iter&#x2F;s &#x3D;&gt; <code>0.77</code> iter&#x2F;s</li>
</ul>
<p>可以明显看到，GPU的显存占用有了明显降低，但是训练速度也变慢了。以笔者当前的使用体感来说，deepspeed并没有带来什么收益。</p>
<p>笔者的机器配有<code>24000</code>MB的显卡，batch_size为2时，占用<code>20513</code>MB；而DeepSpeed仅仅帮助笔者空出了<code>3000</code>MB的显存，<strong>还是完全不够增加batch_size</strong>, 导致笔者总训练时长变长。</p>
<p>因此，DeepSpeed或许仅适用于显存极度短缺（i.e., 模型大到 batch_size &#x3D;&#x3D; 1也跑不了）的情况；亦或是，使用DeepSpped节省下来的显存，刚好够支持更大的batch_size。否则，像笔者当前这种情况下，使用DeepSpeed只会增加时间开销，并没有其他益处。</p>
<p>此后，笔者还尝试使用stage 3，但是<strong>速度极其缓慢</strong>。一个原先需要6h的训练过程，用了DeepSpeed stage3之后，运行了2天2夜，也没有结束的迹象。无奈笔者只好终止测试。</p>
<p>此外，在使用DeepSpeed stage2时，由于分配了模型参数到多个设备上，console里面也看不到任何输出信息（但是GPU还是在呼呼响，utility也为100%），让人都不知道程序的运行进度，可以说对用户非常不友好了。</p>
<p>由于DeepSpeed会通过占用CPU内存来减缓GPU的开销，当系统CPU不够的时候，DeepSpeed进程就会自动被系统停止，<strong>造成没有任何报错，DeepSpeed无法启动的现象</strong>。建议先用上文介绍的<a target="_blank" rel="noopener" href="https://aitechtogether.com/article/45439.html#jump">estimation</a>估计一下CPU内存占用，然后用<code>free -h</code>查看一下机器的CPU内存空余量，来判断能否使用DeepSpeed。</p>
<p>另外，还有可能因为训练精度问题，出现loss为<code>NAN</code>的情况。详见：<a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/main_classes/deepspeed#troubleshooting">Troubleshooting</a>.</p>
<p>使用DeepSpeed stage2之后，就不能灵活地更改optimizer了。下图是DeepSpeed.py的源代码：<br><a target="_blank" rel="noopener" href="https://img-blog.csdnimg.cn/24c2ee1e7328450e940702291ef551f2.png"><img src="https://img-blog.csdnimg.cn/24c2ee1e7328450e940702291ef551f2.png" alt="在这里插入图片描述"></a><br>默认optimizer必须在config里面设置好，也就是使用默认的优化器和学习率，不能实现分组学习率。如果要自定义optimizer的初始化过程，必须实现两个版本的optimizer（CPU+GPU）。如官方所述：</p>
<p>并行训练技术主要是如何使用多块显卡并行训练模型，主要分为三种并行方式：数据并行（Data Parallel）、张量并行（Tensor Parallel）和流水线并行（Pipeline Parallel）。</p>
<h2 id="数据并行"><a href="#数据并行" class="headerlink" title="数据并行"></a>数据并行</h2><p>数据并行是目前最常见的并行方式，它的核心思想是对数据进行划分，将数据分配给不同GPU进行计算。在数据并行中，每个GPU上存储打的模型、优化器状态是相同的。每块GPU上执行玩前后向传播后，会将每个GPU是哪个计算出的梯度汇总求平均。</p>
<p><img src="/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D%20-%20%E5%89%AF%E6%9C%AC/2.jpg"></p>
<p>为了提高预训练模型的泛化能力，近年来预训练模型的一个趋势是参数量在快速增大，目前已经到达万亿规模。但如此大的参数量会使得模型训练变得十分困难，于是不少的相关研究者和机构对此提出了许多大模型高效训练的技术。本文将分为三部分来介绍大模型高效训练所需要的主要技术：并行训练技术、显存优化技术和其他技术。文章最后会展示当前较为流行的训练加速库的统计。欢迎大家批评指正，相互交流。</p>
<p>当前在 GPU 集群上常用的并行计算方式包括：数据并行、模型并行和流水线并行。我们可以混合使用这三种并行计算方式，即「混合并行」。对于 GPU 数量不足的使用者而言，微软 DeepSpeed 框架中提出的并行优化组件 The Zero Redundancy Optimizer (ZeRO) 和 ZeRO-Offload 技术可以混合使用 GPU 和 CPU，降低显存的压力。</p>
<h3 id="数据并行：将训练数据分而治之"><a href="#数据并行：将训练数据分而治之" class="headerlink" title="数据并行：将训练数据分而治之"></a><strong>数据并行：将训练数据分而治之</strong></h3><p>数据并行是一种简单的并行计算方法，其思想是在分布式计算集群中的各个计算节点上复制一份相同的模型参数，进而在各个计算节点上使用相同的模型对各自接收到的输入数据进行计算。根据具体的实现方式，我们又可以将「数据并行」分为「同步训练」和「异步训练」模式。</p>
<h3 id="模型并行与流水线并行：将模型-由大化小"><a href="#模型并行与流水线并行：将模型-由大化小" class="headerlink" title="模型并行与流水线并行：将模型****由大化小"></a><strong>模型并行与流水线并行：将模型****由大化小</strong></h3><p>尽管数据并行往往可以有效提升计算效率，但是该方法无法拓展模型的参数。如果一个模型的参数量已经大到 GPU 显存无法存下，那么仅靠数据并行就无法解决显存不足的问题。此时，我们需要采用模型并行和流水线并行计算方法。</p>
<p>模型并行的主要思想是：将模型进行切分，然后将其分配到多个计算节点上，以减少单个计算节点的参数量。常见的切分方法是：将模型的每一层参数平均切分到多个计算节点上。该方式能够减少单个计算节点的参数量和计算量，但会引入大量的通信和同步开销。</p>
<p>流水线并行是另一种模型并行实现方式，它指的是：将模型按层为粒度进行切分，并且将不同层的参数分配给各个计算节点。流水线并行在对模型参数进行切分的同时，能够降低节点之间的通信量。但是，在我们启动和停止流水线之间的时间里，会有部分节点处于等待状态，这在某种程度上也导致了部分算力的浪费。</p>
<p>模型并行与流水线并行各有优劣，一般需要根据显卡、机器之间的通信速度来决定如何使用。</p>
<p>DeepSpeed是一个开源深度学习训练优化库，包含新的显存优化技术——ZeRO(零冗余优化器)通过扩大规模，提升速度，控制成本，提升可用性，极大地推进了大模型训练能力。</p>
<p>主流并行计算框架一览</p>
<p><img src="/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D%20-%20%E5%89%AF%E6%9C%AC/1.png"></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://aitechtogether.com/article/45439.html">https://aitechtogether.com/article/45439.html</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/414773915">https://zhuanlan.zhihu.com/p/414773915</a></p>
<p><a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/">https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/</a></p>
<p><a target="_blank" rel="noopener" href="https://www.guyuehome.com/39789">https://www.guyuehome.com/39789</a></p>
<p><a target="_blank" rel="noopener" href="https://toutiao.io/posts/p7cpkm6/preview">https://toutiao.io/posts/p7cpkm6/preview</a></p>
<p><a target="_blank" rel="noopener" href="https://hub.baai.ac.cn/view/7771">https://hub.baai.ac.cn/view/7771</a></p>
<p><a target="_blank" rel="noopener" href="https://www.kuxai.com/article/476">https://www.kuxai.com/article/476</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/yaohaishen/article/details/127471992">https://blog.csdn.net/yaohaishen/article/details/127471992</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/489134718">https://zhuanlan.zhihu.com/p/489134718</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/343570325">https://zhuanlan.zhihu.com/p/343570325</a></p>
<p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/distributed-parallel-training-data-parallelism-and-model-parallelism-ec2d234e3214">https://towardsdatascience.com/distributed-parallel-training-data-parallelism-and-model-parallelism-ec2d234e3214</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u010751000/article/details/123516433">https://blog.csdn.net/u010751000/article/details/123516433</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/450689346">https://zhuanlan.zhihu.com/p/450689346</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/yqw0710/p/16060765.html">https://www.cnblogs.com/yqw0710/p/16060765.html</a></p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/02/15/chat-gpt/" rel="prev" title="chat_gpt">
                  <i class="fa fa-chevron-left"></i> chat_gpt
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D/" rel="next" title="deepspeed介绍">
                  deepspeed介绍 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Coder4nlp</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
