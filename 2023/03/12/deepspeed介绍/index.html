<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Noto+Serif+SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.14.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.json","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="DeepSpeed介绍近年来，模型规模迅速增加，动辄几B甚至几百B。但是GPU显存大小根本无法支撑训练推理。首先模型参数过多，导致单机内存放不下，即使能放得下，算力也跟不上。同时，硬件算力的增长远远比不上模型增长的速度，单机训练变得不再可行，需要并行化分布式训练加速。比如Megatron-Turing NLG有 530B 的参数，训练需要超过 10T 的内存来存储权重、梯度和状态。  同时，模型是">
<meta property="og:type" content="article">
<meta property="og:title" content="deepspeed介绍">
<meta property="og:url" content="http://example.com/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D/index.html">
<meta property="og:site_name" content="Coder4nlp&#39;s Blog">
<meta property="og:description" content="DeepSpeed介绍近年来，模型规模迅速增加，动辄几B甚至几百B。但是GPU显存大小根本无法支撑训练推理。首先模型参数过多，导致单机内存放不下，即使能放得下，算力也跟不上。同时，硬件算力的增长远远比不上模型增长的速度，单机训练变得不再可行，需要并行化分布式训练加速。比如Megatron-Turing NLG有 530B 的参数，训练需要超过 10T 的内存来存储权重、梯度和状态。  同时，模型是">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D/trendofnlpmodelsize.png">
<meta property="og:image" content="http://example.com/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D/1.png">
<meta property="article:published_time" content="2023-03-12T13:56:26.000Z">
<meta property="article:modified_time" content="2023-03-16T10:58:52.780Z">
<meta property="article:author" content="Coder4nlp">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D/trendofnlpmodelsize.png">


<link rel="canonical" href="http://example.com/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D/","path":"2023/03/12/deepspeed介绍/","title":"deepspeed介绍"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>deepspeed介绍 | Coder4nlp's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Coder4nlp's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-following"><a href="/following/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>following</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#DeepSpeed%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.</span> <span class="nav-text">DeepSpeed介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#DeepSpeed%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">1.1.</span> <span class="nav-text">DeepSpeed是什么？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Deepspeed%E5%AE%89%E8%A3%85"><span class="nav-number">1.2.</span> <span class="nav-text">Deepspeed安装</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DeepSpeed%E4%BD%BF%E7%94%A8"><span class="nav-number">1.3.</span> <span class="nav-text">DeepSpeed使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DeepSpeed%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">1.3.1.</span> <span class="nav-text">DeepSpeed初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#batch-size%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">batch_size相关参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">优化器相关参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Scheduler-%E5%8F%82%E6%95%B0"><span class="nav-number">1.3.1.3.</span> <span class="nav-text">Scheduler 参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#FP16-%E9%80%89%E9%A1%B9"><span class="nav-number">1.3.1.4.</span> <span class="nav-text">FP16 选项</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ZeRO-Optimizations-for-FP16-Training"><span class="nav-number">1.3.1.5.</span> <span class="nav-text">ZeRO Optimizations for FP16 Training</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%BF%80%E6%B4%BBCheckpointing"><span class="nav-number">1.3.1.6.</span> <span class="nav-text">激活Checkpointing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%97%A5%E5%BF%97"><span class="nav-number">1.3.1.7.</span> <span class="nav-text">日志</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GLM-10B%E4%BD%BF%E7%94%A8%E7%9A%84%E5%8F%82%E6%95%B0"><span class="nav-number">1.3.1.8.</span> <span class="nav-text">GLM 10B使用的参数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%8F%8A%E4%BC%98%E5%8C%96%E5%99%A8%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">1.3.2.</span> <span class="nav-text">模型及优化器初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DeepSpeed%E8%AE%AD%E7%BB%83"><span class="nav-number">1.3.3.</span> <span class="nav-text">DeepSpeed训练</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ZeRO%E6%A6%82%E8%BF%B0"><span class="nav-number">1.4.</span> <span class="nav-text">ZeRO概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E4%BC%B0%E8%AE%A1"><span class="nav-number">1.4.1.</span> <span class="nav-text">内存估计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E6%B5%81%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6%E4%B8%80%E8%A7%88"><span class="nav-number">1.4.2.</span> <span class="nav-text">主流并行计算框架一览</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Coder4nlp</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Coder4nlp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="deepspeed介绍 | Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          deepspeed介绍
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-12 21:56:26" itemprop="dateCreated datePublished" datetime="2023-03-12T21:56:26+08:00">2023-03-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-16 18:58:52" itemprop="dateModified" datetime="2023-03-16T18:58:52+08:00">2023-03-16</time>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="DeepSpeed介绍"><a href="#DeepSpeed介绍" class="headerlink" title="DeepSpeed介绍"></a>DeepSpeed介绍</h1><p>近年来，模型规模迅速增加，动辄几B甚至几百B。但是GPU显存大小根本无法支撑训练推理。首先模型参数过多，导致单机内存放不下，即使能放得下，算力也跟不上。同时，硬件算力的增长远远比不上模型增长的速度，单机训练变得不再可行，需要并行化分布式训练加速。比如<code>Megatron-Turing NLG</code>有 530B 的参数，训练需要超过 10T 的内存来存储权重、梯度和状态。</p>
<p><img src="/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D/trendofnlpmodelsize.png"></p>
<p>同时，模型是一个有机的整体，简单增加机器数量并不能提升算力，需要有并行策略和通信设计，才能实现高效的并行训练。</p>
<p>Pytorch的分布式并行框架（Distributed Data Parallel，简称DDP），它也是仅仅能够将数据并行，放到各个GPU的模型上进行训练。</p>
<p>也就是说，DDP的应用场景在你的模型大小大于显卡显存大小时，它就很难继续使用了，除非你自己再将模型参数拆散分散到各个GPU上。</p>
<p>今天要给大家介绍的<a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a>，它就能实现这个拆散功能，它通过将模型参数拆散分布到各个GPU上，以实现大型模型的计算，弥补了DDP的缺点，非常方便，这也就意味着我们能用更少的GPU训练更大的模型，而且不受限于显存。</p>
<h2 id="DeepSpeed是什么？"><a href="#DeepSpeed是什么？" class="headerlink" title="DeepSpeed是什么？"></a>DeepSpeed是什么？</h2><p>DeepSpeed是一个开源深度学习训练优化库，其中包含的一个新的显存优化技术—— ZeRO（零冗余优化器），通过扩大规模，提升速度，控制成本，提升可用性，极大地推进了大模型训练能力。DeepSpeed的核心就在于：<strong>GPU显存不够，CPU内存来凑</strong>。DeepSpeed使用的一个核心要义是，<strong>时间开销和显存占用的权衡</strong>。</p>
<ul>
<li><p><strong>用 3D 并行化实现万亿参数模型训练：</strong> DeepSpeed 实现了三种并行方法的灵活组合：ZeRO 支持的数据并行，流水线并行和张量切片模型并行。3D 并行性适应了不同工作负载的需求，以支持具有<strong>万亿</strong>参数的<strong>超大型模型</strong>，同时实现了近乎完美的显存扩展性和吞吐量扩展效率。此外，其提高的通信效率使用户可以在网络带宽有限的常规群集上以 2-7 倍的速度训练有数十亿参数的模型。</p>
</li>
<li><p><strong>ZeRO-Offload 使 GPU 单卡能够训练 10 倍大的模型：</strong> 为了同时利用 CPU 和 GPU 内存来训练大型模型，我们扩展了 ZeRO-2。我们的用户在使用带有<strong>单张英伟达 V100 GPU</strong> 的机器时，可以在不耗尽显存的情况下运行<strong>多达 130 亿个参数的模型</strong>，模型规模扩展至现有方法的10倍，并保持有竞争力的吞吐量。此功能使数十亿参数的模型训练更加大众化，，并为许多深度学习从业人员打开了一扇探索更大更好的模型的窗户。</p>
</li>
</ul>
<h2 id="Deepspeed安装"><a href="#Deepspeed安装" class="headerlink" title="Deepspeed安装"></a>Deepspeed安装</h2><p>开始使用DeepSpeed的最快方法是通过pip，这将安装DeepSpeed的最新版本，它不绑定到特定的PyTorch或CUDA版本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install deepspeed</span><br></pre></td></tr></table></figure>

<ul>
<li>在安装DeepSpeed之前必须先安装PyTorch。</li>
<li>为了获得完整的特性支持，我们建议使用PyTorch&gt;&#x3D; 1.8版本，最好是最新的PyTorch稳定版本。</li>
</ul>
<p>安装后，您可以验证您的安装，并通过DeepSpeed环境报告查看您的机器与哪些扩展&#x2F;操作兼容。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ds_report</span><br></pre></td></tr></table></figure>

<h2 id="DeepSpeed使用"><a href="#DeepSpeed使用" class="headerlink" title="DeepSpeed使用"></a>DeepSpeed使用</h2><p>以GLM使用到的deepspeed配置为例</p>
<h3 id="DeepSpeed初始化"><a href="#DeepSpeed初始化" class="headerlink" title="DeepSpeed初始化"></a>DeepSpeed初始化</h3><p>DeepSpeed 通过输入参数来启动训练，因此需要使用<code>argparse</code>解析参数。完整的参数可以查看<a target="_blank" rel="noopener" href="https://www.deepspeed.ai/docs/config-json/">DeepSpeed文档</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser = deepspeed.add_config_arguments(parser)</span><br></pre></td></tr></table></figure>

<p>pytorch初始化的分布式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.distributed.init_process_group(</span><br><span class="line">           backend=args.distributed_backend,</span><br><span class="line">           world_size=args.world_size, rank=args.rank,</span><br><span class="line">           init_method=init_method)</span><br></pre></td></tr></table></figure>

<p>DeepSpeed初始化分布式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deepspeed.init_distributed(dist_backend=args.distributed_backend)</span><br></pre></td></tr></table></figure>

<p>这里介绍一些常用的参数，</p>
<h4 id="batch-size相关参数"><a href="#batch-size相关参数" class="headerlink" title="batch_size相关参数"></a>batch_size相关参数</h4><p><em><strong>train_batch_size</strong></em>: [integer]（注意:train_batch_size必须等于train_micro_batch_size_per_gpu * gradient_accumulation * gpu个数。为了简单起见，你可以选择只指定三个参数中的两个，最后一个参数将由DeepSpeed自动推断。）</p>
<table>
<thead>
<tr>
<th align="left">Value</th>
<th align="left">Example</th>
</tr>
</thead>
<tbody><tr>
<td align="left">The effective training batch size. This is the amount of data samples that leads to one step of model update. <em><strong>train_batch_size</strong></em> is aggregated by the batch size that a single GPU processes in one forward&#x2F;backward pass (a.k.a., <em><strong>train_micro_batch_size_per_gpu</strong></em>), the gradient accumulation steps (a.k.a., <em><strong>gradient_accumulation_steps</strong></em>), and the number of GPUs. Can be omitted if both <em><strong>train_micro_batch_size_per_gpu</strong></em> and <em><strong>gradient_accumulation_steps</strong></em> are provided.</td>
<td align="left"></td>
</tr>
</tbody></table>
<p><em><strong>train_micro_batch_size_per_gpu</strong></em>: [integer]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Batch size to be processed by one GPU in one step (without gradient accumulation). Can be omitted if both <em><strong>train_batch_size</strong></em> and <em><strong>gradient_accumulation_steps</strong></em> are provided.</td>
<td align="left"><em><strong>train_batch_size</strong></em> value</td>
</tr>
</tbody></table>
<p><em><strong>gradient_accumulation_steps</strong></em>: [integer]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Number of training steps to accumulate gradients before averaging and applying them. This feature is sometimes useful to improve scalability since it results in less frequent communication of gradients between steps. Another impact of this feature is the ability to train with larger batch sizes per GPU. Can be omitted if both <em><strong>train_batch_size</strong></em> and <em><strong>train_micro_batch_size_per_gpu</strong></em> are provided.</td>
<td align="left"></td>
</tr>
</tbody></table>
<h4 id="优化器相关参数"><a href="#优化器相关参数" class="headerlink" title="优化器相关参数"></a>优化器相关参数</h4><p><em><strong>optimizer</strong></em>: [dictionary]</p>
<table>
<thead>
<tr>
<th align="left">Fields</th>
<th align="left">Value</th>
<th align="left">Example</th>
</tr>
</thead>
<tbody><tr>
<td align="left">type</td>
<td align="left">The optimizer name. DeepSpeed natively supports <strong>Adam</strong>, <strong>AdamW</strong>, <strong>OneBitAdam</strong>, <strong>Lamb</strong>, and <strong>OneBitLamb</strong> optimizers (See <a target="_blank" rel="noopener" href="https://deepspeed.readthedocs.io/en/latest/optimizers.html">here</a> for details) and will import other optimizers from <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html">torch</a>.</td>
<td align="left"><code>&quot;Adam&quot;</code></td>
</tr>
<tr>
<td align="left">params</td>
<td align="left">Dictionary of parameters to instantiate optimizer. The parameter names must match the optimizer constructor signature (e.g., for <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Adam">Adam</a>).</td>
<td align="left"><code>&#123;&quot;lr&quot;: 0.001, &quot;eps&quot;: 1e-8&#125;</code></td>
</tr>
</tbody></table>
<h4 id="Scheduler-参数"><a href="#Scheduler-参数" class="headerlink" title="Scheduler 参数"></a>Scheduler 参数</h4><p>在执行model_engine.step()时，DeepSpeed在每个训练步骤中调用调度器的step()方法。</p>
<p>*<strong>scheduler*</strong>: [dictionary]</p>
<table>
<thead>
<tr>
<th align="left">Fields</th>
<th align="left">Value</th>
<th align="left">Example</th>
</tr>
</thead>
<tbody><tr>
<td align="left">type</td>
<td align="left">The scheduler name. See <a target="_blank" rel="noopener" href="https://deepspeed.readthedocs.io/en/latest/schedulers.html">here</a> for list of support schedulers.</td>
<td align="left"><code>&quot;WarmupLR&quot;</code></td>
</tr>
<tr>
<td align="left">params</td>
<td align="left">Dictionary of parameters to instantiate scheduler. The parameter names should match scheduler constructor signature.</td>
<td align="left"><code>&#123;&quot;warmup_min_lr&quot;: 0, &quot;warmup_max_lr&quot;: 0.001&#125;</code></td>
</tr>
</tbody></table>
<h4 id="FP16-选项"><a href="#FP16-选项" class="headerlink" title="FP16 选项"></a>FP16 选项</h4><p><strong>注意</strong>:此模式不能与下面描述的amp模式组合。</p>
<p><em><strong>fp16</strong></em>: [dictionary]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Configuration for using mixed precision&#x2F;FP16 training that leverages <a target="_blank" rel="noopener" href="https://nvidia.github.io/apex/">NVIDIA’s Apex package</a>. An example, including the available dictionary keys is illustrated below. NOTE: this does not use Apex’s AMP mode that allows for more flexibility in mixed precision training modes, this mode is similar to AMP’s O2 mode. Please see AMP support below if you want to use more complex mixed precision modes. If you want to use ZeRO (currently) you must use this mode.</td>
<td align="left">None</td>
</tr>
</tbody></table>
<h4 id="ZeRO-Optimizations-for-FP16-Training"><a href="#ZeRO-Optimizations-for-FP16-Training" class="headerlink" title="ZeRO Optimizations for FP16 Training"></a>ZeRO Optimizations for FP16 Training</h4><p>启用和配置ZeRO内存优化</p>
<p><em><strong>stage</strong></em>: [integer]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">选择ZeRO优化器的不同阶段。阶段0、阶段1、阶段2、阶段3分别为禁用、优化器状态分区、优化器+梯度状态分区、优化器+梯度+参数分区。</td>
<td align="left"><code>0</code></td>
</tr>
</tbody></table>
<p><em><strong>reduce_scatter</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">使用reduce或reduce scatter点代替all reduce to average gradient</td>
<td align="left"><code>true</code></td>
</tr>
</tbody></table>
<p>*<strong>reduce_bucket_size*</strong>: [integer]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Number of elements reduced&#x2F;allreduced at a time. Limits the memory required for the allgather for large model sizes</td>
<td align="left"><code>5e8</code></td>
</tr>
</tbody></table>
<p><em><strong>contiguous_gradients</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Copies the gradients to a contiguous buffer as they are produced. Avoids memory fragmentation during backward pass.</td>
<td align="left"><code>True</code></td>
</tr>
</tbody></table>
<p><em><strong>overlap_comm</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">另外一个需要提到的参数是<code>overlap_comm</code>。简单地理解，它控制着多个memory上进程之间通信的buffer的大小。这个值越大，进程之间通信越快，模型训练速度也会提升，但相应的显存占用也会变大；反之亦然。Attempts to overlap the reduction of the gradients with backward computation</td>
<td align="left"><code>false</code></td>
</tr>
</tbody></table>
<p>*<strong>allgather_bucket_size*</strong>: [integer]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Number of elements allgathered at a time. Limits the memory required for the allgather for large model sizes</td>
<td align="left"><code>5e8</code></td>
</tr>
</tbody></table>
<p>*<strong>cpu_offload*</strong>: [boolean]</p>
<p><strong>Deprecated:</strong> <strong>cpu_offload</strong> is deprecated and will be removed in future, please use <code>offload_optimizer</code> instead.</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">在训练过程中，将优化器状态分配到cpu上。从而降低单张GPU的memory占用。Enable offloading of optimizer memory and computation to CPU. This frees up GPU memory for larger models or batch sizes. Valid with stage 1 and 2.</td>
<td align="left"><code>False</code></td>
</tr>
</tbody></table>
<h4 id="激活Checkpointing"><a href="#激活Checkpointing" class="headerlink" title="激活Checkpointing"></a>激活Checkpointing</h4><p><em><strong>partition_activations</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Enables partition activation when used with model parallelism</td>
<td align="left"><code>false</code></td>
</tr>
</tbody></table>
<p><em><strong>contiguous_memory_optimization</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Copies partitioned activations so that they are contiguous in memory</td>
<td align="left"><code>false</code></td>
</tr>
</tbody></table>
<h4 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h4><p><em><strong>wall_clock_breakdown</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Enable timing of the latency of forward&#x2F;backward&#x2F;update training phases</td>
<td align="left"><code>false</code></td>
</tr>
</tbody></table>
<h4 id="GLM-10B使用的参数"><a href="#GLM-10B使用的参数" class="headerlink" title="GLM 10B使用的参数"></a>GLM 10B使用的参数</h4><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="punctuation">:</span> <span class="number">4</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;gradient_accumulation_steps&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;steps_per_print&quot;</span><span class="punctuation">:</span> <span class="number">50</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;gradient_clipping&quot;</span><span class="punctuation">:</span> <span class="number">1.0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;zero_optimization&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;stage&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;contiguous_gradients&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;overlap_comm&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;reduce_scatter&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;reduce_bucket_size&quot;</span><span class="punctuation">:</span> <span class="number">5e7</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;allgather_bucket_size&quot;</span><span class="punctuation">:</span> <span class="number">5e7</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;cpu_offload&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;zero_allow_untested_optimizer&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;fp16&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;enabled&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;loss_scale&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;loss_scale_window&quot;</span><span class="punctuation">:</span> <span class="number">1000</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;hysteresis&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;min_loss_scale&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;optimizer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Adam&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;params&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;lr&quot;</span><span class="punctuation">:</span> <span class="number">5e-6</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;betas&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="number">0.9</span><span class="punctuation">,</span></span><br><span class="line">        <span class="number">0.95</span></span><br><span class="line">      <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;eps&quot;</span><span class="punctuation">:</span> <span class="number">1e-8</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;weight_decay&quot;</span><span class="punctuation">:</span> <span class="number">1e-2</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;activation_checkpointing&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;partition_activations&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;contiguous_memory_optimization&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;wall_clock_breakdown&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="模型及优化器初始化"><a href="#模型及优化器初始化" class="headerlink" title="模型及优化器初始化"></a>模型及优化器初始化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">model, optimizer, _, _ = deepspeed.initialize(</span><br><span class="line">    model=model,</span><br><span class="line">    model_parameters=param_groups,</span><br><span class="line">    args=args,</span><br><span class="line">    mpu=mpu,</span><br><span class="line">    dist_init_required=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="DeepSpeed训练"><a href="#DeepSpeed训练" class="headerlink" title="DeepSpeed训练"></a>DeepSpeed训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deepspeed --hostfile $&#123;HOST_FILE_PATH&#125; --master_port $&#123;MASTER_PORT&#125; --num_nodes $&#123;NUM_WORKERS&#125; --num_gpus $&#123;NUM_GPUS_PER_WORKER&#125;<span class="string">&quot; finetune_glm.py</span></span><br></pre></td></tr></table></figure>

<p>以GLM为例，使用自己的数据训练</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bash scripts/ds_finetune_seq2seq.sh \ </span><br><span class="line">   config_tasks/model_blocklm_10B.sh \ </span><br><span class="line">   config_tasks/seq_customization.sh</span><br></pre></td></tr></table></figure>

<h2 id="ZeRO概述"><a href="#ZeRO概述" class="headerlink" title="ZeRO概述"></a>ZeRO概述</h2><p>Zero Redundancy Optimizer (ZeRO)是DeepSpeed的workhorse. 用户可以提供不同的ZeRO config文件，来实现DeepSpeed的不同功能特性。</p>
<p>来看一下<a target="_blank" rel="noopener" href="https://deepspeed.readthedocs.io/en/latest/zero3.html#getting-started">官网教程</a>对ZeRO的描述：</p>
<blockquote>
<p>The Zero Redundancy Optimizer (ZeRO) removes the memory redundancies across data-parallel processes by partitioning the three model states (optimizer states, gradients, and parameters) across data-parallel processes instead of replicating them. By doing this, it boosts memory efficiency compared to classic data-parallelism while retaining its computational granularity and communication efficiency.</p>
</blockquote>
<p>一句话总结： <code>partitioning instead of replicating</code>，<strong>划分而不是复制</strong>。</p>
<p>即，传统的<a target="_blank" rel="noopener" href="https://aitechtogether.com/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">深度学习</a>，模型训练并行，是将模型参数复制多份到多张GPU上，只将数据拆分（如，torch的Dataparallel），这样就会有大量的显存冗余浪费。而ZeRO就是为了消除这种冗余，提高对memory的利用率。注意，这里的“memory”不仅指多张GPU memory，还包括CPU。</p>
<p>而ZeRO的实现方法，就是把参数占用，逻辑上分成三种类型。将这些类型的参数划分：</p>
<ul>
<li><code>optimizer states</code>：即优化器的参数状态。例如，Adam的动量参数。</li>
<li><code>gradients</code>：梯度缓存，对应于optimizer。</li>
<li><code>parameters</code>：模型参数。</li>
</ul>
<p>对应的，DeepSpeed的ZeRO config文件就可以分为如下几类：</p>
<ul>
<li><code>ZeRO Stage 1</code>: 划分optimizer states。优化器参数被划分到多个memory上，每个momoey上的进程只负责更新它自己那部分参数。</li>
<li><code>ZeRO Stage 2</code>: 划分gradient。每个memory，只保留它分配到的optimizer state所对应的梯度。这很合理，因为梯度和optimizer是紧密联系在一起的。只知道梯度，不知道optimizer state，是没有办法优化模型参数的。</li>
<li><code>ZeRO Stage 3</code>: 划分模型参数，或者说，不同的layer. ZeRO-3会在forward和backward的时候，自动将模型参数分配到多个memory。</li>
</ul>
<p>由于ZeRO-1只分配optimizer states(参数量很小)，实际使用的时候，<strong>我们一般只会考虑<code>ZeRO-2</code>和<code>ZeRO-3</code>。</strong></p>
<h3 id="内存估计"><a href="#内存估计" class="headerlink" title="内存估计"></a>内存估计</h3><p>DeepSpeed使用过程中的一个难点，就在于<code>时间和空间</code>的权衡。</p>
<p>分配更多参数到CPU上，虽然能够降低显存开销，但是也会极大地提升时间开销。</p>
<p>DeepSpeed提供了一段简单的memory估算代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModel</span><br><span class="line"><span class="keyword">from</span> deepspeed.runtime.zero.stage3 <span class="keyword">import</span> estimate_zero3_model_states_mem_needs_all_live</span><br><span class="line"></span><br><span class="line"><span class="comment">## specify the model you want to train on your device</span></span><br><span class="line">model = AutoModel.from_pretrained(<span class="string">&quot;t5-large&quot;</span>)</span><br><span class="line"><span class="comment">## estimate the memory cost (both CPU and GPU)</span></span><br><span class="line">estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=<span class="number">1</span>, num_nodes=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>如上，如果不用stage2和stage3(最下面那两行)，训练T5-large需要一张显存至少为12.49GB的显卡(考虑到很多其他的缓存变量，还有你的batch_size，实际上可能需要24GB大小的卡)。而在相继使用了stage2和3之后，显存开销被极大地降低，转而CPU内存消耗显著提升，模型训练时间开销也相应地增大。</p>
<p>***建议:***在使用DeepSpeed之前，先使用上述代码，大概估计一下显存消耗，决定使用的GPU数目，以及ZeRO-stage。</p>
<p>原则是，<strong>能直接多卡训练，就不要用ZeRO；能用ZeRO-2就不要用ZeRO-3.</strong></p>
<p>尝试使用DeepSpeed进行模型的训练。</p>
<p>首先是stage 2，也就是只把optimizer放到cpu上。下面是使用前后的GPU显存占用和训练速度对比：</p>
<ul>
<li>GPU显存：<code>20513</code>MB &#x3D;&gt; <code>17349</code>MiB</li>
<li>训练速度 (由<code>tqdm</code>估计)：<code>1.3</code> iter&#x2F;s &#x3D;&gt; <code>0.77</code> iter&#x2F;s</li>
</ul>
<p>可以明显看到，GPU的显存占用有了明显降低，但是训练速度也变慢了。以笔者当前的使用体感来说，deepspeed并没有带来什么收益。</p>
<p>机器配有<code>24000</code>MB的显卡，batch_size为2时，占用<code>20513</code>MB；而DeepSpeed仅仅可以帮助空出了<code>3000</code>MB的显存，<strong>还是完全不够增加batch_size</strong>, 导致总训练时长变长。</p>
<p>因此，DeepSpeed或许仅适用于显存极度短缺（i.e., 模型大到 batch_size &#x3D;&#x3D; 1也跑不了）的情况；亦或是，使用DeepSpped节省下来的显存，刚好够支持更大的batch_size。否则，像笔者当前这种情况下，使用DeepSpeed只会增加时间开销，并没有其他益处。</p>
<p>此后，笔者还尝试使用stage 3，但是<strong>速度极其缓慢</strong>。一个原先需要6h的训练过程，用了DeepSpeed stage3之后，运行了2天2夜，也没有结束的迹象。无奈笔者只好终止测试。</p>
<p>此外，在使用DeepSpeed stage2时，由于分配了模型参数到多个设备上，console里面也看不到任何输出信息（但是GPU还是在呼呼响，utility也为100%），让人都不知道程序的运行进度，可以说对用户非常不友好了。</p>
<p>由于DeepSpeed会通过占用CPU内存来减缓GPU的开销，当系统CPU不够的时候，DeepSpeed进程就会自动被系统停止，<strong>造成没有任何报错，DeepSpeed无法启动的现象</strong>。建议先用上文介绍的<a target="_blank" rel="noopener" href="https://aitechtogether.com/article/45439.html#jump">estimation</a>估计一下CPU内存占用，然后用<code>free -h</code>查看一下机器的CPU内存空余量，来判断能否使用DeepSpeed。</p>
<p>另外，还有可能因为训练精度问题，出现loss为<code>NAN</code>的情况。详见：<a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/main_classes/deepspeed#troubleshooting">Troubleshooting</a>.</p>
<h3 id="主流并行计算框架一览"><a href="#主流并行计算框架一览" class="headerlink" title="主流并行计算框架一览"></a>主流并行计算框架一览</h3><p><img src="/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D/1.png"></p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D%20-%20%E5%89%AF%E6%9C%AC/" rel="prev" title="deepspeed介绍">
                  <i class="fa fa-chevron-left"></i> deepspeed介绍
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/03/15/deepspeed%E4%BB%8B%E7%BB%8D/GLM-main/README/" rel="next" title="">
                   <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Coder4nlp</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
