<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Noto+Serif+SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.14.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.json","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Pytorch显存分析在深度学习中，常见的错误是CUDA out of memory，这是因为显存放不下那么多模型参数和中间变量。通过及时清空中间变量，优化代码，减少batch等能够减少显存溢出。">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch显存分析">
<meta property="og:url" content="http://example.com/2023/02/09/Pytorch%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90/index.html">
<meta property="og:site_name" content="Coder4nlp&#39;s Blog">
<meta property="og:description" content="Pytorch显存分析在深度学习中，常见的错误是CUDA out of memory，这是因为显存放不下那么多模型参数和中间变量。通过及时清空中间变量，优化代码，减少batch等能够减少显存溢出。">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-02-09T05:11:54.000Z">
<meta property="article:modified_time" content="2023-02-09T05:13:12.851Z">
<meta property="article:author" content="Coder4nlp">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/2023/02/09/Pytorch%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2023/02/09/Pytorch%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90/","path":"2023/02/09/Pytorch显存分析/","title":"Pytorch显存分析"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Pytorch显存分析 | Coder4nlp's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Coder4nlp's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-following"><a href="/following/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>following</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Pytorch%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90"><span class="nav-number">1.</span> <span class="nav-text">Pytorch显存分析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="nav-number">1.1.</span> <span class="nav-text">深度学习训练过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Torch%E6%9C%BA%E5%88%B6"><span class="nav-number">1.2.</span> <span class="nav-text">Torch机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CUDA-context-%E5%BC%80%E9%94%80"><span class="nav-number">1.2.1.</span> <span class="nav-text">CUDA context 开销</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PyTorch%E6%98%BE%E5%AD%98%E5%88%86%E9%85%8D%E6%9C%BA%E5%88%B6"><span class="nav-number">1.2.2.</span> <span class="nav-text">PyTorch显存分配机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pytorch%E9%87%8A%E6%94%BE%E6%9C%BA%E5%88%B6"><span class="nav-number">1.2.3.</span> <span class="nav-text">Pytorch释放机制</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E7%A4%BA%E4%BE%8B"><span class="nav-number">1.3.</span> <span class="nav-text">训练过程示例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89"><span class="nav-number">1.3.1.</span> <span class="nav-text">模型定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B"><span class="nav-number">1.3.2.</span> <span class="nav-text">前向计算过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%87%E7%A8%8B"><span class="nav-number">1.3.3.</span> <span class="nav-text">反向传播过程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0"><span class="nav-number">1.4.</span> <span class="nav-text">参数更新</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%8A%82%E7%9C%81%E6%98%BE%E5%AD%98"><span class="nav-number">1.5.</span> <span class="nav-text">节省显存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8D%E5%90%8C%E6%A1%86%E6%9E%B6%E7%9A%84%E4%B8%8A%E4%B8%8B%E6%96%87"><span class="nav-number">1.6.</span> <span class="nav-text">不同框架的上下文</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Coder4nlp</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/02/09/Pytorch%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Coder4nlp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Pytorch显存分析 | Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Pytorch显存分析
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-02-09 13:11:54 / Modified: 13:13:12" itemprop="dateCreated datePublished" datetime="2023-02-09T13:11:54+08:00">2023-02-09</time>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="Pytorch显存分析"><a href="#Pytorch显存分析" class="headerlink" title="Pytorch显存分析"></a>Pytorch显存分析</h1><p>在深度学习中，常见的错误是CUDA out of memory，这是因为显存放不下那么多模型参数和中间变量。通过及时清空中间变量，优化代码，减少batch等能够减少显存溢出。</p>
<span id="more"></span>

<p><a target="_blank" rel="noopener" href="https://pytorch.org/">PyTorch</a>在进行深度学习训练的时候，有4大部分的显存开销，分别是</p>
<ul>
<li>模型参数(parameters)</li>
<li>模型参数的梯度(gradients)</li>
<li>优化器状态(optimizer states)</li>
<li>中间激活值(intermediate activations) 或者叫中间结果(intermediate results)。</li>
</ul>
<p>其中activation占绝对大头，50%以上(有些地方说训练过程中占用显存最大的是计算图，把activation看作计算图的节点的话，也没错)优化器占的大小只是weight的2倍(Adam),SGD啥的话甚至1倍。</p>
<h2 id="深度学习训练过程"><a href="#深度学习训练过程" class="headerlink" title="深度学习训练过程"></a>深度学习训练过程</h2><p>模型定义：定义了模型的网络结构，产生模型参数；</p>
<p>while(你想训练):</p>
<ol>
<li>前向传播：执行模型的前向传播，产生中间激活值；</li>
<li>后向传播：执行模型的后向传播，产生梯度；</li>
<li>梯度更新：执行模型参数的更新，第一次执行的时候产生优化器状态。</li>
</ol>
<p>在模型定义完之后，1~3循环执行。</p>
<h2 id="Torch机制"><a href="#Torch机制" class="headerlink" title="Torch机制"></a>Torch机制</h2><h3 id="CUDA-context-开销"><a href="#CUDA-context-开销" class="headerlink" title="CUDA context 开销"></a>CUDA context 开销</h3><p>在第一次执行CUDA操作时，使用GPU所需要创建维护设备间工作的一些相关信息。</p>
<blockquote>
<p>PyTorch will <strong>create the CUDA context in the very first CUDA operation</strong>, which can use ~600-1000MB of GPU memory depending on the CUDA version as well as the used device.<br>PyTorch itself will allocate the needed memory and will use an internal cache mechanism. You can read more about it <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/cuda.html">here</a>.</p>
</blockquote>
<blockquote>
<ol>
<li>You could rebuild PyTorch and remove libraries shipping with device code, such as cuDNN. While this would yield a performance hit, no cuDNN kernels will be loaded and stored in the CUDA context.</li>
<li>Yes, you can use <code>PYTORCH_NO_CUDA_MEMORY_CACHING=1</code> to disable the cache.</li>
<li>PyTorch needs to load its own kernels as well as device code from other libs (cuDNN, cublas, NCCL, etc.), which might not be the case for PyCUDA. It might be possible to share some driver-related code in the context, but I don’t know how much memory savings would be expected and haven’t experimented with it.</li>
</ol>
</blockquote>
<p>显存的值跟CUDA的版本，Pytorch的版本以及所使用的设备都是有关系的。所以只有把任何小的张量放到GPU显存，那么至少会占用1000M左右显存，这部分显存是cuda运行时必须占掉的显存。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">a = torch.tensor([<span class="number">1.0</span>]).cuda()</span><br><span class="line">time.sleep(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<p>torch.tensor([1.0]).cuda()分配一个shape为1的tensor，<strong>理论上应该只有4bytes，但用allocated看会有512bytes</strong>。</p>
<p><strong>在Docker容器中</strong>运行，使用不同的显卡测试，如下：</p>
<blockquote>
<p><strong>在1080显卡，pytorch 1.10.0+cu113，cuda11.4中测试，nvidia-smi初始显存为5266MiB，运行代码后为6171MiB。使用6171M-5266M&#x3D;905MiB。</strong></p>
<p><strong>在RTX2080  Ti上，pytorch 1.10.0+cu113，cuda11.4中测试，初始为3M,，运行后为1476M，使用1476M-3M&#x3D;1473M。</strong></p>
<p><strong>在3090上，pytorch 1.10.0+cu113，cuda11.4中测试，初始为8101M,，运行后为10290M，使用10290M-8101M&#x3D;2189M。</strong></p>
</blockquote>
<p><strong>在win10上运行</strong>，</p>
<blockquote>
<p><strong>在3090上，pytorch 1.10.1+cu111，cuda11.6中测试，初始为375M,，运行后为617M，使用617M-375M&#x3D;242M。</strong></p>
</blockquote>
<h3 id="PyTorch显存分配机制"><a href="#PyTorch显存分配机制" class="headerlink" title="PyTorch显存分配机制"></a>PyTorch显存分配机制</h3><p>在PyTorch中，显存是按页为单位进行分配的，这可能是CUDA设备的限制。以上面的代码为例，就算我们只想申请4字节的显存，Pytorch也会先向CUDA设备申请2MB的显存到自己的cache区中，然后pytorch再为我们分配512字节或者1024字节的空间。这个在使用torch.cuda.memory_allocated()的时候可以看出来512字节；用torch.cuda.memory_cached()（torch.cuda.memory_cached has been renamed to toch.cuda.memory_reserved)可以看出向CUDA申请的2MB。</p>
<p>也就是说，PyTorch Allocated memory使用的是PyTorch reserved Memory里的显存，PyTorch reserved Memory则用的是GPU的显存。</p>
<p>实际上，用nvidia-smi或者gpustat来看Pytorch程序的显存占用不是很合适的。 因为Pytorch的机制是使用缓存分配器来管理缓存分配的(因为这样速度快), 但是在缓存分配器的机制下, <strong>一个Tensor就算被释放了，进程也不会把空闲出来的显存还给GPU，而是等待下一个Tensor来填入这一片被释放的空间(即只要一个Tensor对象在后续不会再被使用，那么PyTorch就会自动回收该Tensor所占用的显存，并以缓冲区的形式继续占用显存，所以在nvidia-smi&#x2F;gpustat中看到的显存并没有减少)</strong></p>
<p>   <em>即用nvidia-smi&#x2F;gpustat看到的其实是pytorch缓存区&#x2F;缓存分配器的情况</em></p>
<p> 要是实在看缓冲区不爽的话，也可以用torch.cuda.empty_cache()把它归零，或者加一个环境变量PYTORCH_NO_CUDA_MEMORY_CACHING&#x3D;1，但是程序速度会变慢哦(试过在一个实验里慢了三倍)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"> </span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line"><span class="comment"># 定义两个tensor</span></span><br><span class="line">tensor1 = torch.randn(<span class="number">120</span>, <span class="number">3</span>, <span class="number">512</span>, <span class="number">512</span>).<span class="built_in">float</span>().to(device)  </span><br><span class="line"><span class="comment"># 120*3*512*512*4/1000/1000 = 377.48M</span></span><br><span class="line">tensor2 = torch.randn(<span class="number">80</span>, <span class="number">3</span>, <span class="number">512</span>, <span class="number">512</span>).<span class="built_in">float</span>().to(device)  </span><br><span class="line"><span class="comment"># 80*3*512*512*4/1000/1000 = 251.64M</span></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.memory_allocated()/ <span class="number">1000</span> / <span class="number">1000</span>) <span class="comment"># 629.15MiB</span></span><br><span class="line"><span class="comment"># 初始5308MiB,之后6811MiB，使用6811-5308=1423MiB</span></span><br><span class="line"><span class="comment"># 然后释放，使用del也可以进行释放</span></span><br><span class="line">dummy_tensor_4 = dummy_tensor_4.cpu()</span><br><span class="line">dummy_tensor_5 = dummy_tensor_5.cpu()</span><br><span class="line"><span class="comment"># 这里虽然将上面的显存释放了，但是我们通过Nvidia-smi命令看到显存依然在占用</span></span><br><span class="line">torch.cuda.empty_cache()</span><br><span class="line"><span class="comment"># 只有执行完上面这句，显存才会在Nvidia-smi中释放</span></span><br></pre></td></tr></table></figure>

<p>进程不需要重新向GPU申请显存了，运行速度会快很多，有什么坏处？他不能准确地给出某一个时间点具体的Tensor占用的显存，nvidia-smi<strong>显示的</strong>而是<strong>已经分配到的显存和context开销之和,</strong> <strong>也就是reserved_memory和torch context显存之和</strong>。</p>
<p>   这也是令很多人在使用PyTorch时对显存占用感到困惑的罪魁祸首！！！</p>
<h3 id="Pytorch释放机制"><a href="#Pytorch释放机制" class="headerlink" title="Pytorch释放机制"></a>Pytorch释放机制</h3><p>简单总结一下，就是在PyTorch中，只要一个Tensor对象在后续不会再被使用，那么PyTorch就会自动回收该Tensor所占用的显存，并以缓冲区的形式继续占用显存。</p>
<p>要是实在看缓冲区不爽的话，也可以用torch.cuda.empty_cache()把它减少，或者加一个环境变量PYTORCH_NO_CUDA_MEMORY_CACHING&#x3D;1，但是程序速度会变慢哦，(曾经试过，在我的一个实验里慢了3倍)。</p>
<p>然后就是选取合适的batch_size就可以开始训练啦，怎么估算多少batch_size可以刚好把咱们的显存占满呢，这里可以通过公式估算一下，（total_gpu_mem - model_used_mem）&#x2F;&#x2F; activation_mem</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 参数占用显存大小</span><br><span class="line">params_mem = num_params * (2 if fp16_enabled else 4)</span><br><span class="line">// 梯度占用显存大小</span><br><span class="line">gradients_mem = num_params * (2 if fp16_enabled else 4)</span><br><span class="line">// 优化器占用显存大小</span><br><span class="line">optimizer_mem = num_params * (16 if fp16_enabled else 8)</span><br></pre></td></tr></table></figure>

<h2 id="训练过程示例"><a href="#训练过程示例" class="headerlink" title="训练过程示例"></a>训练过程示例</h2><h3 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">model = torch.nn.Linear(<span class="number">1024</span>,<span class="number">1024</span>, bias=<span class="literal">False</span>).cuda() </span><br><span class="line"><span class="built_in">print</span>(torch.cuda.memory_allocated()) </span><br></pre></td></tr></table></figure>

<p>输出4194304，刚好等于1024×1024×4</p>
<h3 id="前向计算过程"><a href="#前向计算过程" class="headerlink" title="前向计算过程"></a>前向计算过程</h3><p>结论：显存增加等于每一层模型产生的结果的显存之和，且跟batch_size成正比。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.tensor([<span class="number">1.0</span>]*<span class="number">1024</span>).cuda() </span><br><span class="line"><span class="comment"># shape = (1024)  数据占用显存1024*4=4096</span></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.memory_allocated()) </span><br><span class="line"><span class="comment"># 4194304(模型占用显存) + 4096（中间结果）=4198400</span></span><br><span class="line">outputs = model(inputs) <span class="comment"># outputs shape(1024) # 中间结果（1024*4=4096）</span></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.memory_allocated()) <span class="comment"># </span></span><br><span class="line"><span class="comment">#总共分配的4194304(模型参数占用) + 4096（输入） +4096（中间结果）　=4202496</span></span><br></pre></td></tr></table></figure>

<h3 id="反向传播过程"><a href="#反向传播过程" class="headerlink" title="反向传播过程"></a>反向传播过程</h3><p>后向传播会将模型的中间激活值给消耗并释放掉掉，并为每一个模型中的参数计算其对应的梯度。在第一次执行的时候，会为模型参数分配对应的用来存储梯度的空间。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">loss = <span class="built_in">sum</span>(outputs) <span class="comment">#  + 512 (torch allocate分配最小单位)</span></span><br><span class="line">temp = torch.cuda.memory_allocated() <span class="comment"># 4202496+512=4203008</span></span><br><span class="line">loss.backward() <span class="comment"># + 4194304（模型参数的梯度，1024*1024*4）</span></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.memory_allocated()) <span class="comment"># 4203008+4194304=8397312</span></span><br></pre></td></tr></table></figure>

<p>第一次执行时显存增加: 4194304字节 - 激活值大小</p>
<p>第二次以后执行显存减少: 激活值大小</p>
<p>Note: 由于这个中间值被赋给了outputs，所以后面在后向传播的时候会发现，这个outputs的显存没有被释放掉。但是当层数变深的时候，就能明显看到变化了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 模型初始化</span></span><br><span class="line">linear1 = torch.nn.Linear(<span class="number">1024</span>,<span class="number">1024</span>, bias=<span class="literal">False</span>).cuda() <span class="comment"># + 4194304</span></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.memory_allocated())</span><br><span class="line">linear2 = torch.nn.Linear(<span class="number">1024</span>, <span class="number">1</span>, bias=<span class="literal">False</span>).cuda() <span class="comment"># + 4096</span></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.memory_allocated())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入定义</span></span><br><span class="line">inputs = torch.tensor([[<span class="number">1.0</span>]*<span class="number">1024</span>]*<span class="number">1024</span>).cuda() <span class="comment"># shape = (1024,1024) # + 4194304</span></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.memory_allocated())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播，正常来说，这里增加的中间结果有linear1的结果4194304，</span></span><br><span class="line"><span class="comment"># linear2的结果4096以及sum的结果512</span></span><br><span class="line"><span class="comment"># 实际上增加的是4194304 + 512！！！难道是sum和linear2操作合并了？？？？？？？</span></span><br><span class="line">loss = <span class="built_in">sum</span>(linear2(linear1(inputs))) </span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.memory_allocated())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 后向传播，中间值的显存释放，增加的显存是每个参数的梯度，linear1：4194304，linear2:4096</span></span><br><span class="line">loss.backward() <span class="comment"># memory - （4194304）+ 4194304 + 4096  </span></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.memory_allocated()) <span class="comment"># 由于释放一部分，再增加一部分，实际上增加4096</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 再来一次~</span></span><br><span class="line">loss = <span class="built_in">sum</span>(linear2(linear1(inputs))) <span class="comment"># shape = (1) # memory + 4194304  (512没了，因为loss的ref还在)</span></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.memory_allocated())</span><br><span class="line">loss.backward() <span class="comment"># memory - 4194304</span></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.memory_allocated())</span><br></pre></td></tr></table></figure>

<blockquote>
<p>讨论：从原理上讲，element-wise fusion 是可以省的呀，z&#x3D;linear1(x), w&#x3D;linear2(z), loss&#x3D;sum(w)，那么 dloss&#x2F;dw&#x3D;ones_like(w)，dloss&#x2F;dz&#x3D;linear2.weight.T @ dloss&#x2F;dw，整个过程中是不需要保存w的。但 pytorch 是否做了 fusion，就需要做 profile 来分析了。</p>
</blockquote>
<h2 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer.step() <span class="comment"># 第一次增加8388608，第二次就不增不减了哦</span></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.max_memory_allocated()) <span class="comment"># = torch.memory_allocated + 8388608</span></span><br></pre></td></tr></table></figure>

<p>第一次执行时，会为每一个参数初始化<strong>其优化器状态</strong>，对于这里的AdamW而言，每一个参数需要4*2&#x3D;8个字节。第二次开始，不会再额外分配显存。</p>
<p>显存开销:</p>
<p>第一次: 增加8388608字节</p>
<p>第二次及以后: 无增减</p>
<p>由于计算机计算的特性，有一些计算操作在计算过程中是会带来额外的显存开销的。但是这种开销在torch.memory_allocated中是不能被察觉的。比如在AdamW在进行某一层的更新的时候，会带来2倍该层参数量大小的临时额外开销。这个在max_memory_allocated中可以看到。在本例中就是8388608字节。</p>
<h2 id="节省显存"><a href="#节省显存" class="headerlink" title="节省显存"></a>节省显存</h2><p>即使只创建了个只有一个元素的tensor,它照样会显示用了几百兆的显存。</p>
<p><strong>除非在编译的时候去掉一些cuda的功能模块。自己编译pytorch，能降低不少cuda context的显存开销。</strong></p>
<p>在分析PyTorch的显存时候，一定要使用torch.cuda里的显存分析函数，我用的最多的是torch.cuda.memory_allocated()和torch.cuda.max_memory_allocated()，前者可以精准地反馈当前进程中Torch.Tensor所占用的GPU显存，后者则可以告诉我们到调用函数为止所达到的最大的显存占用字节数。还有像torch.cuda.memory_reserved()这样的函数则是查看当前进程所分配的显存缓冲区是多少的。</p>
<p>nvidia-smi 里看到的占用</p>
<p>&#x3D; CUDA 上下文 + pytorch 缓存区</p>
<p>&#x3D; CUDA 上下文 + 未使用缓存 + 已使用缓存</p>
<ul>
<li>Pytorch 内部有自己的缓存管理系统，能够加速显存分配。</li>
<li>使用 torch.cuda.memory_allocated() 可以看到当前模型实际占用的显存。</li>
<li>使用 torch.cuda.memory_reserved() 可以看到Pytorch总共占用的显存</li>
<li>使用 torch.cuda.empty_cache() 清空未使用的缓存，但是已经使用的是不能释放的。</li>
</ul>
<p>只有一种情况需要使用 torch.cuda.empty_cache()，就是当你想要释放缓存以便让其他人也可以一起使用当前显卡，否则不需要调用这个方法。</p>
<p><code>reserved</code> memory contains the <code>allocated</code> and cached memory.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">a = torch.zeros((1024, 1024)).cuda() # 实际大小4M=1024*1024*4</span><br><span class="line">torch.cuda.memory_allocated() / 1024 / 1024  # 4.0 张量占用</span><br><span class="line">torch.cuda.memory_reserved() / 1024 / 1024  # 20.0  缓存区</span><br><span class="line"># nvidia-smi 上看占用1073M</span><br><span class="line"></span><br><span class="line"># 删除临时变量</span><br><span class="line">del a</span><br><span class="line">torch.cuda.memory_allocated() / 1024 / 1024  # 0.0 张量占用</span><br><span class="line">torch.cuda.memory_reserved() / 1024 / 1024  # 20.0  缓存区</span><br><span class="line"># 可以看到缓存区仍然占用了20M，nvidia-smi里也保持1073M的占用</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 清空缓存区</span><br><span class="line">torch.cuda.empty_cache()</span><br><span class="line">torch.cuda.memory_allocated() / 1024 / 1024  # 0.0 张量占用</span><br><span class="line">torch.cuda.memory_reserved() / 1024 / 1024  # 0.0  缓存区</span><br><span class="line"># nvidia-smi里的占用回到了1053M</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">loss = loss_fn(out, batch_gt_tensor)</span><br><span class="line">loss.backward()</span><br><span class="line"># added lines</span><br><span class="line">del batch_input_data</span><br><span class="line">del batch_gt_data</span><br><span class="line">torch.cuda.empty_cache()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>

<h2 id="不同框架的上下文"><a href="#不同框架的上下文" class="headerlink" title="不同框架的上下文"></a>不同框架的上下文</h2><p> creating an empty tensor in pytorch costs 800MB of GPU memory,</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import torch; torch.zeros((), device=&#x27;cuda&#x27;)</span><br></pre></td></tr></table></figure>

<p>or 170MB to run something similar in tensorflow:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf; tf.constant([])</span><br></pre></td></tr></table></figure>

<p>or 175MB in cupy</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import cupy; cupy.array([])</span><br></pre></td></tr></table></figure>

<p>or 170MB in pycuda</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import pycuda.driver as cuda; import pycuda.autoinit; cuda.mem_alloc(1)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>the context occupying memory is by design of the CUDA &#x2F; NVIDIA software stack. It’s non-actionable because there is nothing we can technically do about it.</p>
<p>You could reduce the CUDA context size by removing kernels e.g. via dropping libraries such as MAGMA or cuDNN, if these are not used.<br>Besides that I’m not aware of reducing the operator set in the framework itself.</p>
<p>it’s because of the large number of kernels. There isn’t an easy way to avoid loading them: that would require separating some kernels into separate shared libraries. Building multiple libraries would be a big engineering challenge because of inter-dependencies between the libraries (i.e the main library depends on kernels in secondary libraries; those libraries depend on code in the main library).</p>
<p>Another strategy is to compile the kernels on-the-fly when needed. I think some of the other packages use that approach. That can slow down the initial use a lot. We could pre-compile and ship the most commonly used kernels and only compile the other kernels on-demand. Again, a big engineering challenge.</p>
</blockquote>
<p>CUDA11.7版本，可以设置 <code>CUDA_MODULE_LOADING=LAZY</code></p>
<blockquote>
<p>The initialization will create the CUDA context loading all kernels for your GPU architecture and is thus expected. The size of the context depends on the CUDA version, your GPU, the number of kernels in loaded CUDA libs as well as native PyTorch kernels.<br>You could update to CUDA 11.7 and enable lazy module loading via <code>CUDA_MODULE_LOADING=LAZY</code> which will load kernels only if they are needed and will thus reduce the context size.</p>
<p>Yes, the env variable takes effect in CUDA 11.7+ and won’t change anything in CUDA 10.2. The context size reduction between PyTorch 1.9.0 and 1.12.0 would come from loading some modules lazily via the framework directly as well as a reduction in the number of kernels.</p>
<p>延迟加载:延迟内核从主机加载到GPU，直到内核被调用。这也只加载已使用的内核，这可能会显著节省设备端内存。这也将加载延迟从应用程序的开始推迟到第一次调用内核的时候——总的二进制加载延迟通常会显著减少，但也会转移到应用程序的后期。<br>要启用这个特性，在启动进程之前设置环境变量CUDA_MODULE_LOADING&#x3D;LAZY。<br>注意，这个特性只兼容CUDA版本&gt;&#x3D; 11.7编译的库。</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/424512257">https://zhuanlan.zhihu.com/p/424512257</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/486360176">https://zhuanlan.zhihu.com/p/486360176</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/486360176%EF%BC%88%E8%B5%9E%EF%BC%81%EF%BC%89">https://zhuanlan.zhihu.com/p/486360176（赞！）</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/master/c10/cuda/CUDACachingAllocator.cpp">https://github.com/pytorch/pytorch/blob/master/c10/cuda/CUDACachingAllocator.cpp</a></p>
<p><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/how-do-i-create-torch-tensor-without-any-wasted-storage-space-baggage/131134/10">https://discuss.pytorch.org/t/how-do-i-create-torch-tensor-without-any-wasted-storage-space-baggage/131134/10</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/devilmaycry812839668/p/15571390.html">https://www.cnblogs.com/devilmaycry812839668/p/15571390.html</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/issues/20532">https://github.com/pytorch/pytorch/issues/20532</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__CTX.html#group__CUDA__CTX_1g0651954dfb9788173e60a9af7201e65a">https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__CTX.html#group__CUDA__CTX_1g0651954dfb9788173e60a9af7201e65a</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/issues/55277">https://github.com/pytorch/pytorch/issues/55277</a></p>
<p><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/reduce-gpu-memory-blocked-by-context/142587">https://discuss.pytorch.org/t/reduce-gpu-memory-blocked-by-context/142587</a></p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/02/01/ERNIE-layout/" rel="prev" title="ERNIE-layout">
                  <i class="fa fa-chevron-left"></i> ERNIE-layout
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/02/15/chat-gpt/" rel="next" title="chat_gpt">
                  chat_gpt <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Coder4nlp</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
