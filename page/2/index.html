<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Noto+Serif+SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.14.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.json","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="Coder4nlp&#39;s Blog">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="Coder4nlp&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Coder4nlp">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Coder4nlp's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Coder4nlp's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-following"><a href="/following/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>following</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Coder4nlp</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/15/deepspeed%E4%BB%8B%E7%BB%8D/GLM-main/config/config_block_10B_longer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Coder4nlp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/15/deepspeed%E4%BB%8B%E7%BB%8D/GLM-main/config/config_block_10B_longer/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-15 18:24:37" itemprop="dateCreated datePublished" datetime="2023-03-15T18:24:37+08:00">2023-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-14 15:54:48" itemprop="dateModified" datetime="2023-03-14T15:54:48+08:00">2023-03-14</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          {"train_micro_batch_size_per_gpu":8,"gradient_accumulation_steps":1,"steps_per_print":50,"gradient_clipping":1,"zero_optimization":{"stage":2,"contiguous_gradients":true,"overlap_comm":true,"reduce_scatter":true,"reduce_bucket_size":50000000,"allgather_bucket_size":500000000},"zero_allow_untested_optimizer":true,"fp16":{"enabled":true,"loss_scale":0,"loss_scale_window":1000,"hysteresis":2,"min_loss_scale":1},"optimizer":{"type":"Adam","params":{"lr":0.00003,"betas":[0.9,0.95],"eps":1e-8,"weight_decay":0.1}},"activation_checkpointing":{"partition_activations":false,"contiguous_memory_optimization":false},"wall_clock_breakdown":false}
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/15/deepspeed%E4%BB%8B%E7%BB%8D/GLM-main/config/config_block_base/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Coder4nlp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/15/deepspeed%E4%BB%8B%E7%BB%8D/GLM-main/config/config_block_base/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-15 18:24:37" itemprop="dateCreated datePublished" datetime="2023-03-15T18:24:37+08:00">2023-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-14 15:54:48" itemprop="dateModified" datetime="2023-03-14T15:54:48+08:00">2023-03-14</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          {"train_micro_batch_size_per_gpu":16,"gradient_accumulation_steps":1,"steps_per_print":100,"gradient_clipping":1,"fp16":{"enabled":true,"loss_scale":0,"loss_scale_window":1000,"hysteresis":2,"min_loss_scale":1},"optimizer":{"type":"Adam","params":{"lr":0.0004,"weight_decay":0.1,"betas":[0.9,0.98],"eps":0.000001}},"activation_checkpointing":{"partition_activations":false,"contiguous_memory_optimization":false},"wall_clock_breakdown":false}
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/15/deepspeed%E4%BB%8B%E7%BB%8D/GLM-main/README/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Coder4nlp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/15/deepspeed%E4%BB%8B%E7%BB%8D/GLM-main/README/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-15 18:24:37" itemprop="dateCreated datePublished" datetime="2023-03-15T18:24:37+08:00">2023-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-14 15:54:48" itemprop="dateModified" datetime="2023-03-14T15:54:48+08:00">2023-03-14</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="GLM"><a href="#GLM" class="headerlink" title="GLM"></a>GLM</h1><p>GLM is a General Language Model pretrained with an autoregressive blank-filling objective and can be finetuned on<br>various natural language understanding and generation tasks.</p>
<p>Please refer to our paper for a detailed description of GLM:</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.10360">GLM: General Language Model Pretraining with Autoregressive Blank Infilling</a> (ACL 2022)</p>
<p>Zhengxiao Du*, Yujie Qian*, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang (*: equal contribution)</p>
<p><strong>News: We release <a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM-6B">ChatGLM-6B</a>, an open pre-trained language model with 6 billion parameters optimized for Chinese QA and dialogue based on the GLM framework.</strong></p>
<h2 id="Pretrained-Models"><a href="#Pretrained-Models" class="headerlink" title="Pretrained Models"></a>Pretrained Models</h2><p>You can download the pretrained models used in the paper<br>from <a target="_blank" rel="noopener" href="https://mailstsinghuaeducn-my.sharepoint.com/:f:/g/personal/zx-du20_mails_tsinghua_edu_cn/En6zA7_utRxHptKWZoDMO14Bkfj3uGRpslYkNvMPdGOmow?e=G0lGSc">OneDrive</a><br>or <a target="_blank" rel="noopener" href="https://cloud.tsinghua.edu.cn/d/13f5b03da9594e5490c4">Tsinghua-Cloud</a>.</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Params</th>
<th>Language</th>
<th>Corpus</th>
<th>Objective</th>
<th>File</th>
<th>Config</th>
</tr>
</thead>
<tbody><tr>
<td>GLM-Base</td>
<td>110M</td>
<td>English</td>
<td>Wiki+Book</td>
<td>Token</td>
<td>glm-base-blank.tar.bz2</td>
<td>model_blocklm_base.sh</td>
</tr>
<tr>
<td>GLM-Large</td>
<td>335M</td>
<td>English</td>
<td>Wiki+Book</td>
<td>Token</td>
<td>glm-large-blank.tar.bz2</td>
<td>model_blocklm_large.sh</td>
</tr>
<tr>
<td>GLM-Large-Chinese</td>
<td>335M</td>
<td>Chinese</td>
<td><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S2666651021000152">WuDaoCorpora</a></td>
<td>Token+Sent+Doc</td>
<td>glm-large-chinese.tar.bz2</td>
<td>model_blocklm_large_chinese.sh</td>
</tr>
<tr>
<td>GLM-Doc</td>
<td>335M</td>
<td>English</td>
<td>Wiki+Book</td>
<td>Token+Doc</td>
<td>glm-large-generation.tar.bz2</td>
<td>model_blocklm_large_generation.sh</td>
</tr>
<tr>
<td>GLM-410M</td>
<td>410M</td>
<td>English</td>
<td>Wiki+Book</td>
<td>Token+Doc</td>
<td>glm-1.25-generation.tar.bz2</td>
<td>model_blocklm_1.25_generation.sh</td>
</tr>
<tr>
<td>GLM-515M</td>
<td>515M</td>
<td>English</td>
<td>Wiki+Book</td>
<td>Token+Doc</td>
<td>glm-1.5-generation.tar.bz2</td>
<td>model_blocklm_1.5_generation.sh</td>
</tr>
<tr>
<td>GLM-RoBERTa</td>
<td>335M</td>
<td>English</td>
<td>RoBERTa</td>
<td>Token</td>
<td>glm-roberta-large-blank.tar.bz2</td>
<td>model_blocklm_roberta_large.sh</td>
</tr>
<tr>
<td>GLM-2B</td>
<td>2B</td>
<td>English</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.00027">Pile</a></td>
<td>Token+Sent+Doc</td>
<td>glm-2b.tar.bz2</td>
<td>model_blocklm_2B.sh</td>
</tr>
<tr>
<td>GLM-10B</td>
<td>10B</td>
<td>English</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.00027">Pile</a></td>
<td>Token+Sent+Doc</td>
<td><a target="_blank" rel="noopener" href="https://lfs.aminer.cn/misc/cogview/glm-10b-1024.zip">Download</a></td>
<td>model_blocklm_10B.sh</td>
</tr>
<tr>
<td>GLM-10B-Chinese</td>
<td>10B</td>
<td>Chinese</td>
<td><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S2666651021000152">WuDaoCorpora</a></td>
<td>Token+Sent+Doc</td>
<td><a target="_blank" rel="noopener" href="https://lfs.aminer.cn/misc/cogview/glm-10b-chinese.zip">Download</a></td>
<td>model_blocklm_10B_chinese.sh</td>
</tr>
</tbody></table>
<p>Unzip the downloaded file into a local folder and set <code>CHECKPOINT_PATH</code> in the corresponding scripts to the folder path.</p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><h3 id="SuperGLUE"><a href="#SuperGLUE" class="headerlink" title="SuperGLUE"></a><a target="_blank" rel="noopener" href="https://super.gluebenchmark.com/">SuperGLUE</a></h3><p>dev set, single model, single-task finetuning</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>COPA</th>
<th>WSC</th>
<th>RTE</th>
<th>WiC</th>
<th>CB</th>
<th>MultiRC</th>
<th>BoolQ</th>
<th>ReCoRD</th>
</tr>
</thead>
<tbody><tr>
<td>GLM-10B</td>
<td>98.0</td>
<td>95.2</td>
<td>93.1</td>
<td>75.7</td>
<td>98.7&#x2F;98.2</td>
<td>88.1&#x2F;63.3</td>
<td>88.7</td>
<td>94.4&#x2F;94.0</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/microsoft/DeBERTa/tree/master/experiments/superglue">DeBERTa-XXLarge-v2</a></td>
<td>97.0</td>
<td>-</td>
<td>93.5</td>
<td>-</td>
<td>-</td>
<td>87.8&#x2F;63.6</td>
<td>88.3</td>
<td>94.1&#x2F;93.7</td>
</tr>
</tbody></table>
<h3 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h3><p><a target="_blank" rel="noopener" href="https://github.com/abisee/cnn-dailymail">CNN&#x2F;Daily Mail</a> (test set, no additional data used)</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>ROUGE-1</th>
<th>ROUGE-2</th>
<th>ROUGE-L</th>
</tr>
</thead>
<tbody><tr>
<td>GLM-10B</td>
<td><strong>44.7</strong></td>
<td>21.4</td>
<td><strong>41.4</strong></td>
</tr>
<tr>
<td>T5-11B</td>
<td>43.5</td>
<td><strong>21.6</strong></td>
<td>40.7</td>
</tr>
<tr>
<td>PEGASUS-Large</td>
<td>44.2</td>
<td>21.5</td>
<td><strong>41.4</strong></td>
</tr>
<tr>
<td>BART-Large</td>
<td>44.2</td>
<td>21.3</td>
<td>40.9</td>
</tr>
</tbody></table>
<p><a target="_blank" rel="noopener" href="https://github.com/EdinburghNLP/XSum">XSum</a> (test set, no additional data used)</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>ROUGE-1</th>
<th>ROUGE-2</th>
<th>ROUGE-L</th>
</tr>
</thead>
<tbody><tr>
<td>GLM-10B</td>
<td><strong>48.9</strong></td>
<td><strong>25.7</strong></td>
<td><strong>40.4</strong></td>
</tr>
<tr>
<td>PEGASUS-Large</td>
<td>47.2</td>
<td>24.6</td>
<td>39.3</td>
</tr>
<tr>
<td>BART-Large</td>
<td>45.1</td>
<td>22.3</td>
<td>37.3</td>
</tr>
</tbody></table>
<h3 id="Language-Modeling"><a href="#Language-Modeling" class="headerlink" title="Language Modeling"></a>Language Modeling</h3><p>test set, zero-shot</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>LAMBADA (accuracy)</th>
<th>Wikitext103 (perplexity)</th>
</tr>
</thead>
<tbody><tr>
<td>GLM-10B (bi)</td>
<td>72.35</td>
<td>11.33</td>
</tr>
<tr>
<td>GLM-10B (uni)</td>
<td>67.18</td>
<td>12.22</td>
</tr>
<tr>
<td>GPT-2</td>
<td>52.66</td>
<td>17.48</td>
</tr>
<tr>
<td>Megatron-LM (8.3B)</td>
<td>66.51</td>
<td>10.81</td>
</tr>
<tr>
<td>Turing-NLG</td>
<td>67.98</td>
<td>10.21</td>
</tr>
</tbody></table>
<h2 id="Get-Started"><a href="#Get-Started" class="headerlink" title="Get Started"></a>Get Started</h2><h3 id="Hugging-Face-Hub"><a href="#Hugging-Face-Hub" class="headerlink" title="Hugging Face Hub"></a>Hugging Face Hub</h3><p>You can access GLM models via HuggingFace Hub. Please<br>install <code>transformers&gt;=4.23.1</code> and find all the available models <a target="_blank" rel="noopener" href="https://huggingface.co/models?filter=glm,thudm">here</a>.</p>
<h4 id="Generation"><a href="#Generation" class="headerlink" title="Generation"></a>Generation</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForSeq2SeqLM</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;THUDM/glm-10b&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(<span class="string">&quot;THUDM/glm-10b&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = model.half().cuda()</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inference</span></span><br><span class="line">inputs = tokenizer(<span class="string">&quot;Ng is an adjunct professor at [MASK] (formerly associate professor and Director of its Stanford AI Lab or SAIL ). Also a pioneer in online education, Ng co-founded Coursera and deeplearning.ai.&quot;</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">inputs = tokenizer.build_inputs_for_generation(inputs, max_gen_length=<span class="number">512</span>)</span><br><span class="line">inputs = inputs.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">outputs = model.generate(**inputs, max_length=<span class="number">512</span>, eos_token_id=tokenizer.eop_token_id)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(outputs[<span class="number">0</span>].tolist()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line">inputs = tokenizer(</span><br><span class="line">    [<span class="string">&quot;Tsinghua University is located in [MASK].&quot;</span>, <span class="string">&quot;One minus one equals zero, is it correct? Answer: [MASK]&quot;</span>],</span><br><span class="line">    return_tensors=<span class="string">&quot;pt&quot;</span>, padding=<span class="literal">True</span>)</span><br><span class="line">inputs = tokenizer.build_inputs_for_generation(inputs, targets=[<span class="string">&quot;Beijing&quot;</span>, <span class="string">&quot;No&quot;</span>], max_gen_length=<span class="number">8</span>, padding=<span class="literal">False</span>)</span><br><span class="line">inputs = inputs.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">outputs = model(**inputs)</span><br><span class="line">loss = outputs.loss</span><br><span class="line">logits = outputs.logits</span><br></pre></td></tr></table></figure>
<h4 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForMultipleChoice</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;THUDM/glm-10b&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModelForMultipleChoice.from_pretrained(<span class="string">&quot;THUDM/glm-10b&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = model.half().cuda()</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">inputs = tokenizer([<span class="string">&quot;Tsinghua University is located in [MASK].&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;One minus one equals zero, is it correct? Answer: [MASK]&quot;</span>], return_tensors=<span class="string">&quot;pt&quot;</span>, padding=<span class="literal">True</span>)</span><br><span class="line">choices = [[<span class="string">&quot;Beijing&quot;</span>, <span class="string">&quot;Shanghai&quot;</span>], [<span class="string">&quot;Yes&quot;</span>, <span class="string">&quot;No&quot;</span>]]</span><br><span class="line">inputs = tokenizer.build_inputs_for_multiple_choice(inputs, choices)</span><br><span class="line">inputs = inputs.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">outputs = model(**inputs)</span><br><span class="line">logits = outputs.logits</span><br></pre></td></tr></table></figure>
<p>You can also convert the finetuned checkpoints with <code>scripts/convert_glm_checkpoint_to_transformers.py</code>. </p>
<h3 id="Docker-Image"><a href="#Docker-Image" class="headerlink" title="Docker Image"></a>Docker Image</h3><p>We prepare two docker images based on CUDA 10.2 and CUDA 11.2. You can pull the pre-built images from Docker Hub and run<br>with docker v19.03+</p>
  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --gpus all --rm -it --ipc=host zxdu20/glm-cuda102</span><br></pre></td></tr></table></figure>

<p>or replace <code>glm-cuda102</code> with <code>glm-cuda112</code>.</p>
<p>You can also modify the image according to your requirements in <a href="docker/cuda102.dockerfile">docker&#x2F;cuda102.dockerfile</a><br>and build the image yourself</p>
  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -f cuda102.dockerfile . -t glm-cuda102</span><br></pre></td></tr></table></figure>

<h3 id="Manual-Installation"><a href="#Manual-Installation" class="headerlink" title="Manual Installation"></a>Manual Installation</h3><p>Please first install PyTorch (we use 1.7.0) and <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/apex">apex</a>, and then install other<br>dependencies by <code>pip install -r requirements.txt</code></p>
<h3 id="Clone-this-repo"><a href="#Clone-this-repo" class="headerlink" title="Clone this repo"></a>Clone this repo</h3>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/THUDM/GLM</span><br><span class="line">cd GLM</span><br></pre></td></tr></table></figure>

<h3 id="Model-Parallelism"><a href="#Model-Parallelism" class="headerlink" title="Model Parallelism"></a>Model Parallelism</h3><p>If your encounter the <code>CUDA out of memory</code> error, which means you GPU memory is limited, you can try the model<br>parallelism to divide the parameters into multiple GPUs. Take the two-way model parallelism as an example. First<br>run <code>change_mp.py</code> to divide the checkpoint:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python change_mp.py path_to_the_checkpoint 2</span><br></pre></td></tr></table></figure>

<p>Then update the checkpoint path in the model config file (such<br>as <a href="config_tasks/model_blocklm_10B.sh">config_tasks&#x2F;model_blocklm_10B.sh</a>) and change <code>MP_SIZE</code> in the script (such<br>as <a href="scripts/ds_finetune_superglue.sh">scripts&#x2F;ds_finetune_superglue.sh</a>) to <code>2</code>.</p>
<h2 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h2><p>We provide scripts for finetuning GLM on some downstream tasks.</p>
<h3 id="Left-to-Right-Generation-x2F-Blank-Filling-Interactive"><a href="#Left-to-Right-Generation-x2F-Blank-Filling-Interactive" class="headerlink" title="Left-to-Right Generation &#x2F; Blank Filling (Interactive)"></a>Left-to-Right Generation &#x2F; Blank Filling (Interactive)</h3><ul>
<li>Change <code>CHECKPOINT_PATH</code> to your local path. Run the following script</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bash scripts/generate_block.sh \</span><br><span class="line">     config_tasks/model_blocklm_10B_chinese.sh</span><br></pre></td></tr></table></figure>

<p>Some models (GLM-2B, GLM-10B, and GLM-10B-Chinese) use three different mask tokens: <code>[MASK]</code> for short blank<br>filling, <code>[sMASK]</code> for sentence filling, and <code>[gMASK]</code> for left-to-right generation.</p>
<details>
<summary><b>Examples</b></summary>

<h4 id="Usage-of-MASK-Entity-Prediction"><a href="#Usage-of-MASK-Entity-Prediction" class="headerlink" title="Usage of [MASK] (Entity Prediction):"></a>Usage of <code>[MASK]</code> (Entity Prediction):</h4><h5 id="Example1"><a href="#Example1" class="headerlink" title="Example1"></a>Example1</h5><p>Context: Ng is an adjunct professor at [MASK] (formerly associate professor and Director of its Stanford AI Lab or SAIL ). Also a pioneer in online education, Ng co-founded Coursera and deeplearning.ai.</p>
<p>GLM: the stanford university</p>
<h5 id="Example2-Chinese"><a href="#Example2-Chinese" class="headerlink" title="Example2 (Chinese)"></a>Example2 (Chinese)</h5><p>Context: 凯旋门位于意大利米兰市古城堡旁。1807年为纪念[MASK]而建，门高25米，顶上矗立两武士青铜古兵车铸像。</p>
<p>GLM:拿破仑军队攻克米兰城</p>
<h4 id="Usage-of-sMASK-Sentence-Prediction"><a href="#Usage-of-sMASK-Sentence-Prediction" class="headerlink" title="Usage of [sMASK] (Sentence Prediction)"></a>Usage of <code>[sMASK]</code> (Sentence Prediction)</h4><h5 id="Example3"><a href="#Example3" class="headerlink" title="Example3"></a>Example3</h5><p>Context: There have been various types of pretraining architectures including autoencoding models (e.g., BERT),<br>autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). [sMASK] We propose a General Language Model (<br>GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding<br>2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT<br>and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of<br>blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT<br>given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25×<br>parameters of BERT Large, demonstrating its generalizability to different downstream tasks.</p>
<p>GLM: However, there is a growing need to develop a single pretraining model that is not only good at natural language<br>understanding (NLU) or dialog generation&#x2F;generation (dialog), but is also able to predict other tasks such as sentiment<br>analysis, conditional generation, or machine translation (MT).</p>
<h5 id="Example4-Chinese"><a href="#Example4-Chinese" class="headerlink" title="Example4 (Chinese)"></a>Example4 (Chinese)</h5><p>Context: 工业互联网（Industrial<br>Internet）是新一代信息通信技术与工业经济深度融合的新型基础设施、应用模式和工业生态，通过对人、机、物、系统等的全面连接，构建起覆盖全产业链、全价值链的全新制造和服务体系，为工业乃至产业数字化、网络化、智能化发展提供了实现途径，是第四次工业革命的重要基石。[sMASK]<br>它以网络为基础、平台为中枢、数据为要素、安全为保障，既是工业数字化、网络化、智能化转型的基础设施，也是互联网、大数据、人工智能与实体经济深度融合的应用模式，同时也是一种新业态、新产业，将重塑企业形态、供应链和产业链。当前，工业互联网融合应用向国民经济重点行业广泛拓展，形成平台化设计、智能化制造、网络化协同、个性化定制、服务化延伸、数字化管理六大新模式，赋能、赋智、赋值作用不断显现，有力的促进了实体经济提质、增效、降本、绿色、安全发展。</p>
<p>GLM: 工业互联网是制造业技术、管理、模式的重大变革,是推动互联网、大数据、人工智能和实体经济深度融合的重要载体,是建设制造强国和网络强国的重要基础。</p>
<h4 id="Usage-of-gMASK-Long-Text-Generation"><a href="#Usage-of-gMASK-Long-Text-Generation" class="headerlink" title="Usage of [gMASK] (Long Text Generation)"></a>Usage of <code>[gMASK]</code> (Long Text Generation)</h4><h5 id="Example5-Chinese"><a href="#Example5-Chinese" class="headerlink" title="Example5 (Chinese)"></a>Example5 (Chinese)</h5><p>Context: 问题：冬天，中国哪座城市最适合避寒？问题描述：能推荐一些国内适合冬天避寒的城市吗？回答用户：旅游爱好者 回答： [gMASK]</p>
<p>GLM:<br>海南三亚,三亚的冬天是三亚的旺季,因为冬天国内大部分地方都有雾霾,而三亚却是没有雾霾的地方,所以三亚是冬天最惬意不过的地方了。在东北长大的我觉得三亚简直就是一个天堂,冬天去海口三亚旅游,享受冬天,享受阳光沙滩。但是海口却是很干燥,需要多喝水。<br>三亚冬天最热门的玩法就是晒太阳,在海边晒晒太阳,在沙滩上晒晒太阳,感觉整个人都得到了解放。三亚还有一个特色项目,就是海上冲浪,在三亚的沙滩上冲浪也是一件非常刺激的事情。<br>海口,海口冬季的阳光十分温暖,海南的冬季也是属于冬季旅游的旺季。冬季的海口最棒的是去海南的热带野生动植物园,那里有数之不尽的热带小动物,在这里可以近距离的和它们接触,海南的热带野生动植物园也是海南的天然氧吧。还可以在海口观澜湖公园里感受海口美丽的海景。<br>贵阳,贵州的冬天也是十分温暖的,贵阳也是冬季避寒很好的城市之一。冬季去贵阳玩一定要去黔灵山,黔灵山是贵州香火很旺盛的一个寺庙,寺庙的冬季香火鼎盛,在冬季去寺庙游玩也是一个很好的体验。除了黔灵山,贵阳在冬季还有花溪公园可以去玩,花溪公园也是去当地公园玩最好的选择。<br>青岛,青岛的冬天是青岛最舒服的时候,青岛有很多海滨浴场,冬天去海边泡一泡温泉,然后晒晒太阳是一件十分惬意的事情。青岛也有沙滩,冬天在沙滩上晒晒太阳,看看海,再玩玩沙滩游戏,感觉十分快乐的事。</p>
</details>

<p>You can also add multiple <code>[MASK]</code> and <code>[sMASK]</code> in a single example. The model will fill the blanks one by one from left to right. The answer to each blank always begins with a special <code>&lt;|startofpiece|&gt;</code>.</p>
<details>
<summary><b>Examples</b></summary>

<h5 id="Example1-1"><a href="#Example1-1" class="headerlink" title="Example1"></a>Example1</h5><p>Context: There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and [MASK] (e.g., T5). [sMASK] We propose a General Language Model ( GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over [MASK] on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and [MASK], GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25× parameters of BERT Large , demonstrating its generalizability to different downstream tasks.</p>
<p>GLM: &lt;|startofpiece|&gt; blank filling models&lt;|startofpiece|&gt; However, most of them cannot easily transfer to other downstream tasks due to the different characteristics of these tasks.&lt;|startofpiece|&gt; other pretrained models&lt;|startofpiece|&gt; unconditional reading, and semantic role labeling tasks</p>
<h5 id="Example2-Chinese-1"><a href="#Example2-Chinese-1" class="headerlink" title="Example2 (Chinese)"></a>Example2 (Chinese)</h5><p>Context: 工业互联网（Industrial Internet）是新一代[MASK]与[MASK]深度融合的新型基础设施、应用模式和工业生态，通过对人、机、物、系统等的全面连接，构建起覆盖全产业链、全价值链的全新制造和服务体系，为工业乃至产业数字化、网络化、智能化发展提供了实现途径，是第四次工业革命的重要基石。[sMASK] 它以网络为基础、平台为中枢、数据为要素、安全为保障，既是工业数字化、网络化、智能化转型的基础设施，也是互联网、大数据、人工智能与实体经济深度融合的应用模式，同时也是一种新业态、新产业，将重塑企业形态、供应链和产业链。当前，工业互联网融合应用向国民经济重点行业广泛拓展，形成[MASK]、智能化制造、[MASK]、个性化定制、服务化延伸、数字化管理六大新模式，赋能、赋智、赋值作用不断显现，有力的促进了实体经济提质、增效、降本、绿色、安全发展。</p>
<p>GLM:<br>&lt;|startofpiece|&gt;信息技术(ICT)&lt;|startofpiece|&gt;工业经济(II2O)&lt;|startofpiece|&gt;我国工业互联网是面向工业全领域、全流程、全体系的互联网,具有多产业、多领域融合的特点。&lt;|startofpiece|&gt;网络化协同&lt;|startofpiece|&gt;平台企业</p>
</details>


<h3 id="SuperGLUE-1"><a href="#SuperGLUE-1" class="headerlink" title="SuperGLUE"></a>SuperGLUE</h3><ul>
<li><p>Download the <a target="_blank" rel="noopener" href="https://super.gluebenchmark.com/tasks">SuperGlue</a> data and check the experiment setup in<br><a href="scripts/ds_finetune_superglue.sh">scripts&#x2F;ds_finetune_superglue.sh</a>. Note<br>that <code>DATA_ROOT, CHECKPOINT_PATH, SAVE_PATH</code><br>need to be changed to your local path. You may also change the <code>batch-size</code> and <code>nproc_per_node</code> according to your<br>available hardware.</p>
</li>
<li><p>Run the following script (use the COPA dataset as an example)</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bash scripts/ds_finetune_superglue.sh \</span><br><span class="line">     config_tasks/model_blocklm_10B.sh \</span><br><span class="line">     config_tasks/task_copa.sh</span><br></pre></td></tr></table></figure>

<ul>
<li>We also implement <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.10385">P-Tuning</a> in our code. Run the following script to integrate<br>p-tuning:</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bash scripts/ds_finetune_superglue_prompt.sh \</span><br><span class="line">     config_tasks/model_blocklm_10B.sh \</span><br><span class="line">     config_tasks/task_copa.sh</span><br></pre></td></tr></table></figure>

<ul>
<li>To apply GLM to a new NLU dataset with cloze-filling finetuning, implement a <code>DataProcessor</code> in<br><a href="tasks/superglue/dataset.py">tasks&#x2F;superglue&#x2F;dataset.py</a> for data loading and add a <code>PVP</code> in<br><a href="tasks/superglue/pvp.py">tasks&#x2F;superglue&#x2F;pvp.py</a> for the cloze question. More details can be found<br><a href="tasks/superglue/README.md">here</a>.</li>
</ul>
<h3 id="Seq2Seq-1"><a href="#Seq2Seq-1" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h3><ul>
<li><p>Download the <a target="_blank" rel="noopener" href="https://github.com/harvardnlp/sent-summary">Gigaword</a><br>, <a target="_blank" rel="noopener" href="https://github.com/artmatsak/cnn-dailymail">CNN&#x2F;Daily Mail</a><br>or <a target="_blank" rel="noopener" href="https://github.com/EdinburghNLP/XSum/tree/master/XSum-Dataset">XSum</a> dataset and check the experiment setup in<br><a href="scripts/ds_finetune_seq2seq.sh">scripts&#x2F;ds_finetune_seq2seq.sh</a>. Change <code>DATA_ROOT, CHECKPOINT_PATH, SAVE_PATH</code> to<br>your<br>local path.</p>
</li>
<li><p>Run the following script (use the CNN&#x2F;Daily Mail dataset as an example)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bash scripts/ds_finetune_seq2seq.sh \ </span><br><span class="line">   config_tasks/model_blocklm_10B.sh \ </span><br><span class="line">   config_tasks/seq_cnndm_org.sh</span><br></pre></td></tr></table></figure></li>
<li><p>The summaries are written into <code>./runs/experiment_name/test.jsonl.hyps</code>. The references are written<br>into <code>test.jsonl.refs</code> in the same directory. For calculating rouge,<br>install <a target="_blank" rel="noopener" href="https://github.com/pltrdy/files2rouge">file2rouge</a> and download Stanford CoreNLP<br>from <a target="_blank" rel="noopener" href="http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip">here</a>. Run the following script</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bash scripts/evaluate_seq2seq.sh \</span><br><span class="line"> ./runs/experiment_name/test.jsonl.hyps ./runs/experiment_name/test.jsonl.refs</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="Train-with-your-own-data"><a href="#Train-with-your-own-data" class="headerlink" title="Train with your own data"></a>Train with your own data</h4><p>Process your seq2seq data into <code>&#123;split&#125;.source</code> and <code>&#123;split&#125;.target</code>, with each line being the context or the target of<br>a sample, and <code>split</code> being <code>train</code>, <code>val</code>, and <code>test</code>.</p>
<p>Run the following script</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bash scripts/ds_finetune_seq2seq.sh \ </span><br><span class="line">   config_tasks/model_blocklm_10B.sh \ </span><br><span class="line">   config_tasks/seq_customization.sh</span><br></pre></td></tr></table></figure>

<p>You can specify the hyperparameters in <code>config_tasks/seq_customization.sh</code><br>and <code>config_tasks/config_blocklm_10B_cnndm.json</code></p>
<h3 id="Multiple-Choice-Zero-shot"><a href="#Multiple-Choice-Zero-shot" class="headerlink" title="Multiple Choice (Zero-shot)"></a>Multiple Choice (Zero-shot)</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash scripts/evaluate_multichoice.sh config_tasks/model_blocklm_10B.sh</span><br></pre></td></tr></table></figure>

<p>Note that <code>CHECKPOINT_PATH</code> and <code>DATA_PATH</code> need to be changed to your local path.</p>
<p>The format of each line of the data file should be</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;inputs_pretokenized&quot;: &quot;Context and question here&quot;, &quot;choices_pretokenized&quot;: [&quot;Choice 1&quot;, &quot;Choice 2&quot;, &quot;Choice 3&quot;], &quot;label&quot;: int&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Language-Modeling-1"><a href="#Language-Modeling-1" class="headerlink" title="Language Modeling"></a>Language Modeling</h3><h4 id="LAMBADA-Cloze-Accuracy"><a href="#LAMBADA-Cloze-Accuracy" class="headerlink" title="LAMBADA Cloze Accuracy"></a>LAMBADA Cloze Accuracy</h4><ul>
<li>Download the <a target="_blank" rel="noopener" href="https://github.com/cybertronai/bflm/blob/master/lambada_test.jsonl">LAMBADA</a> data and change<br><code>DATA_ROOT, CHECKPOINT_PATH</code> in <a href="scripts/evaluate_lm.sh">scripts&#x2F;evaluate_lm.sh</a></li>
<li>Run the following script</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bash scripts/evaluate_lm.sh \ </span><br><span class="line">     config_tasks/model_blocklm_large_generation.sh \</span><br><span class="line">     config_tasks/zero_lambada.sh </span><br></pre></td></tr></table></figure>

<h4 id="LM-Perplexity"><a href="#LM-Perplexity" class="headerlink" title="LM Perplexity"></a>LM Perplexity</h4><ul>
<li>Download<br>our <a target="_blank" rel="noopener" href="https://mailstsinghuaeducn-my.sharepoint.com/:t:/g/personal/duzx16_mails_tsinghua_edu_cn/EQa_B6KY_q1FjtUeG-T52iMBFtNrfhfHcZbzMxfkJKXKRQ?e=inTdHh">test set of wikibook</a><br>or <a target="_blank" rel="noopener" href="https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip">Wikitext103</a> dataset and<br>change <code>DATA_ROOT, CHECKPOINT_PATH</code><br>in <a href="scripts/evaluate_lm.sh">scripts&#x2F;evaluate_lm.sh</a></li>
<li>Run the following script<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bash scripts/evaluate_lm.sh \ </span><br><span class="line">   config_tasks/model_blocklm_large_generation.sh \</span><br><span class="line">   config_tasks/zero_wikitext.sh </span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="Text-Infilling"><a href="#Text-Infilling" class="headerlink" title="Text Infilling"></a>Text Infilling</h3><ul>
<li><p>Download the <a target="_blank" rel="noopener" href="https://github.com/Varal7/blank_language_model">Yahoo</a> dataset and check the experiment setup in<br><a href="scripts/finetune_blank.sh">scripts&#x2F;finetune_blank.sh</a>. Change <code>DATA_ROOT, CHECKPOINT_PATH, SAVE_PATH</code> to your<br>local path.</p>
</li>
<li><p>Run the following script</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bash scripts/finetune_blank.sh \ </span><br><span class="line">     config_tasks/model_blocklm_large.sh \ </span><br><span class="line">     config_tasks/seq_blank.sh</span><br></pre></td></tr></table></figure>

<h2 id="Pretrain"><a href="#Pretrain" class="headerlink" title="Pretrain"></a>Pretrain</h2><p>Run the following script to pre-train the GLM-Large model</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash scripts/ds_pretrain_nvidia.sh config/ds_block_large.sh</span><br></pre></td></tr></table></figure>

<p>The script <a href="scripts/ds_pretrain_nvidia.sh">scripts&#x2F;ds_pretrain_nvidia.sh</a> launches the training program with DeepSpeed.<br>You should change <code>NUM_WORKERS</code> and <code>NUM_GPUS_PER_WORKER</code> to the number of workers and the number of gpus per worker.<br>Also change <code>HOST_FILE_PATH</code> to the path to an OpenMPI-style hostfile. More details about DeepSpeed launcher can be<br>found <a target="_blank" rel="noopener" href="https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node">here</a>.</p>
<p>The file <a href="config/ds_block_large.sh">config&#x2F;ds_block_large.sh</a> defines the hyperparameters for pretraining. Most of the<br>arguments are fairly self-explanatory. Specifically, <code>--train-data</code> can be multiple keywords defined in <code>NAMED_CORPORA</code><br>in <a href="data_utils/corpora.py">data_utils&#x2F;corpora.py</a>. The hyperparameters of the optimizer are defined in the corresponding<br>json file under <code>config</code>. The semantics of the json file can be found <a target="_blank" rel="noopener" href="https://www.deepspeed.ai/docs/config-json">here</a>.</p>
<h2 id="Citation"><a href="#Citation" class="headerlink" title="Citation"></a>Citation</h2><p>Part of the code is based on <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/Megatron-LM">Megatron-LM</a><br>and <a target="_blank" rel="noopener" href="https://github.com/timoschick/pet">PET</a>.</p>
<p>Please cite our paper if you find this code useful for your research:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">@article&#123;DBLP:conf/acl/DuQLDQY022,</span><br><span class="line">  author    = &#123;Zhengxiao Du and</span><br><span class="line">               Yujie Qian and</span><br><span class="line">               Xiao Liu and</span><br><span class="line">               Ming Ding and</span><br><span class="line">               Jiezhong Qiu and</span><br><span class="line">               Zhilin Yang and</span><br><span class="line">               Jie Tang&#125;,</span><br><span class="line">  title     = &#123;&#123;GLM:&#125; General Language Model Pretraining with Autoregressive Blank Infilling&#125;,</span><br><span class="line">  booktitle = &#123;Proceedings of the 60th Annual Meeting of the Association for Computational</span><br><span class="line">               Linguistics (Volume 1: Long Papers), &#123;ACL&#125; 2022, Dublin, Ireland,</span><br><span class="line">               May 22-27, 2022&#125;,</span><br><span class="line">  pages     = &#123;320--335&#125;,</span><br><span class="line">  publisher = &#123;Association for Computational Linguistics&#125;,</span><br><span class="line">  year      = &#123;2022&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Coder4nlp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D/" class="post-title-link" itemprop="url">deepspeed介绍</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-12 21:56:26" itemprop="dateCreated datePublished" datetime="2023-03-12T21:56:26+08:00">2023-03-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-16 18:58:52" itemprop="dateModified" datetime="2023-03-16T18:58:52+08:00">2023-03-16</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="DeepSpeed介绍"><a href="#DeepSpeed介绍" class="headerlink" title="DeepSpeed介绍"></a>DeepSpeed介绍</h1><p>近年来，模型规模迅速增加，动辄几B甚至几百B。但是GPU显存大小根本无法支撑训练推理。首先模型参数过多，导致单机内存放不下，即使能放得下，算力也跟不上。同时，硬件算力的增长远远比不上模型增长的速度，单机训练变得不再可行，需要并行化分布式训练加速。比如<code>Megatron-Turing NLG</code>有 530B 的参数，训练需要超过 10T 的内存来存储权重、梯度和状态。</p>
<p><img src="/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D/trendofnlpmodelsize.png"></p>
<p>同时，模型是一个有机的整体，简单增加机器数量并不能提升算力，需要有并行策略和通信设计，才能实现高效的并行训练。</p>
<p>Pytorch的分布式并行框架（Distributed Data Parallel，简称DDP），它也是仅仅能够将数据并行，放到各个GPU的模型上进行训练。</p>
<p>也就是说，DDP的应用场景在你的模型大小大于显卡显存大小时，它就很难继续使用了，除非你自己再将模型参数拆散分散到各个GPU上。</p>
<p>今天要给大家介绍的<a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a>，它就能实现这个拆散功能，它通过将模型参数拆散分布到各个GPU上，以实现大型模型的计算，弥补了DDP的缺点，非常方便，这也就意味着我们能用更少的GPU训练更大的模型，而且不受限于显存。</p>
<h2 id="DeepSpeed是什么？"><a href="#DeepSpeed是什么？" class="headerlink" title="DeepSpeed是什么？"></a>DeepSpeed是什么？</h2><p>DeepSpeed是一个开源深度学习训练优化库，其中包含的一个新的显存优化技术—— ZeRO（零冗余优化器），通过扩大规模，提升速度，控制成本，提升可用性，极大地推进了大模型训练能力。DeepSpeed的核心就在于：<strong>GPU显存不够，CPU内存来凑</strong>。DeepSpeed使用的一个核心要义是，<strong>时间开销和显存占用的权衡</strong>。</p>
<ul>
<li><p><strong>用 3D 并行化实现万亿参数模型训练：</strong> DeepSpeed 实现了三种并行方法的灵活组合：ZeRO 支持的数据并行，流水线并行和张量切片模型并行。3D 并行性适应了不同工作负载的需求，以支持具有<strong>万亿</strong>参数的<strong>超大型模型</strong>，同时实现了近乎完美的显存扩展性和吞吐量扩展效率。此外，其提高的通信效率使用户可以在网络带宽有限的常规群集上以 2-7 倍的速度训练有数十亿参数的模型。</p>
</li>
<li><p><strong>ZeRO-Offload 使 GPU 单卡能够训练 10 倍大的模型：</strong> 为了同时利用 CPU 和 GPU 内存来训练大型模型，我们扩展了 ZeRO-2。我们的用户在使用带有<strong>单张英伟达 V100 GPU</strong> 的机器时，可以在不耗尽显存的情况下运行<strong>多达 130 亿个参数的模型</strong>，模型规模扩展至现有方法的10倍，并保持有竞争力的吞吐量。此功能使数十亿参数的模型训练更加大众化，，并为许多深度学习从业人员打开了一扇探索更大更好的模型的窗户。</p>
</li>
</ul>
<h2 id="Deepspeed安装"><a href="#Deepspeed安装" class="headerlink" title="Deepspeed安装"></a>Deepspeed安装</h2><p>开始使用DeepSpeed的最快方法是通过pip，这将安装DeepSpeed的最新版本，它不绑定到特定的PyTorch或CUDA版本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install deepspeed</span><br></pre></td></tr></table></figure>

<ul>
<li>在安装DeepSpeed之前必须先安装PyTorch。</li>
<li>为了获得完整的特性支持，我们建议使用PyTorch&gt;&#x3D; 1.8版本，最好是最新的PyTorch稳定版本。</li>
</ul>
<p>安装后，您可以验证您的安装，并通过DeepSpeed环境报告查看您的机器与哪些扩展&#x2F;操作兼容。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ds_report</span><br></pre></td></tr></table></figure>

<h2 id="DeepSpeed使用"><a href="#DeepSpeed使用" class="headerlink" title="DeepSpeed使用"></a>DeepSpeed使用</h2><p>以GLM使用到的deepspeed配置为例</p>
<h3 id="DeepSpeed初始化"><a href="#DeepSpeed初始化" class="headerlink" title="DeepSpeed初始化"></a>DeepSpeed初始化</h3><p>DeepSpeed 通过输入参数来启动训练，因此需要使用<code>argparse</code>解析参数。完整的参数可以查看<a target="_blank" rel="noopener" href="https://www.deepspeed.ai/docs/config-json/">DeepSpeed文档</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser = deepspeed.add_config_arguments(parser)</span><br></pre></td></tr></table></figure>

<p>pytorch初始化的分布式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.distributed.init_process_group(</span><br><span class="line">           backend=args.distributed_backend,</span><br><span class="line">           world_size=args.world_size, rank=args.rank,</span><br><span class="line">           init_method=init_method)</span><br></pre></td></tr></table></figure>

<p>DeepSpeed初始化分布式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deepspeed.init_distributed(dist_backend=args.distributed_backend)</span><br></pre></td></tr></table></figure>

<p>这里介绍一些常用的参数，</p>
<h4 id="batch-size相关参数"><a href="#batch-size相关参数" class="headerlink" title="batch_size相关参数"></a>batch_size相关参数</h4><p><em><strong>train_batch_size</strong></em>: [integer]（注意:train_batch_size必须等于train_micro_batch_size_per_gpu * gradient_accumulation * gpu个数。为了简单起见，你可以选择只指定三个参数中的两个，最后一个参数将由DeepSpeed自动推断。）</p>
<table>
<thead>
<tr>
<th align="left">Value</th>
<th align="left">Example</th>
</tr>
</thead>
<tbody><tr>
<td align="left">The effective training batch size. This is the amount of data samples that leads to one step of model update. <em><strong>train_batch_size</strong></em> is aggregated by the batch size that a single GPU processes in one forward&#x2F;backward pass (a.k.a., <em><strong>train_micro_batch_size_per_gpu</strong></em>), the gradient accumulation steps (a.k.a., <em><strong>gradient_accumulation_steps</strong></em>), and the number of GPUs. Can be omitted if both <em><strong>train_micro_batch_size_per_gpu</strong></em> and <em><strong>gradient_accumulation_steps</strong></em> are provided.</td>
<td align="left"></td>
</tr>
</tbody></table>
<p><em><strong>train_micro_batch_size_per_gpu</strong></em>: [integer]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Batch size to be processed by one GPU in one step (without gradient accumulation). Can be omitted if both <em><strong>train_batch_size</strong></em> and <em><strong>gradient_accumulation_steps</strong></em> are provided.</td>
<td align="left"><em><strong>train_batch_size</strong></em> value</td>
</tr>
</tbody></table>
<p><em><strong>gradient_accumulation_steps</strong></em>: [integer]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Number of training steps to accumulate gradients before averaging and applying them. This feature is sometimes useful to improve scalability since it results in less frequent communication of gradients between steps. Another impact of this feature is the ability to train with larger batch sizes per GPU. Can be omitted if both <em><strong>train_batch_size</strong></em> and <em><strong>train_micro_batch_size_per_gpu</strong></em> are provided.</td>
<td align="left"></td>
</tr>
</tbody></table>
<h4 id="优化器相关参数"><a href="#优化器相关参数" class="headerlink" title="优化器相关参数"></a>优化器相关参数</h4><p><em><strong>optimizer</strong></em>: [dictionary]</p>
<table>
<thead>
<tr>
<th align="left">Fields</th>
<th align="left">Value</th>
<th align="left">Example</th>
</tr>
</thead>
<tbody><tr>
<td align="left">type</td>
<td align="left">The optimizer name. DeepSpeed natively supports <strong>Adam</strong>, <strong>AdamW</strong>, <strong>OneBitAdam</strong>, <strong>Lamb</strong>, and <strong>OneBitLamb</strong> optimizers (See <a target="_blank" rel="noopener" href="https://deepspeed.readthedocs.io/en/latest/optimizers.html">here</a> for details) and will import other optimizers from <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html">torch</a>.</td>
<td align="left"><code>&quot;Adam&quot;</code></td>
</tr>
<tr>
<td align="left">params</td>
<td align="left">Dictionary of parameters to instantiate optimizer. The parameter names must match the optimizer constructor signature (e.g., for <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Adam">Adam</a>).</td>
<td align="left"><code>&#123;&quot;lr&quot;: 0.001, &quot;eps&quot;: 1e-8&#125;</code></td>
</tr>
</tbody></table>
<h4 id="Scheduler-参数"><a href="#Scheduler-参数" class="headerlink" title="Scheduler 参数"></a>Scheduler 参数</h4><p>在执行model_engine.step()时，DeepSpeed在每个训练步骤中调用调度器的step()方法。</p>
<p>*<strong>scheduler*</strong>: [dictionary]</p>
<table>
<thead>
<tr>
<th align="left">Fields</th>
<th align="left">Value</th>
<th align="left">Example</th>
</tr>
</thead>
<tbody><tr>
<td align="left">type</td>
<td align="left">The scheduler name. See <a target="_blank" rel="noopener" href="https://deepspeed.readthedocs.io/en/latest/schedulers.html">here</a> for list of support schedulers.</td>
<td align="left"><code>&quot;WarmupLR&quot;</code></td>
</tr>
<tr>
<td align="left">params</td>
<td align="left">Dictionary of parameters to instantiate scheduler. The parameter names should match scheduler constructor signature.</td>
<td align="left"><code>&#123;&quot;warmup_min_lr&quot;: 0, &quot;warmup_max_lr&quot;: 0.001&#125;</code></td>
</tr>
</tbody></table>
<h4 id="FP16-选项"><a href="#FP16-选项" class="headerlink" title="FP16 选项"></a>FP16 选项</h4><p><strong>注意</strong>:此模式不能与下面描述的amp模式组合。</p>
<p><em><strong>fp16</strong></em>: [dictionary]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Configuration for using mixed precision&#x2F;FP16 training that leverages <a target="_blank" rel="noopener" href="https://nvidia.github.io/apex/">NVIDIA’s Apex package</a>. An example, including the available dictionary keys is illustrated below. NOTE: this does not use Apex’s AMP mode that allows for more flexibility in mixed precision training modes, this mode is similar to AMP’s O2 mode. Please see AMP support below if you want to use more complex mixed precision modes. If you want to use ZeRO (currently) you must use this mode.</td>
<td align="left">None</td>
</tr>
</tbody></table>
<h4 id="ZeRO-Optimizations-for-FP16-Training"><a href="#ZeRO-Optimizations-for-FP16-Training" class="headerlink" title="ZeRO Optimizations for FP16 Training"></a>ZeRO Optimizations for FP16 Training</h4><p>启用和配置ZeRO内存优化</p>
<p><em><strong>stage</strong></em>: [integer]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">选择ZeRO优化器的不同阶段。阶段0、阶段1、阶段2、阶段3分别为禁用、优化器状态分区、优化器+梯度状态分区、优化器+梯度+参数分区。</td>
<td align="left"><code>0</code></td>
</tr>
</tbody></table>
<p><em><strong>reduce_scatter</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">使用reduce或reduce scatter点代替all reduce to average gradient</td>
<td align="left"><code>true</code></td>
</tr>
</tbody></table>
<p>*<strong>reduce_bucket_size*</strong>: [integer]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Number of elements reduced&#x2F;allreduced at a time. Limits the memory required for the allgather for large model sizes</td>
<td align="left"><code>5e8</code></td>
</tr>
</tbody></table>
<p><em><strong>contiguous_gradients</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Copies the gradients to a contiguous buffer as they are produced. Avoids memory fragmentation during backward pass.</td>
<td align="left"><code>True</code></td>
</tr>
</tbody></table>
<p><em><strong>overlap_comm</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">另外一个需要提到的参数是<code>overlap_comm</code>。简单地理解，它控制着多个memory上进程之间通信的buffer的大小。这个值越大，进程之间通信越快，模型训练速度也会提升，但相应的显存占用也会变大；反之亦然。Attempts to overlap the reduction of the gradients with backward computation</td>
<td align="left"><code>false</code></td>
</tr>
</tbody></table>
<p>*<strong>allgather_bucket_size*</strong>: [integer]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Number of elements allgathered at a time. Limits the memory required for the allgather for large model sizes</td>
<td align="left"><code>5e8</code></td>
</tr>
</tbody></table>
<p>*<strong>cpu_offload*</strong>: [boolean]</p>
<p><strong>Deprecated:</strong> <strong>cpu_offload</strong> is deprecated and will be removed in future, please use <code>offload_optimizer</code> instead.</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">在训练过程中，将优化器状态分配到cpu上。从而降低单张GPU的memory占用。Enable offloading of optimizer memory and computation to CPU. This frees up GPU memory for larger models or batch sizes. Valid with stage 1 and 2.</td>
<td align="left"><code>False</code></td>
</tr>
</tbody></table>
<h4 id="激活Checkpointing"><a href="#激活Checkpointing" class="headerlink" title="激活Checkpointing"></a>激活Checkpointing</h4><p><em><strong>partition_activations</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Enables partition activation when used with model parallelism</td>
<td align="left"><code>false</code></td>
</tr>
</tbody></table>
<p><em><strong>contiguous_memory_optimization</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Copies partitioned activations so that they are contiguous in memory</td>
<td align="left"><code>false</code></td>
</tr>
</tbody></table>
<h4 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h4><p><em><strong>wall_clock_breakdown</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Enable timing of the latency of forward&#x2F;backward&#x2F;update training phases</td>
<td align="left"><code>false</code></td>
</tr>
</tbody></table>
<h4 id="GLM-10B使用的参数"><a href="#GLM-10B使用的参数" class="headerlink" title="GLM 10B使用的参数"></a>GLM 10B使用的参数</h4><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="punctuation">:</span> <span class="number">4</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;gradient_accumulation_steps&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;steps_per_print&quot;</span><span class="punctuation">:</span> <span class="number">50</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;gradient_clipping&quot;</span><span class="punctuation">:</span> <span class="number">1.0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;zero_optimization&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;stage&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;contiguous_gradients&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;overlap_comm&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;reduce_scatter&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;reduce_bucket_size&quot;</span><span class="punctuation">:</span> <span class="number">5e7</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;allgather_bucket_size&quot;</span><span class="punctuation">:</span> <span class="number">5e7</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;cpu_offload&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;zero_allow_untested_optimizer&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;fp16&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;enabled&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;loss_scale&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;loss_scale_window&quot;</span><span class="punctuation">:</span> <span class="number">1000</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;hysteresis&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;min_loss_scale&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;optimizer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Adam&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;params&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;lr&quot;</span><span class="punctuation">:</span> <span class="number">5e-6</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;betas&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="number">0.9</span><span class="punctuation">,</span></span><br><span class="line">        <span class="number">0.95</span></span><br><span class="line">      <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;eps&quot;</span><span class="punctuation">:</span> <span class="number">1e-8</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;weight_decay&quot;</span><span class="punctuation">:</span> <span class="number">1e-2</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;activation_checkpointing&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;partition_activations&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;contiguous_memory_optimization&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;wall_clock_breakdown&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="模型及优化器初始化"><a href="#模型及优化器初始化" class="headerlink" title="模型及优化器初始化"></a>模型及优化器初始化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">model, optimizer, _, _ = deepspeed.initialize(</span><br><span class="line">    model=model,</span><br><span class="line">    model_parameters=param_groups,</span><br><span class="line">    args=args,</span><br><span class="line">    mpu=mpu,</span><br><span class="line">    dist_init_required=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="DeepSpeed训练"><a href="#DeepSpeed训练" class="headerlink" title="DeepSpeed训练"></a>DeepSpeed训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deepspeed --hostfile $&#123;HOST_FILE_PATH&#125; --master_port $&#123;MASTER_PORT&#125; --num_nodes $&#123;NUM_WORKERS&#125; --num_gpus $&#123;NUM_GPUS_PER_WORKER&#125;<span class="string">&quot; finetune_glm.py</span></span><br></pre></td></tr></table></figure>

<p>以GLM为例，使用自己的数据训练</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bash scripts/ds_finetune_seq2seq.sh \ </span><br><span class="line">   config_tasks/model_blocklm_10B.sh \ </span><br><span class="line">   config_tasks/seq_customization.sh</span><br></pre></td></tr></table></figure>

<h2 id="ZeRO概述"><a href="#ZeRO概述" class="headerlink" title="ZeRO概述"></a>ZeRO概述</h2><p>Zero Redundancy Optimizer (ZeRO)是DeepSpeed的workhorse. 用户可以提供不同的ZeRO config文件，来实现DeepSpeed的不同功能特性。</p>
<p>来看一下<a target="_blank" rel="noopener" href="https://deepspeed.readthedocs.io/en/latest/zero3.html#getting-started">官网教程</a>对ZeRO的描述：</p>
<blockquote>
<p>The Zero Redundancy Optimizer (ZeRO) removes the memory redundancies across data-parallel processes by partitioning the three model states (optimizer states, gradients, and parameters) across data-parallel processes instead of replicating them. By doing this, it boosts memory efficiency compared to classic data-parallelism while retaining its computational granularity and communication efficiency.</p>
</blockquote>
<p>一句话总结： <code>partitioning instead of replicating</code>，<strong>划分而不是复制</strong>。</p>
<p>即，传统的<a target="_blank" rel="noopener" href="https://aitechtogether.com/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">深度学习</a>，模型训练并行，是将模型参数复制多份到多张GPU上，只将数据拆分（如，torch的Dataparallel），这样就会有大量的显存冗余浪费。而ZeRO就是为了消除这种冗余，提高对memory的利用率。注意，这里的“memory”不仅指多张GPU memory，还包括CPU。</p>
<p>而ZeRO的实现方法，就是把参数占用，逻辑上分成三种类型。将这些类型的参数划分：</p>
<ul>
<li><code>optimizer states</code>：即优化器的参数状态。例如，Adam的动量参数。</li>
<li><code>gradients</code>：梯度缓存，对应于optimizer。</li>
<li><code>parameters</code>：模型参数。</li>
</ul>
<p>对应的，DeepSpeed的ZeRO config文件就可以分为如下几类：</p>
<ul>
<li><code>ZeRO Stage 1</code>: 划分optimizer states。优化器参数被划分到多个memory上，每个momoey上的进程只负责更新它自己那部分参数。</li>
<li><code>ZeRO Stage 2</code>: 划分gradient。每个memory，只保留它分配到的optimizer state所对应的梯度。这很合理，因为梯度和optimizer是紧密联系在一起的。只知道梯度，不知道optimizer state，是没有办法优化模型参数的。</li>
<li><code>ZeRO Stage 3</code>: 划分模型参数，或者说，不同的layer. ZeRO-3会在forward和backward的时候，自动将模型参数分配到多个memory。</li>
</ul>
<p>由于ZeRO-1只分配optimizer states(参数量很小)，实际使用的时候，<strong>我们一般只会考虑<code>ZeRO-2</code>和<code>ZeRO-3</code>。</strong></p>
<h3 id="内存估计"><a href="#内存估计" class="headerlink" title="内存估计"></a>内存估计</h3><p>DeepSpeed使用过程中的一个难点，就在于<code>时间和空间</code>的权衡。</p>
<p>分配更多参数到CPU上，虽然能够降低显存开销，但是也会极大地提升时间开销。</p>
<p>DeepSpeed提供了一段简单的memory估算代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModel</span><br><span class="line"><span class="keyword">from</span> deepspeed.runtime.zero.stage3 <span class="keyword">import</span> estimate_zero3_model_states_mem_needs_all_live</span><br><span class="line"></span><br><span class="line"><span class="comment">## specify the model you want to train on your device</span></span><br><span class="line">model = AutoModel.from_pretrained(<span class="string">&quot;t5-large&quot;</span>)</span><br><span class="line"><span class="comment">## estimate the memory cost (both CPU and GPU)</span></span><br><span class="line">estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=<span class="number">1</span>, num_nodes=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>如上，如果不用stage2和stage3(最下面那两行)，训练T5-large需要一张显存至少为12.49GB的显卡(考虑到很多其他的缓存变量，还有你的batch_size，实际上可能需要24GB大小的卡)。而在相继使用了stage2和3之后，显存开销被极大地降低，转而CPU内存消耗显著提升，模型训练时间开销也相应地增大。</p>
<p>***建议:***在使用DeepSpeed之前，先使用上述代码，大概估计一下显存消耗，决定使用的GPU数目，以及ZeRO-stage。</p>
<p>原则是，<strong>能直接多卡训练，就不要用ZeRO；能用ZeRO-2就不要用ZeRO-3.</strong></p>
<p>尝试使用DeepSpeed进行模型的训练。</p>
<p>首先是stage 2，也就是只把optimizer放到cpu上。下面是使用前后的GPU显存占用和训练速度对比：</p>
<ul>
<li>GPU显存：<code>20513</code>MB &#x3D;&gt; <code>17349</code>MiB</li>
<li>训练速度 (由<code>tqdm</code>估计)：<code>1.3</code> iter&#x2F;s &#x3D;&gt; <code>0.77</code> iter&#x2F;s</li>
</ul>
<p>可以明显看到，GPU的显存占用有了明显降低，但是训练速度也变慢了。以笔者当前的使用体感来说，deepspeed并没有带来什么收益。</p>
<p>机器配有<code>24000</code>MB的显卡，batch_size为2时，占用<code>20513</code>MB；而DeepSpeed仅仅可以帮助空出了<code>3000</code>MB的显存，<strong>还是完全不够增加batch_size</strong>, 导致总训练时长变长。</p>
<p>因此，DeepSpeed或许仅适用于显存极度短缺（i.e., 模型大到 batch_size &#x3D;&#x3D; 1也跑不了）的情况；亦或是，使用DeepSpped节省下来的显存，刚好够支持更大的batch_size。否则，像笔者当前这种情况下，使用DeepSpeed只会增加时间开销，并没有其他益处。</p>
<p>此后，笔者还尝试使用stage 3，但是<strong>速度极其缓慢</strong>。一个原先需要6h的训练过程，用了DeepSpeed stage3之后，运行了2天2夜，也没有结束的迹象。无奈笔者只好终止测试。</p>
<p>此外，在使用DeepSpeed stage2时，由于分配了模型参数到多个设备上，console里面也看不到任何输出信息（但是GPU还是在呼呼响，utility也为100%），让人都不知道程序的运行进度，可以说对用户非常不友好了。</p>
<p>由于DeepSpeed会通过占用CPU内存来减缓GPU的开销，当系统CPU不够的时候，DeepSpeed进程就会自动被系统停止，<strong>造成没有任何报错，DeepSpeed无法启动的现象</strong>。建议先用上文介绍的<a target="_blank" rel="noopener" href="https://aitechtogether.com/article/45439.html#jump">estimation</a>估计一下CPU内存占用，然后用<code>free -h</code>查看一下机器的CPU内存空余量，来判断能否使用DeepSpeed。</p>
<p>另外，还有可能因为训练精度问题，出现loss为<code>NAN</code>的情况。详见：<a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/main_classes/deepspeed#troubleshooting">Troubleshooting</a>.</p>
<h3 id="主流并行计算框架一览"><a href="#主流并行计算框架一览" class="headerlink" title="主流并行计算框架一览"></a>主流并行计算框架一览</h3><p><img src="/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D/1.png"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D%20-%20%E5%89%AF%E6%9C%AC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Coder4nlp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D%20-%20%E5%89%AF%E6%9C%AC/" class="post-title-link" itemprop="url">deepspeed介绍</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-12 21:56:26" itemprop="dateCreated datePublished" datetime="2023-03-12T21:56:26+08:00">2023-03-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-15 19:35:07" itemprop="dateModified" datetime="2023-03-15T19:35:07+08:00">2023-03-15</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="并行训练技术"><a href="#并行训练技术" class="headerlink" title="并行训练技术"></a>并行训练技术</h1><p>近年来，模型规模迅速增加，动辄几B甚至几百B。但是GPU显存大小根本无法支撑训练推理。首先模型参数过多，导致单机内存放不下，即使能放得下，算力也跟不上。同时，硬件算力的增长远远比不上模型增长的速度，单机训练变得不再可行，需要并行化分布式训练加速。比如<code>Megatron-Turing NLG</code>有 530B 的参数，训练需要超过 10T 的内存来存储权重、梯度和状态。</p>
<p><img src="/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D%20-%20%E5%89%AF%E6%9C%AC/trendofnlpmodelsize.png"></p>
<p>同时，模型是一个有机的整体，简单增加机器数量并不能提升算力，需要有并行策略和通信设计，才能实现高效的并行训练。</p>
<p>Pytorch的分布式并行框架（Distributed Data Parallel，简称DDP），它也是仅仅能够将数据并行，放到各个GPU的模型上进行训练。</p>
<p>也就是说，DDP的应用场景在你的模型大小大于显卡显存大小时，它就很难继续使用了，除非你自己再将模型参数拆散分散到各个GPU上。</p>
<p>今天要给大家介绍的<a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a>，它就能实现这个拆散功能，它通过将模型参数拆散分布到各个GPU上，以实现大型模型的计算，弥补了DDP的缺点，非常方便，这也就意味着我们能用更少的GPU训练更大的模型，而且不受限于显存。</p>
<h2 id="DeepSpeed是什么？"><a href="#DeepSpeed是什么？" class="headerlink" title="DeepSpeed是什么？"></a>DeepSpeed是什么？</h2><p>DeepSpeed是一个开源深度学习训练优化库，其中包含的一个新的显存优化技术—— ZeRO（零冗余优化器），通过扩大规模，提升速度，控制成本，提升可用性，极大地推进了大模型训练能力。DeepSpeed的核心就在于：<strong>GPU显存不够，CPU内存来凑</strong>。DeepSpeed使用的一个核心要义是，<strong>时间开销和显存占用的权衡</strong>。</p>
<ul>
<li><p><strong>用 3D 并行化实现万亿参数模型训练：</strong> DeepSpeed 实现了三种并行方法的灵活组合：ZeRO 支持的数据并行，流水线并行和张量切片模型并行。3D 并行性适应了不同工作负载的需求，以支持具有<strong>万亿</strong>参数的<strong>超大型模型</strong>，同时实现了近乎完美的显存扩展性和吞吐量扩展效率。此外，其提高的通信效率使用户可以在网络带宽有限的常规群集上以 2-7 倍的速度训练有数十亿参数的模型。</p>
</li>
<li><p><strong>ZeRO-Offload 使 GPU 单卡能够训练 10 倍大的模型：</strong> 为了同时利用 CPU 和 GPU 内存来训练大型模型，我们扩展了 ZeRO-2。我们的用户在使用带有<strong>单张英伟达 V100 GPU</strong> 的机器时，可以在不耗尽显存的情况下运行<strong>多达 130 亿个参数的模型</strong>，模型规模扩展至现有方法的10倍，并保持有竞争力的吞吐量。此功能使数十亿参数的模型训练更加大众化，，并为许多深度学习从业人员打开了一扇探索更大更好的模型的窗户。</p>
</li>
</ul>
<h2 id="Deepspeed安装"><a href="#Deepspeed安装" class="headerlink" title="Deepspeed安装"></a>Deepspeed安装</h2><p>开始使用DeepSpeed的最快方法是通过pip，这将安装DeepSpeed的最新版本，它不绑定到特定的PyTorch或CUDA版本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install deepspeed</span><br></pre></td></tr></table></figure>

<ul>
<li>在安装DeepSpeed之前必须先安装PyTorch。</li>
<li>为了获得完整的特性支持，我们建议使用PyTorch&gt;&#x3D; 1.8版本，最好是最新的PyTorch稳定版本。</li>
</ul>
<p>安装后，您可以验证您的安装，并通过DeepSpeed环境报告查看您的机器与哪些扩展&#x2F;操作兼容。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ds_report</span><br></pre></td></tr></table></figure>

<h2 id="DeepSpeed使用"><a href="#DeepSpeed使用" class="headerlink" title="DeepSpeed使用"></a>DeepSpeed使用</h2><p>以GLM使用到的deepspeed配置为例</p>
<h3 id="DeepSpeed初始化"><a href="#DeepSpeed初始化" class="headerlink" title="DeepSpeed初始化"></a>DeepSpeed初始化</h3><p>DeepSpeed 通过输入参数来启动训练，因此需要使用<code>argparse</code>解析参数。完整的参数可以查看<a target="_blank" rel="noopener" href="https://www.deepspeed.ai/docs/config-json/">DeepSpeed文档</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser = deepspeed.add_config_arguments(parser)</span><br></pre></td></tr></table></figure>

<p>pytorch初始化的分布式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.distributed.init_process_group(</span><br><span class="line">           backend=args.distributed_backend,</span><br><span class="line">           world_size=args.world_size, rank=args.rank,</span><br><span class="line">           init_method=init_method)</span><br></pre></td></tr></table></figure>

<p>DeepSpeed初始化分布式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deepspeed.init_distributed(dist_backend=args.distributed_backend)</span><br></pre></td></tr></table></figure>

<p>这里介绍一些常用的参数，</p>
<h4 id="batch-size相关参数"><a href="#batch-size相关参数" class="headerlink" title="batch_size相关参数"></a>batch_size相关参数</h4><p><em><strong>train_batch_size</strong></em>: [integer]（注意:train_batch_size必须等于train_micro_batch_size_per_gpu * gradient_accumulation * gpu个数。为了简单起见，你可以选择只指定三个参数中的两个，最后一个参数将由DeepSpeed自动推断。）</p>
<table>
<thead>
<tr>
<th align="left">Value</th>
<th align="left">Example</th>
</tr>
</thead>
<tbody><tr>
<td align="left">The effective training batch size. This is the amount of data samples that leads to one step of model update. <em><strong>train_batch_size</strong></em> is aggregated by the batch size that a single GPU processes in one forward&#x2F;backward pass (a.k.a., <em><strong>train_micro_batch_size_per_gpu</strong></em>), the gradient accumulation steps (a.k.a., <em><strong>gradient_accumulation_steps</strong></em>), and the number of GPUs. Can be omitted if both <em><strong>train_micro_batch_size_per_gpu</strong></em> and <em><strong>gradient_accumulation_steps</strong></em> are provided.</td>
<td align="left"></td>
</tr>
</tbody></table>
<p><em><strong>train_micro_batch_size_per_gpu</strong></em>: [integer]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Batch size to be processed by one GPU in one step (without gradient accumulation). Can be omitted if both <em><strong>train_batch_size</strong></em> and <em><strong>gradient_accumulation_steps</strong></em> are provided.</td>
<td align="left"><em><strong>train_batch_size</strong></em> value</td>
</tr>
</tbody></table>
<p><em><strong>gradient_accumulation_steps</strong></em>: [integer]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Number of training steps to accumulate gradients before averaging and applying them. This feature is sometimes useful to improve scalability since it results in less frequent communication of gradients between steps. Another impact of this feature is the ability to train with larger batch sizes per GPU. Can be omitted if both <em><strong>train_batch_size</strong></em> and <em><strong>train_micro_batch_size_per_gpu</strong></em> are provided.</td>
<td align="left"></td>
</tr>
</tbody></table>
<h4 id="优化器相关参数"><a href="#优化器相关参数" class="headerlink" title="优化器相关参数"></a>优化器相关参数</h4><p><em><strong>optimizer</strong></em>: [dictionary]</p>
<table>
<thead>
<tr>
<th align="left">Fields</th>
<th align="left">Value</th>
<th align="left">Example</th>
</tr>
</thead>
<tbody><tr>
<td align="left">type</td>
<td align="left">The optimizer name. DeepSpeed natively supports <strong>Adam</strong>, <strong>AdamW</strong>, <strong>OneBitAdam</strong>, <strong>Lamb</strong>, and <strong>OneBitLamb</strong> optimizers (See <a target="_blank" rel="noopener" href="https://deepspeed.readthedocs.io/en/latest/optimizers.html">here</a> for details) and will import other optimizers from <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html">torch</a>.</td>
<td align="left"><code>&quot;Adam&quot;</code></td>
</tr>
<tr>
<td align="left">params</td>
<td align="left">Dictionary of parameters to instantiate optimizer. The parameter names must match the optimizer constructor signature (e.g., for <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Adam">Adam</a>).</td>
<td align="left"><code>&#123;&quot;lr&quot;: 0.001, &quot;eps&quot;: 1e-8&#125;</code></td>
</tr>
</tbody></table>
<h4 id="Scheduler-参数"><a href="#Scheduler-参数" class="headerlink" title="Scheduler 参数"></a>Scheduler 参数</h4><p>在执行model_engine.step()时，DeepSpeed在每个训练步骤中调用调度器的step()方法。</p>
<p>*<strong>scheduler*</strong>: [dictionary]</p>
<table>
<thead>
<tr>
<th align="left">Fields</th>
<th align="left">Value</th>
<th align="left">Example</th>
</tr>
</thead>
<tbody><tr>
<td align="left">type</td>
<td align="left">The scheduler name. See <a target="_blank" rel="noopener" href="https://deepspeed.readthedocs.io/en/latest/schedulers.html">here</a> for list of support schedulers.</td>
<td align="left"><code>&quot;WarmupLR&quot;</code></td>
</tr>
<tr>
<td align="left">params</td>
<td align="left">Dictionary of parameters to instantiate scheduler. The parameter names should match scheduler constructor signature.</td>
<td align="left"><code>&#123;&quot;warmup_min_lr&quot;: 0, &quot;warmup_max_lr&quot;: 0.001&#125;</code></td>
</tr>
</tbody></table>
<h4 id="FP16-选项"><a href="#FP16-选项" class="headerlink" title="FP16 选项"></a>FP16 选项</h4><p><strong>注意</strong>:此模式不能与下面描述的amp模式组合。</p>
<p><em><strong>fp16</strong></em>: [dictionary]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Configuration for using mixed precision&#x2F;FP16 training that leverages <a target="_blank" rel="noopener" href="https://nvidia.github.io/apex/">NVIDIA’s Apex package</a>. An example, including the available dictionary keys is illustrated below. NOTE: this does not use Apex’s AMP mode that allows for more flexibility in mixed precision training modes, this mode is similar to AMP’s O2 mode. Please see AMP support below if you want to use more complex mixed precision modes. If you want to use ZeRO (currently) you must use this mode.</td>
<td align="left">None</td>
</tr>
</tbody></table>
<h4 id="ZeRO-Optimizations-for-FP16-Training"><a href="#ZeRO-Optimizations-for-FP16-Training" class="headerlink" title="ZeRO Optimizations for FP16 Training"></a>ZeRO Optimizations for FP16 Training</h4><p>启用和配置ZeRO内存优化</p>
<p><em><strong>stage</strong></em>: [integer]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">选择ZeRO优化器的不同阶段。阶段0、阶段1、阶段2、阶段3分别为禁用、优化器状态分区、优化器+梯度状态分区、优化器+梯度+参数分区。</td>
<td align="left"><code>0</code></td>
</tr>
</tbody></table>
<p><em><strong>reduce_scatter</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">使用reduce或reduce scatter点代替all reduce to average gradient</td>
<td align="left"><code>true</code></td>
</tr>
</tbody></table>
<p>*<strong>reduce_bucket_size*</strong>: [integer]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Number of elements reduced&#x2F;allreduced at a time. Limits the memory required for the allgather for large model sizes</td>
<td align="left"><code>5e8</code></td>
</tr>
</tbody></table>
<p><em><strong>contiguous_gradients</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Copies the gradients to a contiguous buffer as they are produced. Avoids memory fragmentation during backward pass.</td>
<td align="left"><code>True</code></td>
</tr>
</tbody></table>
<p><em><strong>overlap_comm</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Attempts to overlap the reduction of the gradients with backward computation</td>
<td align="left"><code>false</code></td>
</tr>
</tbody></table>
<p>*<strong>allgather_bucket_size*</strong>: [integer]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Number of elements allgathered at a time. Limits the memory required for the allgather for large model sizes</td>
<td align="left"><code>5e8</code></td>
</tr>
</tbody></table>
<p>*<strong>cpu_offload*</strong>: [boolean]</p>
<p><strong>Deprecated:</strong> <strong>cpu_offload</strong> is deprecated and will be removed in future, please use <code>offload_optimizer</code> instead.</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Enable offloading of optimizer memory and computation to CPU. This frees up GPU memory for larger models or batch sizes. Valid with stage 1 and 2.</td>
<td align="left"><code>False</code></td>
</tr>
</tbody></table>
<h4 id="激活Checkpointing"><a href="#激活Checkpointing" class="headerlink" title="激活Checkpointing"></a>激活Checkpointing</h4><p><em><strong>partition_activations</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Enables partition activation when used with model parallelism</td>
<td align="left"><code>false</code></td>
</tr>
</tbody></table>
<p><em><strong>contiguous_memory_optimization</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Copies partitioned activations so that they are contiguous in memory</td>
<td align="left"><code>false</code></td>
</tr>
</tbody></table>
<h4 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h4><p><em><strong>wall_clock_breakdown</strong></em>: [boolean]</p>
<table>
<thead>
<tr>
<th align="left">Description</th>
<th align="left">Default</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Enable timing of the latency of forward&#x2F;backward&#x2F;update training phases</td>
<td align="left"><code>false</code></td>
</tr>
</tbody></table>
<h4 id="GLM-10B使用的参数"><a href="#GLM-10B使用的参数" class="headerlink" title="GLM 10B使用的参数"></a>GLM 10B使用的参数</h4><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="punctuation">:</span> <span class="number">4</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;gradient_accumulation_steps&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;steps_per_print&quot;</span><span class="punctuation">:</span> <span class="number">50</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;gradient_clipping&quot;</span><span class="punctuation">:</span> <span class="number">1.0</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;zero_optimization&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;stage&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;contiguous_gradients&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;overlap_comm&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;reduce_scatter&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;reduce_bucket_size&quot;</span><span class="punctuation">:</span> <span class="number">5e7</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;allgather_bucket_size&quot;</span><span class="punctuation">:</span> <span class="number">5e7</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;cpu_offload&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;zero_allow_untested_optimizer&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;fp16&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;enabled&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;loss_scale&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;loss_scale_window&quot;</span><span class="punctuation">:</span> <span class="number">1000</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;hysteresis&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;min_loss_scale&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;optimizer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Adam&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;params&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;lr&quot;</span><span class="punctuation">:</span> <span class="number">5e-6</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;betas&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="number">0.9</span><span class="punctuation">,</span></span><br><span class="line">        <span class="number">0.95</span></span><br><span class="line">      <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;eps&quot;</span><span class="punctuation">:</span> <span class="number">1e-8</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;weight_decay&quot;</span><span class="punctuation">:</span> <span class="number">1e-2</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;activation_checkpointing&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;partition_activations&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;contiguous_memory_optimization&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;wall_clock_breakdown&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="模型及优化器初始化"><a href="#模型及优化器初始化" class="headerlink" title="模型及优化器初始化"></a>模型及优化器初始化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">model, optimizer, _, _ = deepspeed.initialize(</span><br><span class="line">    model=model,</span><br><span class="line">    model_parameters=param_groups,</span><br><span class="line">    args=args,</span><br><span class="line">    mpu=mpu,</span><br><span class="line">    dist_init_required=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="DeepSpeed训练"><a href="#DeepSpeed训练" class="headerlink" title="DeepSpeed训练"></a>DeepSpeed训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deepspeed --hostfile $&#123;HOST_FILE_PATH&#125; --master_port $&#123;MASTER_PORT&#125; --num_nodes $&#123;NUM_WORKERS&#125; --num_gpus $&#123;NUM_GPUS_PER_WORKER&#125;<span class="string">&quot; finetune_glm.py</span></span><br></pre></td></tr></table></figure>

<p>以GLM为例，使用自己的数据训练</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bash scripts/ds_finetune_seq2seq.sh \ </span><br><span class="line">   config_tasks/model_blocklm_10B.sh \ </span><br><span class="line">   config_tasks/seq_customization.sh</span><br></pre></td></tr></table></figure>

<h2 id="ZeRO概述"><a href="#ZeRO概述" class="headerlink" title="ZeRO概述"></a>ZeRO概述</h2><p>Zero Redundancy Optimizer (ZeRO)是DeepSpeed的workhorse. 用户可以提供不同的ZeRO config文件，来实现DeepSpeed的不同功能特性。</p>
<p>来看一下<a target="_blank" rel="noopener" href="https://deepspeed.readthedocs.io/en/latest/zero3.html#getting-started">官网教程</a>对ZeRO的描述：</p>
<blockquote>
<p>The Zero Redundancy Optimizer (ZeRO) removes the memory redundancies across data-parallel processes by partitioning the three model states (optimizer states, gradients, and parameters) across data-parallel processes instead of replicating them. By doing this, it boosts memory efficiency compared to classic data-parallelism while retaining its computational granularity and communication efficiency.</p>
</blockquote>
<p>一句话总结： <code>partitioning instead of replicating</code>，<strong>划分而不是复制</strong>。</p>
<p>即，传统的<a target="_blank" rel="noopener" href="https://aitechtogether.com/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">深度学习</a>，模型训练并行，是将模型参数复制多份到多张GPU上，只将数据拆分（如，torch的Dataparallel），这样就会有大量的显存冗余浪费。而ZeRO就是为了消除这种冗余，提高对memory的利用率。注意，这里的“memory”不仅指多张GPU memory，还包括CPU。</p>
<p>而ZeRO的实现方法，就是把参数占用，逻辑上分成三种类型。将这些类型的参数划分：</p>
<ul>
<li><code>optimizer states</code>：即优化器的参数状态。例如，Adam的动量参数。</li>
<li><code>gradients</code>：梯度缓存，对应于optimizer。</li>
<li><code>parameters</code>：模型参数。</li>
</ul>
<p>对应的，DeepSpeed的ZeRO config文件就可以分为如下几类：</p>
<ul>
<li><code>ZeRO Stage 1</code>: 划分optimizer states。优化器参数被划分到多个memory上，每个momoey上的进程只负责更新它自己那部分参数。</li>
<li><code>ZeRO Stage 2</code>: 划分gradient。每个memory，只保留它分配到的optimizer state所对应的梯度。这很合理，因为梯度和optimizer是紧密联系在一起的。只知道梯度，不知道optimizer state，是没有办法优化模型参数的。</li>
<li><code>ZeRO Stage 3</code>: 划分模型参数，或者说，不同的layer. ZeRO-3会在forward和backward的时候，自动将模型参数分配到多个memory。</li>
</ul>
<p>由于ZeRO-1只分配optimizer states(参数量很小)，实际使用的时候，<strong>我们一般只会考虑<code>ZeRO-2</code>和<code>ZeRO-3</code>。</strong></p>
<p>接下来介绍stage 2和3的常用config文件。</p>
<h3 id="3-2-ZeRO-Stage-2"><a href="#3-2-ZeRO-Stage-2" class="headerlink" title="3.2 ZeRO Stage 2"></a>3.2 ZeRO Stage 2</h3><p>结合官网的介绍，笔者提供一个常用的ZeRO-stage-2的config文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;bfloat16&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: &quot;auto&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;fp16&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: &quot;auto&quot;,</span><br><span class="line">        &quot;loss_scale&quot;: 0,</span><br><span class="line">        &quot;loss_scale_window&quot;: 1000,</span><br><span class="line">        &quot;initial_scale_power&quot;: 16,</span><br><span class="line">        &quot;hysteresis&quot;: 2,</span><br><span class="line">        &quot;min_loss_scale&quot;: 1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;optimizer&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;AdamW&quot;,</span><br><span class="line">        &quot;params&quot;: &#123;</span><br><span class="line">            &quot;lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;betas&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;eps&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;weight_decay&quot;: &quot;auto&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;scheduler&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;WarmupLR&quot;,</span><br><span class="line">        &quot;params&quot;: &#123;</span><br><span class="line">            &quot;warmup_min_lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;warmup_max_lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;warmup_num_steps&quot;: &quot;auto&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;zero_optimization&quot;: &#123;</span><br><span class="line">        &quot;stage&quot;: 2,</span><br><span class="line">        &quot;offload_optimizer&quot;: &#123;</span><br><span class="line">            &quot;device&quot;: &quot;cpu&quot;,</span><br><span class="line">            &quot;pin_memory&quot;: true</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;allgather_partitions&quot;: true,</span><br><span class="line">        &quot;allgather_bucket_size&quot;: 2e8,</span><br><span class="line">        &quot;overlap_comm&quot;: true,</span><br><span class="line">        &quot;reduce_scatter&quot;: true,</span><br><span class="line">        &quot;reduce_bucket_size&quot;: 2e8,</span><br><span class="line">        &quot;contiguous_gradients&quot;: true</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;gradient_accumulation_steps&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;gradient_clipping&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;train_batch_size&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;train_micro_batch_size_per_gpu&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;steps_per_print&quot;: 1e5</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>有关于<code>offload</code></li>
</ul>
<p>上述参数中，最重要的一个就是<code>&quot;offload_optimizer&quot;</code>。如上述所示，我们将其<code>”device“</code>设置成了cpu，DeepSpeed就会按照之前提到过的ZeRO操作，在训练过程中，将优化器状态分配到cpu上。从而降低单张GPU的memory占用。</p>
<ul>
<li>有关于<code>overlap_comm</code></li>
</ul>
<p>另外一个需要提到的参数是<code>overlap_comm</code>。简单地理解，它控制着多个memory上进程之间通信的buffer的大小。这个值越大，进程之间通信越快，模型训练速度也会提升，但相应的显存占用也会变大；反之亦然。</p>
<p>因此，<code>overlap_comm</code>也是一个需要进行一定权衡的参数。</p>
<h3 id="3-3-ZeRO-Stage-3"><a href="#3-3-ZeRO-Stage-3" class="headerlink" title="3.3 ZeRO Stage 3"></a>3.3 ZeRO Stage 3</h3><p>和Stage-2类似，笔者也提供一个stage-3的模板config</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;bfloat16&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: false</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;fp16&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: &quot;auto&quot;,</span><br><span class="line">        &quot;loss_scale&quot;: 0,</span><br><span class="line">        &quot;loss_scale_window&quot;: 1000,</span><br><span class="line">        &quot;initial_scale_power&quot;: 16,</span><br><span class="line">        &quot;hysteresis&quot;: 2,</span><br><span class="line">        &quot;min_loss_scale&quot;: 1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;optimizer&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;AdamW&quot;,</span><br><span class="line">        &quot;params&quot;: &#123;</span><br><span class="line">            &quot;lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;betas&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;eps&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;weight_decay&quot;: &quot;auto&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;scheduler&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;WarmupLR&quot;,</span><br><span class="line">        &quot;params&quot;: &#123;</span><br><span class="line">            &quot;warmup_min_lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;warmup_max_lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;warmup_num_steps&quot;: &quot;auto&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;zero_optimization&quot;: &#123;</span><br><span class="line">        &quot;stage&quot;: 3,</span><br><span class="line">        &quot;offload_optimizer&quot;: &#123;</span><br><span class="line">            &quot;device&quot;: &quot;cpu&quot;,</span><br><span class="line">            &quot;pin_memory&quot;: true</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;offload_param&quot;: &#123;</span><br><span class="line">            &quot;device&quot;: &quot;cpu&quot;,</span><br><span class="line">            &quot;pin_memory&quot;: true</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;overlap_comm&quot;: true,</span><br><span class="line">        &quot;contiguous_gradients&quot;: true,</span><br><span class="line">        &quot;sub_group_size&quot;: 1e9,</span><br><span class="line">        &quot;reduce_bucket_size&quot;: &quot;auto&quot;,</span><br><span class="line">        &quot;stage3_prefetch_bucket_size&quot;: &quot;auto&quot;,</span><br><span class="line">        &quot;stage3_param_persistence_threshold&quot;: &quot;auto&quot;,</span><br><span class="line">        &quot;stage3_max_live_parameters&quot;: 1e9,</span><br><span class="line">        &quot;stage3_max_reuse_distance&quot;: 1e9,</span><br><span class="line">        &quot;stage3_gather_fp16_weights_on_model_save&quot;: true</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;gradient_accumulation_steps&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;gradient_clipping&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;steps_per_print&quot;: 1e5,</span><br><span class="line">    &quot;train_batch_size&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;train_micro_batch_size_per_gpu&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;wall_clock_breakdown&quot;: false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>有关于<code>“offload_param”</code></li>
</ul>
<p>可以看到，除了和stage2一样，有<code>offload_optimizer</code>参数之外，stage3还有一个<code>offload_param</code>参数。即，将模型参数进行划分。</p>
<ul>
<li>stage-3相关的其他参数</li>
</ul>
<p>下面这些参数是stage-3-specific的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;sub_group_size&quot;: 1e9,</span><br><span class="line">&quot;reduce_bucket_size&quot;: &quot;auto&quot;,</span><br><span class="line">&quot;stage3_prefetch_bucket_size&quot;: &quot;auto&quot;,</span><br><span class="line">&quot;stage3_param_persistence_threshold&quot;: &quot;auto&quot;,</span><br><span class="line">&quot;stage3_max_live_parameters&quot;: 1e9,</span><br><span class="line">&quot;stage3_max_reuse_distance&quot;: 1e9,</span><br><span class="line">&quot;stage3_gather_fp16_weights_on_model_save&quot;: true</span><br></pre></td></tr></table></figure>

<h3 id="4-2-内存估计"><a href="#4-2-内存估计" class="headerlink" title="4.2 内存估计"></a>4.2 内存估计</h3><p>如之前多次强调的，DeepSpeed使用过程中的一个难点，就在于<code>时间和空间</code>的权衡。</p>
<p>分配更多参数到CPU上，虽然能够降低显存开销，但是也会极大地提升时间开销。</p>
<p>DeepSpeed提供了一段简单的memory估算代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from transformers import AutoModel</span><br><span class="line">from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live</span><br><span class="line"></span><br><span class="line">## specify the model you want to train on your device</span><br><span class="line">model = AutoModel.from_pretrained(&quot;t5-large&quot;)</span><br><span class="line">## estimate the memory cost (both CPU and GPU)</span><br><span class="line">estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=1, num_nodes=1)</span><br></pre></td></tr></table></figure>

<p>如上，如果不用stage2和stage3(最下面那两行)，训练T5-large需要一张显存至少为12.49GB的显卡(考虑到很多其他的缓存变量，还有你的batch_size，实际上可能需要24GB大小的卡)。而在相继使用了stage2和3之后，显存开销被极大地降低，转而CPU内存消耗显著提升，模型训练时间开销也相应地增大。</p>
<p><em><strong>建议:</strong></em><br>在使用DeepSpeed之前，先使用上述代码，大概估计一下显存消耗，决定使用的GPU数目，以及ZeRO-stage。</p>
<p>原则是，<strong>能直接多卡训练，就不要用ZeRO；能用ZeRO-2就不要用ZeRO-3.</strong></p>
<p>笔者尝试使用DeepSpeed进行模型的训练。</p>
<p>首先是stage 2，也就是只把optimizer放到cpu上。下面是使用前后的GPU显存占用和训练速度对比：</p>
<ul>
<li>GPU显存：<code>20513</code>MB &#x3D;&gt; <code>17349</code>MiB</li>
<li>训练速度 (由<code>tqdm</code>估计)：<code>1.3</code> iter&#x2F;s &#x3D;&gt; <code>0.77</code> iter&#x2F;s</li>
</ul>
<p>可以明显看到，GPU的显存占用有了明显降低，但是训练速度也变慢了。以笔者当前的使用体感来说，deepspeed并没有带来什么收益。</p>
<p>笔者的机器配有<code>24000</code>MB的显卡，batch_size为2时，占用<code>20513</code>MB；而DeepSpeed仅仅帮助笔者空出了<code>3000</code>MB的显存，<strong>还是完全不够增加batch_size</strong>, 导致笔者总训练时长变长。</p>
<p>因此，DeepSpeed或许仅适用于显存极度短缺（i.e., 模型大到 batch_size &#x3D;&#x3D; 1也跑不了）的情况；亦或是，使用DeepSpped节省下来的显存，刚好够支持更大的batch_size。否则，像笔者当前这种情况下，使用DeepSpeed只会增加时间开销，并没有其他益处。</p>
<p>此后，笔者还尝试使用stage 3，但是<strong>速度极其缓慢</strong>。一个原先需要6h的训练过程，用了DeepSpeed stage3之后，运行了2天2夜，也没有结束的迹象。无奈笔者只好终止测试。</p>
<p>此外，在使用DeepSpeed stage2时，由于分配了模型参数到多个设备上，console里面也看不到任何输出信息（但是GPU还是在呼呼响，utility也为100%），让人都不知道程序的运行进度，可以说对用户非常不友好了。</p>
<p>由于DeepSpeed会通过占用CPU内存来减缓GPU的开销，当系统CPU不够的时候，DeepSpeed进程就会自动被系统停止，<strong>造成没有任何报错，DeepSpeed无法启动的现象</strong>。建议先用上文介绍的<a target="_blank" rel="noopener" href="https://aitechtogether.com/article/45439.html#jump">estimation</a>估计一下CPU内存占用，然后用<code>free -h</code>查看一下机器的CPU内存空余量，来判断能否使用DeepSpeed。</p>
<p>另外，还有可能因为训练精度问题，出现loss为<code>NAN</code>的情况。详见：<a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/main_classes/deepspeed#troubleshooting">Troubleshooting</a>.</p>
<p>使用DeepSpeed stage2之后，就不能灵活地更改optimizer了。下图是DeepSpeed.py的源代码：<br><a target="_blank" rel="noopener" href="https://img-blog.csdnimg.cn/24c2ee1e7328450e940702291ef551f2.png"><img src="https://img-blog.csdnimg.cn/24c2ee1e7328450e940702291ef551f2.png" alt="在这里插入图片描述"></a><br>默认optimizer必须在config里面设置好，也就是使用默认的优化器和学习率，不能实现分组学习率。如果要自定义optimizer的初始化过程，必须实现两个版本的optimizer（CPU+GPU）。如官方所述：</p>
<p>并行训练技术主要是如何使用多块显卡并行训练模型，主要分为三种并行方式：数据并行（Data Parallel）、张量并行（Tensor Parallel）和流水线并行（Pipeline Parallel）。</p>
<h2 id="数据并行"><a href="#数据并行" class="headerlink" title="数据并行"></a>数据并行</h2><p>数据并行是目前最常见的并行方式，它的核心思想是对数据进行划分，将数据分配给不同GPU进行计算。在数据并行中，每个GPU上存储打的模型、优化器状态是相同的。每块GPU上执行玩前后向传播后，会将每个GPU是哪个计算出的梯度汇总求平均。</p>
<p><img src="/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D%20-%20%E5%89%AF%E6%9C%AC/2.jpg"></p>
<p>为了提高预训练模型的泛化能力，近年来预训练模型的一个趋势是参数量在快速增大，目前已经到达万亿规模。但如此大的参数量会使得模型训练变得十分困难，于是不少的相关研究者和机构对此提出了许多大模型高效训练的技术。本文将分为三部分来介绍大模型高效训练所需要的主要技术：并行训练技术、显存优化技术和其他技术。文章最后会展示当前较为流行的训练加速库的统计。欢迎大家批评指正，相互交流。</p>
<p>当前在 GPU 集群上常用的并行计算方式包括：数据并行、模型并行和流水线并行。我们可以混合使用这三种并行计算方式，即「混合并行」。对于 GPU 数量不足的使用者而言，微软 DeepSpeed 框架中提出的并行优化组件 The Zero Redundancy Optimizer (ZeRO) 和 ZeRO-Offload 技术可以混合使用 GPU 和 CPU，降低显存的压力。</p>
<h3 id="数据并行：将训练数据分而治之"><a href="#数据并行：将训练数据分而治之" class="headerlink" title="数据并行：将训练数据分而治之"></a><strong>数据并行：将训练数据分而治之</strong></h3><p>数据并行是一种简单的并行计算方法，其思想是在分布式计算集群中的各个计算节点上复制一份相同的模型参数，进而在各个计算节点上使用相同的模型对各自接收到的输入数据进行计算。根据具体的实现方式，我们又可以将「数据并行」分为「同步训练」和「异步训练」模式。</p>
<h3 id="模型并行与流水线并行：将模型-由大化小"><a href="#模型并行与流水线并行：将模型-由大化小" class="headerlink" title="模型并行与流水线并行：将模型****由大化小"></a><strong>模型并行与流水线并行：将模型****由大化小</strong></h3><p>尽管数据并行往往可以有效提升计算效率，但是该方法无法拓展模型的参数。如果一个模型的参数量已经大到 GPU 显存无法存下，那么仅靠数据并行就无法解决显存不足的问题。此时，我们需要采用模型并行和流水线并行计算方法。</p>
<p>模型并行的主要思想是：将模型进行切分，然后将其分配到多个计算节点上，以减少单个计算节点的参数量。常见的切分方法是：将模型的每一层参数平均切分到多个计算节点上。该方式能够减少单个计算节点的参数量和计算量，但会引入大量的通信和同步开销。</p>
<p>流水线并行是另一种模型并行实现方式，它指的是：将模型按层为粒度进行切分，并且将不同层的参数分配给各个计算节点。流水线并行在对模型参数进行切分的同时，能够降低节点之间的通信量。但是，在我们启动和停止流水线之间的时间里，会有部分节点处于等待状态，这在某种程度上也导致了部分算力的浪费。</p>
<p>模型并行与流水线并行各有优劣，一般需要根据显卡、机器之间的通信速度来决定如何使用。</p>
<p>DeepSpeed是一个开源深度学习训练优化库，包含新的显存优化技术——ZeRO(零冗余优化器)通过扩大规模，提升速度，控制成本，提升可用性，极大地推进了大模型训练能力。</p>
<p>主流并行计算框架一览</p>
<p><img src="/2023/03/12/deepspeed%E4%BB%8B%E7%BB%8D%20-%20%E5%89%AF%E6%9C%AC/1.png"></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://aitechtogether.com/article/45439.html">https://aitechtogether.com/article/45439.html</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/414773915">https://zhuanlan.zhihu.com/p/414773915</a></p>
<p><a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/">https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/</a></p>
<p><a target="_blank" rel="noopener" href="https://www.guyuehome.com/39789">https://www.guyuehome.com/39789</a></p>
<p><a target="_blank" rel="noopener" href="https://toutiao.io/posts/p7cpkm6/preview">https://toutiao.io/posts/p7cpkm6/preview</a></p>
<p><a target="_blank" rel="noopener" href="https://hub.baai.ac.cn/view/7771">https://hub.baai.ac.cn/view/7771</a></p>
<p><a target="_blank" rel="noopener" href="https://www.kuxai.com/article/476">https://www.kuxai.com/article/476</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/yaohaishen/article/details/127471992">https://blog.csdn.net/yaohaishen/article/details/127471992</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/489134718">https://zhuanlan.zhihu.com/p/489134718</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/343570325">https://zhuanlan.zhihu.com/p/343570325</a></p>
<p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/distributed-parallel-training-data-parallelism-and-model-parallelism-ec2d234e3214">https://towardsdatascience.com/distributed-parallel-training-data-parallelism-and-model-parallelism-ec2d234e3214</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u010751000/article/details/123516433">https://blog.csdn.net/u010751000/article/details/123516433</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/450689346">https://zhuanlan.zhihu.com/p/450689346</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/yqw0710/p/16060765.html">https://www.cnblogs.com/yqw0710/p/16060765.html</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/02/15/chat-gpt/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Coder4nlp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/02/15/chat-gpt/" class="post-title-link" itemprop="url">chat_gpt</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-02-15 00:11:34" itemprop="dateCreated datePublished" datetime="2023-02-15T00:11:34+08:00">2023-02-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-02-21 19:39:42" itemprop="dateModified" datetime="2023-02-21T19:39:42+08:00">2023-02-21</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="ChatGPT"><a href="#ChatGPT" class="headerlink" title="ChatGPT"></a>ChatGPT</h1><h2 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h2><ol>
<li><p>【GPT-1】<strong>Improving Language Understanding by Generative Pre-Training.</strong></p>
<p><em>Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever</em> [<a target="_blank" rel="noopener" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">pdf</a>] 2018.6</p>
</li>
<li><p>【GPT-2】<strong>Language Models are Unsupervised Multitask Learners.</strong></p>
<p><em>Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario Amodei, Ilya Sutskeve</em> [<a target="_blank" rel="noopener" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">pdf</a>] 2019.2</p>
</li>
<li><p>【GPT-3】<strong>Language Models are Few-Shot Learners.</strong></p>
<p><em>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei</em> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.14165">pdf</a>] 2020.5</p>
</li>
<li><p>【InstructGPT】<strong>Training language models to follow instructions with human feedback.</strong></p>
<p><em>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe</em> [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.02155.pdf">pdf</a>] 2022.3</p>
</li>
<li><p>【RLHF】<strong>Augmenting Reinforcement Learning with Human Feedback.</strong></p>
<p><em>W. Bradley Knox, Peter Stone</em> [<a target="_blank" rel="noopener" href="https://www.cs.utexas.edu/~ai-lab/pubs/ICML_IL11-knox.pdf">pdf</a>] 2011.7</p>
</li>
<li><p>【PPO】<strong>Proximal Policy Optimization Algorithms.</strong></p>
<p><em>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov</em> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.06347">pdf</a>] 2017.7</p>
</li>
<li><p>【LaMda】 <strong>LaMDA: Language Models for Dialog Applications.</strong></p>
<p><em>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le</em> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2201.08239">pdf</a>] 2022.1</p>
</li>
<li><p>【Sparrow】 <strong>Improving alignment of dialogue agents via targeted human judgements.</strong></p>
<p><em>Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Soňa Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, Geoffrey Irving</em> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2209.14375">pdf</a>] 2022.9</p>
</li>
<li><p><strong>Fine-tuning language models from human preferences.</strong></p>
<p><em>Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, Geoffrey Irving</em> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.08593">pdf</a>][<a target="_blank" rel="noopener" href="https://github.com/openai/lm-human-preferences">code</a>] 2019.9</p>
</li>
<li><p><strong>Learning to summarize from human feedback.</strong></p>
<p><em>Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano</em> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2009.01325">pdf</a>] [<a target="_blank" rel="noopener" href="https://github.com/openai/summarize-from-feedback">code</a>] 2020.9</p>
</li>
<li><p><strong>Cross-task generalization via natural language crowdsourcing instructions.</strong></p>
<p><em>Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi</em> [<a target="_blank" rel="noopener" href="https://aclanthology.org/2022.acl-long.244/">pdf]</a> 2021.4</p>
</li>
<li><p><strong>Finetuned language models are zero-shot learners</strong></p>
<p><em>Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, Quoc V. Le</em> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2109.01652">pdf]</a> 2021.9</p>
</li>
<li><p><strong>Multitask Prompted Training Enables Zero-Shot Task Generalization.</strong></p>
<p><em>Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, Alexander M. Rush</em> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2110.08207">pdf]</a> 2021.10</p>
</li>
<li><p><strong>Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks.</strong></p>
<p><em>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, Daniel Khashabi</em> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2204.07705">pdf]</a> 2022.4</p>
</li>
<li><p><strong>Putting Humans in the Natural Language Processing Loop: A Survey.</strong></p>
<p><em>Zijie J. Wang, Dongjin Choi, Shenyu Xu, Diyi Yang</em> [<a target="_blank" rel="noopener" href="https://aclanthology.org/2021.hcinlp-1.8.pdf">pdf</a>] 2021.4</p>
</li>
<li><p><strong>Scaling Instruction-Finetuned Language Models.</strong></p>
<p><em>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei</em> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.11416">pdf]</a> 2022.10</p>
</li>
<li><p><strong>A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity.</strong></p>
<p><em>Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, Pascale Fung</em> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.04023">pdf</a>] 2023.2</p>
</li>
<li><p><strong>Is ChatGPT a General-Purpose Natural Language Processing Task Solver?</strong></p>
<p><em>Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang</em> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.06476">pdf</a>] 2023.2</p>
</li>
</ol>
<h2 id="Transformer-进化史"><a href="#Transformer-进化史" class="headerlink" title="Transformer 进化史"></a>Transformer 进化史</h2><p><img src="/2023/02/15/chat-gpt/1.png"></p>
<p><a target="_blank" rel="noopener" href="https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/">Transformer models: an introduction and catalog — 2023 Edition</a></p>
<hr>
<h1 id="Training-language-models-to-follow-instructions-with-human-feedback"><a href="#Training-language-models-to-follow-instructions-with-human-feedback" class="headerlink" title="Training language models to follow instructions with human feedback"></a>Training language models to follow instructions with human feedback</h1><hr>
<p>InstructGPT</p>
<p>时间：</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>使语言模型更大并不能使它们更好地遵循用户的意图。例如，大型语言模型可能生成不真实、有害或对用户毫无帮助的输出。换句话说，这些模型并没有与它们的用户对齐。在本文中，我们展示了一种方法，通过对人类反馈进行微调，使语言模型与用户意图在广泛的任务中保持一致。从一组标注员编写的提示和通过OpenAI API收集到的提示开始，我们收集了所需模型行为的标注器演示数据集，我们使用监督学习对GPT-3进行微调。然后，我们收集模型输出的排名数据集，使用从人类反馈中强化学习来进一步微调这个监督模型。我们将这个模型称为<strong>InstructGPT</strong>。在对提示分布的人类评估中，来自1.3B参数InstructGPT模型的输出优于来自175B GPT-3的输出，尽管它的参数少了100倍。此外，InstructGPT模型显示了真实性的改善和有毒产出的减少，同时在公开数据集上没有明显变差。尽管InstructGPT仍然会犯一些简单的错误，但我们的结果表明，使用人类反馈进行微调是使语言模型符合人类意图是一个有前途的方向。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>大型语言模型(LMs)可以通过“提示”，即给出一些任务示例作为输入，执行一系列自然语言处理(NLP)任务。然而，这些模型经常表达意想不到的行为，如编造事实、生成有偏见或有毒的文本，或根本不遵循用户指示。这是因为最近许多语言模型所使用目标——从互联网上预测网页上的下一个token与“有帮助地、安全地听从用户指示”的目标不同。因此，我们说语言模型的目标是不一致的。避免这些意想不到的行为对于在数百个应用程序中部署和使用的语言模型尤其重要。</p>
<p>通过训练语言模型按照用户的意图行事，我们在对齐语言模型方面取得了进展。这既包括明确的意图，如遵循指示，也包括隐含的意图，如保持诚实，不偏见，有毒或其他有害。我们希望语言模型是有帮助的（helpful）(它们应该帮助用户解决他们的任务)，诚实的（honest）(它们不应该编造信息或误导用户)，无害的（harmless）(它们不应该对人或环境造成身体、心理或社会伤害)。我们将在第3.6节详细说明这些标准的评估。</p>
<p>我们将重点关注调整语言模型的微调方法。具体来说，我们使用来自人类反馈的强化学习(RLHF，reinforcement learning from human feedback)微调GPT-3遵循广泛的书面指令(参见图2)（OpenAI是强化学习出家）。这项技术使用人类的偏好作为奖励信号来微调我们的模型。我们首先雇佣了一个由40名承包商组成的团队，根据他们在筛选测试中的表现来标记我们的数据(详情请参阅第3.4节和附录B.1)。然后，我们收集了提交给OpenAI API的人工编写的所需输出行为(主要是英语)演示数据集和一些标签编写的提示，并使用它来训练我们的监督学习基线。接下来，我们在更大的一组API提示上收集一个数据集，用于比较我们模型的输出。然后，我们在这个数据集上训练一个奖励模型(RM，Reward Model )，以预测我们的标签者更喜欢哪种模型输出。最后，我们使用这个RM作为奖励函数，并使用PPO算法微调我们的监督学习基线，以最大化这个奖励。我们在图2中说明了这个过程。这一过程使GPT-3的行为与特定人群(主要是我们的标签者和研究人员)的偏好相一致，而不是任何更广泛的“人类价值观”概念;我们将在第5.2节对此进行进一步讨论。我们将结果模型称为InstructGPT。</p>
<p><img src="/2023/02/15/chat-gpt/2.PNG"></p>
<p>标了2个数据，训练3个模型</p>
<p>找到人写问题，或者api中收集的问题，也就是prompt</p>
<p>然后人类写回答，这个数据做有监督的微调，step1</p>
<p>step2: 给一个prompt，模型输出多个答案，人类对答案进行排序。</p>
<p>有这个模型以后，这个模型可以对输出内容进行打分</p>
<p>step3：继续微调STF，使得生成的答案能够得到一个比较高的分数。</p>
<p>如果在step1中生成足够多的数据，不需要step2,3应该也是可行的，但是考虑到写一个答案做这种生成式的标注远远这种判别式的标注，step2可以使数据标注更加简单，更快更多的标注数据。</p>
<p>我们主要通过让标签师对我们测试集上的模型输出质量进行评分来评估我们的模型，包括来自固定客户的提示(他们没有在训练数据中表示)。我们还对一系列公开NLP数据集进行自动评估。使用GPT-3框架训练了3个尺寸的模（1.3B, 6B, and 175B parameters）型。<strong>主要发现如下</strong>：</p>
<ul>
<li><p><strong>与GPT-3的输出相比，标注员明显更喜欢InstructGPT输出</strong>。在我们的测试集中，来自1.3B参数的InstructGPT模型输出优于来自175B GPT-3的输出，尽管它的参数要少100倍以上。这些模型具有相同的架构，不同之处在于InstructGPT对我们的人类数据进行了微调。即使我们在GPT-3中添加了一些few-shot以使它更好地遵循指令，这个结果也仍然成立。我们的175B InstructGPT的输出比175B GPT-3的输出更优(85±3%)，比few-shot 175B GPT-3更优(71±4%)。InstructGPT模型还根据我们的标签生成更合适的输出，并且更可靠地遵循指令中的显式约束。</p>
</li>
<li><p><strong>InstructGPT模型的真实性比GPT-3有所提高。</strong>在TruthfulQA基准测试中，InstructGPT生成真实且信息丰富的答案的频率大约是GPT-3的两倍。在未针对GPT-3进行反向选择的问题子集中，我们的结果同样强大。在我们的API prompt distribution的“闭域”任务中，输出不应该包含在输入中不存在的信息(例如总结和闭域QA)， InstructGPT模型在输入中不存在的信息大约是GPT-3的一半(分别是21%和41%的虚幻率（hallucination rate）)。</p>
</li>
<li><p><strong>InstructGPT的毒性比GPT-3略有改善，但没有偏倚。</strong>为了测量毒性，我们使用了RealToxicityPrompts数据集(Gehman等人，2020年)，并进行了自动和人工评估。当提示要尊重时，InstructGPT模型产生的有毒输出比GPT-3少约25%。在Winogender (Rudinger et al.， 2018)和CrowSPairs (Nangia et al.， 2020)数据集上，与GPT-3相比，InstructGPT没有显著改善。</p>
</li>
<li><p>在公开数据集上没有太大影响</p>
</li>
<li><p>我们的模型推广到不产生任何训练数据的“保留”标签者的偏好。为了检验我们的模型的泛化性，我们对测试者进行了一个初步的实验，发现他们更喜欢InstructGPT的输出而不是GPT-3的输出，与我们的训练者的输出率大致相同。然而，还需要更多的工作来研究这些模型在更广泛的用户群体中如何表现，以及在人们对期望行为存在分歧的输入中如何表现。</p>
</li>
<li><p>公共NLP数据集并不能反映我们的语言模型是如何使用的。我们比较了在我们的人类偏好数据(即InstructGPT)上微调的GPT-3和在公开NLP任务上微调的GPT-3: FLAN和T0(特别是T0++变体)。这些数据集由各种NLP任务组成，并结合了每个任务的自然语言指令。在我们的API prompt distribution中，我们的FLAN和T0模型的表现比我们的SFT基线略差，并且标签者明显更喜欢InstructGPT(与我们的基线相比，InstructGPT的胜率为73.4±2%，而我们的T0和FLAN版本分别为26.8±2%和29.8±2%)。</p>
</li>
<li><p>InstructGPT模型对RLHF调优分布之外的指令显示出很好的泛化效果。我们定性地探究了InstructGPT的功能，发现它能够遵循用于总结代码的指令，回答关于代码的问题，有时还遵循不同语言的指令，尽管这些指令在微调发行版中非常罕见。相比之下，GPT-3可以执行这些任务，但需要更仔细的提示，并且通常不遵循这些领域中的说明。这个结果令人兴奋，因为它表明我们的模型能够概括“遵循指令”的概念。即使在很少得到直接监督信号的任务上，他们也会保持一定的一致性。</p>
</li>
<li><p>InstructGPT仍然会犯简单的错误。例如，InstructGPT仍然可能无法遵循指示、编造事实、对简单问题给出冗长的模棱两可的答案，或者无法检测带有错误前提的指示。</p>
</li>
<li></li>
<li><p>总的来说，</p>
</li>
</ul>
<p>总的来说，我们的结果表明，使用人类偏好微调大型语言模型显著改善了它们在广泛任务上的行为，尽管仍有许多工作要做，以提高它们的安全性和可靠性。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>我们的方法是跟之前的工作一样的，在</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>产生了三个不同的数据集用于我们的微调过程：</p>
<ul>
<li>我们的SFT数据集，使用标签演示来训练我们的SFT模型，SFT数据集包含大约13k个训练提示(来自API和标签编写)，</li>
<li>我们的RM数据集，使用模型输出的标签排名来训练我们的RM，SRM数据集有33k个训练提示(来自API和标签编写)</li>
<li>我们的PPO数据集，没有任何人类标签，用作RLHF微调的输入。PPO数据集有31k个训练提示(仅来自API)。关于数据集大小的更多细节见表6。</li>
</ul>
<p>我们的提示数据集主要由提交给OpenAI API的文本提示组成，特别是那些在Playground界面上使用早期版本的InstructGPT模型(通过我们演示数据子集的监督学习进行训练)的提示</p>
<p>标注人员写prompt,API 收集的数据</p>
<p>。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/02/09/Pytorch%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Coder4nlp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/02/09/Pytorch%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90/" class="post-title-link" itemprop="url">Pytorch显存分析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-02-09 13:11:54 / Modified: 13:13:12" itemprop="dateCreated datePublished" datetime="2023-02-09T13:11:54+08:00">2023-02-09</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Pytorch显存分析"><a href="#Pytorch显存分析" class="headerlink" title="Pytorch显存分析"></a>Pytorch显存分析</h1><p>在深度学习中，常见的错误是CUDA out of memory，这是因为显存放不下那么多模型参数和中间变量。通过及时清空中间变量，优化代码，减少batch等能够减少显存溢出。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2023/02/09/Pytorch%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/02/01/ERNIE-layout/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Coder4nlp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/02/01/ERNIE-layout/" class="post-title-link" itemprop="url">ERNIE-layout</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-02-01 00:45:20 / Modified: 12:16:32" itemprop="dateCreated datePublished" datetime="2023-02-01T00:45:20+08:00">2023-02-01</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="ERNIE-Layout-Layout-Knowledge-Enhanced-Pre-training-for-Visually-rich-Document-Understanding"><a href="#ERNIE-Layout-Layout-Knowledge-Enhanced-Pre-training-for-Visually-rich-Document-Understanding" class="headerlink" title="ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding"></a>ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>近年来，在富文档理解方面，已经见证了预训练技术的兴起和成功。然而，现有的大多数方法缺乏对以布局为中心的知识的系统挖掘和利用，导致性能不佳。论文提出了ERNIE-Layout，这是一种新颖的文档预训练解决方案，在整个工作流程中增强布局知识，以学习更好的表示方式，结合文本、布局和图像的特征。具体来说，我们首先在序列化阶段对输入序列进行重新排列，然后提出相关的预训练任务——阅读顺序预测，学习文档的正确阅读顺序。为了提高模型的布局意识，我们在多模态transformer中集成了空间感知解耦注意力，在预训练阶段集成了区域替换预测任务。实验结果表明ERNIE-Layout在各种下游任务上实现了卓越的性能，在关键信息提取、文档图像分类和文档问答数据集上达到新的技术水平。代码和模型可以在PaddleNLP上公开获取。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2023/02/01/ERNIE-layout/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/15/vision-language/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Coder4nlp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/01/15/vision-language/" class="post-title-link" itemprop="url">Vision-Language Pretraining &#58 Current Trends and the Future</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-01-15 21:27:17 / Modified: 23:05:16" itemprop="dateCreated datePublished" datetime="2023-01-15T21:27:17+08:00">2023-01-15</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>由<a target="_blank" rel="noopener" href="https://www.iro.umontreal.ca/~agrawal/">Aishwarya Agrawal</a> (DeepMind，蒙特利尔大学，Mila)， <a target="_blank" rel="noopener" href="http://www.damienteney.info/">Damien Teney</a> (Idiap研究所)和<a target="_blank" rel="noopener" href="http://www.aidanematzadeh.me/">Aida Nematzadeh</a> (DeepMind)编写的ACL 2022教程。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2023/01/15/vision-language/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/10/Hugging-Face%E7%9A%84Datasets-%E5%BA%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Coder4nlp">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Coder4nlp's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/01/10/Hugging-Face%E7%9A%84Datasets-%E5%BA%93/" class="post-title-link" itemprop="url">Hugging Face的Datasets 库</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-01-10 23:39:09" itemprop="dateCreated datePublished" datetime="2023-01-10T23:39:09+08:00">2023-01-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-01-11 00:22:28" itemprop="dateModified" datetime="2023-01-11T00:22:28+08:00">2023-01-11</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><code>Datasets</code>是一个轻量级库，提供两个主要特性:</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2023/01/10/Hugging-Face%E7%9A%84Datasets-%E5%BA%93/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Coder4nlp</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
