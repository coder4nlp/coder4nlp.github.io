[{"title":"ERNIE-layout","url":"/2023/02/01/ERNIE-layout/","content":"<h1 id=\"ERNIE-Layout-Layout-Knowledge-Enhanced-Pre-training-for-Visually-rich-Document-Understanding\"><a href=\"#ERNIE-Layout-Layout-Knowledge-Enhanced-Pre-training-for-Visually-rich-Document-Understanding\" class=\"headerlink\" title=\"ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding\"></a>ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding</h1><h2 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h2><p>近年来，在富文档理解方面，已经见证了预训练技术的兴起和成功。然而，现有的大多数方法缺乏对以布局为中心的知识的系统挖掘和利用，导致性能不佳。论文提出了ERNIE-Layout，这是一种新颖的文档预训练解决方案，在整个工作流程中增强布局知识，以学习更好的表示方式，结合文本、布局和图像的特征。具体来说，我们首先在序列化阶段对输入序列进行重新排列，然后提出相关的预训练任务——阅读顺序预测，学习文档的正确阅读顺序。为了提高模型的布局意识，我们在多模态transformer中集成了空间感知解耦注意力，在预训练阶段集成了区域替换预测任务。实验结果表明ERNIE-Layout在各种下游任务上实现了卓越的性能，在关键信息提取、文档图像分类和文档问答数据集上达到新的技术水平。代码和模型可以在PaddleNLP上公开获取。</p>\n<span id=\"more\"></span>\n\n<p>VrDU：Visually-rich Document Understanding，NLU：Natural Language Understanding</p>\n<h2 id=\"问题及方案\"><a href=\"#问题及方案\" class=\"headerlink\" title=\"问题及方案\"></a>问题及方案</h2><h3 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h3><p>文档格式的多样性和复杂性对任务提出了新的挑战，理想的模型需要充分利用文本、布局甚至视觉信息，像人类一样充分理解视觉丰富的文档。</p>\n<p>VrDU的早期工作常采用单模态或浅多模态融合方法，这些方法是针对特定任务的，需要大量标注数据。最近预训练语言模型已经席卷了这个领域。LayoutLM、 LayoutLMv2、以及一些先进的文档预训练方法相继提出，并在各种VrDU任务中取得了巨大成功。与流行的单模态或视觉语言（Vision-Language）框架不同，<strong>文档理解模型的独特性在于如何利用布局知识。</strong>然而，现有的文档预训练解决方案通常会陷入将二维坐标作为一维位置扩展的陷阱，赋予模型布局感知能力。考虑到VrDU的特点，我们认为以布局为中心的知识应该从两个方面进行系统的挖掘和利用：</p>\n<p>（1）一方面，布局隐式地反映了文档的正确阅读顺序，而以往的方法都是将光学字符识别(OCR)的结果进行多路复用，大致按照从上到下、从左到右的方式排列token。对于具有复杂布局的文档(例如，表格、表单、多列模板)，这与人类的阅读习惯不一致，并导致下游任务的性能不佳。（2）另一方面，<strong>布局实际上是语言和语言之外的第三种形式</strong>，而目前的模型通常将布局作为一种特殊的位置特征，例如嵌入在输入层中的布局(LayoutLM)或注意层中的偏向项(LayoutLMv2)。布局与文本&#x2F;图像之间缺乏跨模态交互可能会限制模型学习布局在语义表达中的作用。</p>\n<h3 id=\"方案\"><a href=\"#方案\" class=\"headerlink\" title=\"方案\"></a>方案</h3><p>（1）首先，我们在序列化阶段采用现成的基于布局的文档解析器，为每个输入文档生成合适的阅读顺序，使模型接收到的输入序列比使用粗略的光栅扫描顺序更符合人类的阅读习惯。</p>\n<p>（2）然后，每个文本&#x2F;视觉token都配备了其位置嵌入和布局嵌入，并送到堆叠的多模态transformer层。受DeBERTa的解耦注意力启发， 我们提出了一种空间感知的解耦注意力机制，其中token之间的注意力权重是根据它们的隐藏状态和相对位置使用解耦矩阵计算的。最后，布局不仅作为输入token的二维位置属性，而且为语义相似度的计算提供了一个空间视角。</p>\n<h2 id=\"模型结构\"><a href=\"#模型结构\" class=\"headerlink\" title=\"模型结构\"></a>模型结构</h2><p>Ernie-layout 整体采用 Transformer Encoder 架构。</p>\n<p><img src=\"/2023/02/01/ERNIE-layout/image9.PNG\"></p>\n<h3 id=\"序列模块\"><a href=\"#序列模块\" class=\"headerlink\" title=\"序列模块\"></a>序列模块</h3><p>用Document-Parse作为先验知识告诉模型阅读的顺序（a layout-knowledge enhanced pre-training approach），将每个token加入阅读顺序的特征。PPL被广泛用于度量语言模型的性能。通过Document-Parser序列化的输入序列的PPL比光栅扫描顺序的PPL低。</p>\n<h3 id=\"输入表示\"><a href=\"#输入表示\" class=\"headerlink\" title=\"输入表示\"></a>输入表示</h3><p>ERNIE-Layout的输入序列包括文本部分和视觉部分，每个部分的表示是其模态特征和布局嵌入的组合。</p>\n<p><strong>文本嵌入</strong>（Text Embedding）。序列化模块之后的文档token用作文本序列。</p>\n<p>在BERT-Style模型的预处理之后，两个特殊标记[CLS]和[SEP]分别附加在文本序列的开头和结尾。最后，token序列T的文本嵌入表示为:<br>$$<br>T&#x3D;E_{tk}(T)+E_{1p}(T)+E_{tp}(T)<br>$$<br>这里$E_{tk}$、$E_{1p}$、$E_{tp}$分别是token embedding、1D position embedding以及token type  embedding。其中采用可学习的 position_embeddings 。position_ids通过 OCR 工具获得。采用 [Layout-Parser](<a href=\"https://github.com/Layout-Parser/\">https://github.com/Layout-Parser/</a> layout-parser) 对图片中的文本内容，根据阅读顺序进行排序，安排对应的 position_ids。</p>\n<p><strong>视觉嵌入（Visual Embedding）</strong>用 Faster-RCNN 当作encoder，图片先resize成224×224 ，然后池化得到7x7的feature,之后flaten成视觉序列，特征线性映射到text embedding 同样的维度。同样地，此外还键入position embedding和token type embedding。<br>$$<br>V&#x3D;F_{vs}(V)+E_{1p}(T)+E_{tp}(T)<br>$$</p>\n<p><strong>Layout Embedding</strong>。对于每个文本token，OCR工具提供包含边界框宽度和高度的2D坐标$(x_0,y_0,x_1,y_1,w,h)$，$(x_0,y_0)$表示左上角的坐标，$(x_1,y_1)$表示右下角的坐标。$w&#x3D;x_1-x_0$，$h&#x3D;y_1-y_0$，所有坐标规范化到$[0,1000]$，使用两个嵌入层表示横坐标和纵坐标。<br>$$<br>L&#x3D;E_{2x}(x_0,x_1,W)+E_{2y}(y_0,y_1,H)<br>$$<br>$E_{2x}$表示x轴嵌入层，$E_{2y}$表示y轴嵌入层。</p>\n<p>为了获得ERNIE-Layout最终的输入，我们将每个文本和视觉嵌入和它们对应的布局嵌入整合到一起。最终的序列长度是$N+HW$，文本和视觉与相关的Layout Embedding相加后concat。<br>$$<br>H&#x3D;[T+L;V+L]<br>$$</p>\n<h3 id=\"多模态Transformer\"><a href=\"#多模态Transformer\" class=\"headerlink\" title=\"多模态Transformer\"></a>多模态Transformer</h3><p>在最终的输入表示中，文本和视觉token被拼接在一起，Transformer的自注意机制支持它们的层感知跨模式交互。但是，作为一种独特的模态，在计算注意力权重时需要考虑布局特征，并明确考虑布局特征与内容(统称文字和图像)之间的紧密性。受DeBERTa 解耦注意力的启发，其中token之间的注意力权重是使用其内容上的解耦矩阵计算的。</p>\n<p>以1D位置为例，token $i$和token $j$的相对距离为$\\delta_{1p}$如下：<br>$$<br>y&#x3D; \\begin{cases}<br>0,\\quad &amp; i-j\\leq -k \\<br>2k-1,\\quad &amp;i-j\\ge k \\<br>i-j +k,\\quad &amp; others<br>\\end{cases}<br>$$<br>2D位置的相对距离同理。</p>\n<p>而后计算上下文-上下文，上下文-1D 位置信息, 上下文-2D 位置信息对应的 attention 权重：<br>$$<br>A_{ij}^{ct,ct}&#x3D;Q_i^{ct}K_l^{ct} \\<br>A_{ij}^{ct,1p}&#x3D;Q_i^{ct}K_{\\delta_{1p}(i,j)}^{1p} + {K_{j}^{ct}Q_{\\delta_{1p}(j,i)}^{1p}}^{\\top} \\<br>A_{ij}^{ct,2x}&#x3D;Q_i^{ct}K_{\\delta_{2x}(i,j)}^{2x} + {K_{j}^{ct}Q_{\\delta_{2x}(j,i)}^{2x}}^{\\top} \\<br>A_{ij}^{ct,2y}&#x3D;Q_i^{ct}K_{\\delta_{2y}(i,j)}^{2y} + {K_{j}^{ct}Q_{\\delta_{2y}(j,i)}^{2y}}^{\\top} \\<br>$$<br>最后，将所有这些注意得分进行汇总，得到注意矩阵$\\hat A $。通过缩放和归一化操作，空间感知解耦注意力的输出为：<br>$$<br>\\hat A_{ij}&#x3D;A_{ij}^{ct,ct}+A_{ij}^{ct,1p}+A_{ij}^{ct,2x}+A_{ij}^{ct,2y} \\<br>H_{out}&#x3D;softmax (\\frac{\\hat A}{\\sqrt 3d})<br>$$</p>\n<p><strong>issue 可以确认，paddlenlp 开源的 ernie-layoutx 为论文的降级版，其中的注意力模块、输入embedding模块等均与论文描述的不同。</strong></p>\n<blockquote>\n<p>ernie-layout代码中的实现:<a href=\"https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/transformers/ernie_layout/modeling.py#L315\">https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/transformers/ernie_layout/modeling.py#L315</a><br>这个attention和layoutlmv2一样的吧？</p>\n<p>考虑到商用，目前为降级开源，当前开出版本仍旧好于LayoutXLM，后续会适当的时候对外开源spatial-aware disentangled attention版本，如商业有需求请联系<a href=\"https://ai.baidu.com/tech/nlp/Textanalysis\">https://ai.baidu.com/tech/nlp/Textanalysis</a></p>\n</blockquote>\n<h2 id=\"预训练任务\"><a href=\"#预训练任务\" class=\"headerlink\" title=\"预训练任务\"></a>预训练任务</h2><p>ERNIE-Layout采用了四种预训练任务，包括新提出的阅读顺序预测（reading order prediction）、区域替换预测（replaced region prediction tasks）任务和传统的masked视觉语言建模、文本-图像对齐任务。</p>\n<ul>\n<li><p><strong>Reading Order Prediction：</strong> 希望注意矩阵能携带关于阅读顺序的知识，通过这种方式，我们赋予$\\hat A_{ij}$一个额外的含义，即第j个token是第i个token的下一个token的概率。此外，ground truth是一个0-1矩阵G，其中1表示两个token之间存在阅读顺序关系，反之亦然。对于最后一个token，下一个token是它自己。在预训练阶段，使用交叉熵损失函数。<br>$$<br>L_{ROP}&#x3D;-\\sum_{0\\le i &lt; N}\\sum_{0\\le j &lt; N}G_{ij}log(\\hat {A_{ij}})<br>$$</p>\n</li>\n<li><p><strong>Replaced Region Prediction：</strong> 在视觉编码器中，每个文档图像被处理成一个固定长度HW的序列。为了使模型能够感知图像块和文本之间的细粒度对应，在布局知识的帮助下，我们提出了区域替换预测(RRP)。具体来说，随机选取10%的图像块，用另一个图像中的块替换，处理后的图像由视觉编码器编码并输入到多模态 transformer中。然后，使用 transformer输出的[CLS]向量来预测哪些补丁被替换。所以这个任务的损失是：<br>$$<br>L_{RRP}&#x3D;-\\sum_{0 \\le i &lt;HW}[G_ilog(P_i)+(1-G_i)*log(1-P_i)]<br>$$<br>$G_i$是替换的图像块，$P_i$是规范化的概率。</p>\n</li>\n<li><p><strong>Masked Visual-Language Modeling</strong> ：类似 MLM（masked language modeling），MVLM（masked visual-language modeling）目标是根据masked文本上下文和整个多模态线索恢复masked文本token。</p>\n</li>\n<li><p><strong>Text-Image Alignment</strong> ：除了图像侧跨模态任务RRP，我们还采用了文本-图像对齐(TIA,Text-Image Alignment)作为文本侧任务，帮助模型学习图像区域与边界框坐标之间的空间对应关系。在这里，一些文本行是随机选择的，它们对应的区域覆盖在文档图像上。然后，引入分类层来预测每个文本标记是否被覆盖。</p>\n</li>\n</ul>\n<p>预训练任务的最终目标是<br>$$<br>L&#x3D;L_{ROP}+L_{RRP}+L_{MVLM}+L_{TIA}<br>$$</p>\n<h2 id=\"实验设置\"><a href=\"#实验设置\" class=\"headerlink\" title=\"实验设置\"></a>实验设置</h2><p>ERNIE-Layout有24个transformer层，1024个隐藏单元和16个注意头。文本token的最大序列长度为512，视觉token的最大序列长度为49。transformer初始化自RoBERTa large，视觉编码器采用Faster-RCNN 作为初始化模型。其余参数随机初始化。我们使用Adam作为优化器，学习率为1e-4，权值衰减为0.01。学习率在前10%的步骤中线性升温，然后线性衰减到0。ERNIE-Layout在24个Tesla A100 gpu上训练了20个epoch，batch大小为576。</p>\n<h2 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h2><p>为了实验的公平性，我们只使用序列化来重新排列预训练数据的读取顺序，这意味着ERNIE-Layout在微调阶段接收到的输入与比较方法中输入是一样的。</p>\n<h3 id=\"Key-Information-Extraction\"><a href=\"#Key-Information-Extraction\" class=\"headerlink\" title=\"Key Information Extraction\"></a>Key Information Extraction</h3><p>ERNIE-Layout在FUNSD, CORD, Kleister-NDA上实现STOA，并在SROIE上实现了具有竞争力的性能。值得一提的是，在FUNSD中，ERNIE-Layout较之前的最佳结果获得了7.98%的显著稳定改善(标准差为0.0011)。以上现象足以验证我们在文档预训练模型中挖掘和利用布局知识的设计理念的有效性。</p>\n<p><img src=\"/2023/02/01/ERNIE-layout/image1.PNG\"></p>\n<p><img src=\"/2023/02/01/ERNIE-layout/image7.PNG\"></p>\n<h3 id=\"与最新的LayoutLMv3对比一下\"><a href=\"#与最新的LayoutLMv3对比一下\" class=\"headerlink\" title=\"与最新的LayoutLMv3对比一下\"></a>与最新的LayoutLMv3对比一下</h3><p><img src=\"/2023/02/01/ERNIE-layout/image8.PNG\"></p>\n<center><strong>LayoutLMv3 Large与ERNIE-layout Large 对比</strong></center>\n\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>FUNSD</th>\n<th>CORD</th>\n<th>DocVQA</th>\n<th>RVL-CDIP</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>LayoutLMv3 large</strong></td>\n<td>92.08</td>\n<td><strong>97.46</strong></td>\n<td>83.37</td>\n<td>95.93</td>\n</tr>\n<tr>\n<td><strong>ERNIE-layout large</strong></td>\n<td><strong>93.12</strong></td>\n<td>97.21</td>\n<td><strong>88.41</strong></td>\n<td><strong>96.27</strong></td>\n</tr>\n</tbody></table>\n<h3 id=\"Document-Question-Answering\"><a href=\"#Document-Question-Answering\" class=\"headerlink\" title=\"Document Question Answering\"></a>Document Question Answering</h3><p>表4列出了平均归一化Levenshtein相似度(ANLS, Average Normalized Levenshtein Similarity) DocVQA评分。</p>\n<p>请注意，LayoutLMv2(#7)是基于UniLMv2(#3)开发的，该模型具有强大的问答能力，甚至在任务上击败了多模型LayoutLM(#4)。不幸的是，UniLMv2没有公开任何预训练代码或预训练模型，我们只能使用RoBERTa的参数来初始化我们的ERNIE-Layout。然而，我们感到惊讶的是ERNIE-Layout带来了令人兴奋的性能改进，(几乎是LayoutLMv2增加的两倍)。此外，我们使用模型集成在DocVQA排行榜上获得了第一名。</p>\n<p><img src=\"/2023/02/01/ERNIE-layout/image2.PNG\"></p>\n<h3 id=\"Document-Image-Classification\"><a href=\"#Document-Image-Classification\" class=\"headerlink\" title=\"Document Image Classification\"></a>Document Image Classification</h3><p>与这些重点关注多模态语义理解的关键信息提取或文档问答任务不同，文档图像分类需要对文本内容和文档布局的宏观感知。尽管我们的预训练任务关注的是细粒度的跨模态匹配，ERNIE-Layout仍然刷新跨粒度任务的最佳性能。</p>\n<img src=\"/2023/02/01/ERNIE-layout/image3.PNG\" style=\"zoom:67%;\">\n\n\n\n<h2 id=\"预训练分析\"><a href=\"#预训练分析\" class=\"headerlink\" title=\"预训练分析\"></a>预训练分析</h2><h3 id=\"预训练任务的分析\"><a href=\"#预训练任务的分析\" class=\"headerlink\" title=\"预训练任务的分析\"></a>预训练任务的分析</h3><p>在这个实验中，我们从基本的MVLM任务开始实现基线模型(#1)，并集成新的任务逐步直到最终模型包含所有四个训练任务(#5)。从表6中，我们观察到RRP带来了0.95%的改善FUNSD，展示了细粒度跨模态的相互作用。当加入ROP时，FUNSD的性能进一步提高1.3%。我们认为ROP有助于模型学习阅读顺序知识的更好表示。</p>\n<p><img src=\"/2023/02/01/ERNIE-layout/image4.PNG\"></p>\n<h3 id=\"注意力机制的有效性\"><a href=\"#注意力机制的有效性\" class=\"headerlink\" title=\"注意力机制的有效性\"></a>注意力机制的有效性</h3><p>LayoutLMv2 最初提出了空间感知的自注意，在注意力计算中考虑布局特征，后续许多方法都遵循这一思路。从表6中，我们发现采用这种机制可以提高下游任务的性能(#4 v.s. #6)。与此同时，将注意力分散到位置和内容部分是获得进一步性能提升的另一个有效解决方案(#5 v.s. #6)。</p>\n<h3 id=\"序列化模块的有效性\"><a href=\"#序列化模块的有效性\" class=\"headerlink\" title=\"序列化模块的有效性\"></a>序列化模块的有效性</h3><p>在这里，我们将探讨使用不同的序列化模块对下游VrDU任务的影响。如表7所示，使用基于布局知识的序列化模块(#2，#3)，模型可以获得更好的性能(即使没有解耦注意力)。我们将这种改进归因于这样一个事实:尽管序列化没有用于微调数据集，但在预训练后，模型有能力理解文档的正确阅读顺序。</p>\n<img src=\"/2023/02/01/ERNIE-layout/image5.PNG\" style=\"zoom:67%;\">\n\n\n\n<p>如图，一个复杂文档布局，使用raster-scanning order序列化为“… Session Chair: Session Chair: Session Chair: Tuula Hakkarainen …”，而使用Document-Parser序列化为：“… Session Chair: Tuula wz Session Chair: Frank Markert …”，这更符合人类的阅读习惯。</p>\n<img src=\"/2023/02/01/ERNIE-layout/image6.PNG\" style=\"zoom:67%;\">\n\n<p>使用Document-Parser序列化后，文档的PPL大幅下降。</p>\n","tags":["NLP"]},{"title":"Hugging Face的Datasets 库","url":"/2023/01/10/Hugging-Face%E7%9A%84Datasets-%E5%BA%93/","content":"<p><code>Datasets</code>是一个轻量级库，提供两个主要特性:</p>\n<span id=\"more\"></span>\n\n<ul>\n<li>用于许多公开数据集的单行<strong>dataloaders</strong>：只需一行程序即可下载和预处理<code>HuggingFace</code>数据集中心提供的任何数量的主要公开数据集(图像数据集、音频数据集、467种语言和方言的文本数据集等)。使用一个简单的命令，如<code>squad_dataset = load_dataset(&quot;squad&quot;)</code>，即可获取任何这些数据集，准备在数据加载器中使用，用于训练&#x2F;评估ML模型(<code>Numpy/Pandas/PyTorch/TensorFlow/JAX</code>)</li>\n<li>高效的数据预处理：简单，快速和可重复的数据预处理，用于公开数据集以及您自己的<code>CSV</code>, <code>JSON</code>，<code>text</code>，<code>PNG</code>，<code>JPEG</code>，<code>WAV</code>,，<code>MP3</code>, <code>Parquet</code>等本地数据集。使用<code>processed_dataset = dataset.map(process_example)</code>这样的简单命令，可以有效地为ML模型评估和训练准备数据集。</li>\n</ul>\n<p>Datasets旨在让社区轻松添加和共享新的数据集。</p>\n<p><code>Datasets</code>有许多其他有趣的特性:</p>\n<ul>\n<li><p>在大型数据集上发展：数据集自然地将用户从RAM内存限制中解放出来，所有数据集都使用高效的<code>zero-serialization cost backend</code>(Apache Arrow)进行内存映射。</p>\n</li>\n<li><p>智能缓存：永远不要等待您的数据处理多次。</p>\n</li>\n<li><p>轻量级且快速，使用透明的<code>python API</code>(多处理&#x2F;缓存&#x2F;内存映射)。</p>\n</li>\n<li><p>内置与<code>NumPy</code>, <code>Pandas</code>, <code>PyTorch</code>, <code>Tensorflow 2</code>和<code>JAX</code>的互操作性。</p>\n</li>\n<li><p>对音频和图像数据的原生支持。</p>\n</li>\n<li><p>启用流模式以节省磁盘空间并立即开始遍历数据集。</p>\n<p><code>Datasets</code>起源于很棒的<a href=\"https://github.com/tensorflow/datasets\">TensorFlow Datasets</a> 的一个分支，HuggingFace团队想要深切感谢<code>TensorFlow Datasets</code>团队构建这个惊人的库。关于<code>Datasets</code>和<code>tfds</code>之间差异的更多详细信息，可以在<a href=\"https://github.com/huggingface/datasets#main-differences-between--datasets-and-tfds\">Main differences between 🤗 Datasets and <code>tfds</code></a>部分中找到。</p>\n</li>\n</ul>\n<h1 id=\"从hub加载一个数据\"><a href=\"#从hub加载一个数据\" class=\"headerlink\" title=\"从hub加载一个数据\"></a>从hub加载一个数据</h1><p>找到可复现和可访问的高质量数据集可能很困难。<code>Datasets</code>的主要目标之一是提供一种简单的方法来加载任何格式或类型的数据集。最简单的入门方法是在<a href=\"https://huggingface.co/datasets\">Hugging Face Hub</a> 上发现现有数据集(一个社区驱动的数据集集合，用于自然语言处理、计算机视觉和音频任务)，并使用<code>Datasets</code>下载并生成数据集。</p>\n<p>本教程使用<a href=\"https://huggingface.co/datasets/rotten_tomatoes\">rotten_tomatoes</a>和<a href=\"https://huggingface.co/datasets/PolyAI/minds14\">MInDS-14</a>数据集，但请随意加载任何你想要的数据集并跟随。现在前往Hub，为您的任务找到一个数据集!</p>\n<h2 id=\"加载一个数据\"><a href=\"#加载一个数据\" class=\"headerlink\" title=\"加载一个数据\"></a>加载一个数据</h2><p>在花时间下载数据集之前，快速获取关于数据集的一些一般信息通常是有帮助的。数据集的信息存储在 <a href=\"https://huggingface.co/docs/datasets/v2.6.1/en/package_reference/main_classes#datasets.DatasetInfo\">DatasetInfo</a>中，可以包括数据集描述、特征和数据集大小等信息。</p>\n<p>使用<a href=\"https://huggingface.co/docs/datasets/v2.6.1/en/package_reference/loading_methods#datasets.load_dataset_builder\">load_dataset_builder()</a>函数加载数据集构建器，并检查数据集的属性，而不是下载数据集:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">from datasets import load_dataset_builder</span><br><span class=\"line\">ds_builder = load_dataset_builder(&quot;rotten_tomatoes&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">ds_builder.info.description</span><br><span class=\"line\"></span><br><span class=\"line\">ds_builder.info.features</span><br></pre></td></tr></table></figure>\n\n<p>如果你对数据集满意，那么用<code>load_dataset()</code>加载它:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">from datasets import load_dataset</span><br><span class=\"line\">dataset = load_dataset(&quot;rotten_tomatoes&quot;, split=&quot;train&quot;)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"划分\"><a href=\"#划分\" class=\"headerlink\" title=\"划分\"></a>划分</h2><p>一个划分是数据集的特定子集(如<code>train</code>和<code>test</code>)。使用<code>get_dataset_split_names()</code>函数列出数据集的拆分名称:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> datasets <span class=\"keyword\">import</span> get_dataset_split_names</span><br><span class=\"line\"></span><br><span class=\"line\">get_dataset_split_names(<span class=\"string\">&quot;rotten_tomatoes&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<p>然后你可以用<code>split</code>参数加载一个特定的划分。加载一个数据集划分返回一个<code>Dataset</code>对象:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> datasets <span class=\"keyword\">import</span> load_dataset</span><br><span class=\"line\"></span><br><span class=\"line\">dataset = load_dataset(<span class=\"string\">&quot;rotten_tomatoes&quot;</span>, split=<span class=\"string\">&quot;train&quot;</span>)</span><br><span class=\"line\">dataset</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">Dataset(&#123;</span><br><span class=\"line\">    features: [&#x27;text&#x27;, &#x27;label&#x27;],</span><br><span class=\"line\">    num_rows: 8530</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>如果你没有指定<code>split</code>，<code>Datasets</code>会返回一个<code>DatasetDict</code>对象:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">from datasets import load_dataset</span><br><span class=\"line\"></span><br><span class=\"line\">dataset = load_dataset(&quot;rotten_tomatoes&quot;)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">DatasetDict(&#123;</span><br><span class=\"line\">    train: Dataset(&#123;</span><br><span class=\"line\">        features: [&#x27;text&#x27;, &#x27;label&#x27;],</span><br><span class=\"line\">        num_rows: 8530</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">    validation: Dataset(&#123;</span><br><span class=\"line\">        features: [&#x27;text&#x27;, &#x27;label&#x27;],</span><br><span class=\"line\">        num_rows: 1066</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">    test: Dataset(&#123;</span><br><span class=\"line\">        features: [&#x27;text&#x27;, &#x27;label&#x27;],</span><br><span class=\"line\">        num_rows: 1066</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n\n\n\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p><a href=\"https://huggingface.co/docs/datasets/load_hub\">https://huggingface.co/docs/datasets/load_hub</a></p>\n<p><a href=\"https://huggingface.co/docs/datasets/index\">https://huggingface.co/docs/datasets/index</a></p>\n"},{"title":"Python中的ChainMap","url":"/2023/04/23/Python%E4%B8%AD%E7%9A%84ChainMap/","content":"<h1 id=\"ChainMap介绍\"><a href=\"#ChainMap介绍\" class=\"headerlink\" title=\"ChainMap介绍\"></a>ChainMap介绍</h1><p><code>collections.ChainMap</code>用于快速链接多个映射，并将它们视为一个单元。它通常比创建一个新字典并运行多次<code>update()</code>调用要快得多。</p>\n<p><code>ChainMap</code>的作用：</p>\n<ul>\n<li><code>ChainMap</code>将多个字典或其他映射组合在一起，以创建一个可更新的单一视图。如果没有指定映射，则提供一个空字典，以便新链始终至少有一个映射。</li>\n<li>底层映射存储在一个列表中。该列表是公共的，可以使用maps属性访问或更新。</li>\n<li>查找时，依次搜索底层映射，直到找到一个键。<strong>但是，写、更新和删除只对第一个映射进行操作！！！</strong></li>\n<li><code>ChainMap</code>通过引用合并底层映射。因此，如果某个底层映射更新了，这些更改将反映在<code>ChainMap</code>中。</li>\n<li>支持所有常用的字典方法。</li>\n</ul>\n<span id=\"more\"></span>\n\n<p>下面介绍<code>ChainMap</code>中常见的属性和方法。</p>\n<h2 id=\"maps\"><a href=\"#maps\" class=\"headerlink\" title=\"maps\"></a>maps</h2><p><code>maps</code>属性：用户可更新的映射列表。列表从最先搜索到最后搜索。它是唯一存储的状态，可以通过修改来更改要搜索的映射。<strong>该列表应始终包含至少一个映射</strong>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> ChainMap</span><br><span class=\"line\">a = <span class=\"built_in\">dict</span>(one=<span class=\"number\">1</span>, two=<span class=\"number\">2</span>)</span><br><span class=\"line\">b = <span class=\"built_in\">dict</span>(three=<span class=\"number\">3</span>, four=<span class=\"number\">4</span>)</span><br><span class=\"line\">chain_map = ChainMap(a, b)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(chain_map)</span><br><span class=\"line\"><span class=\"comment\"># ChainMap(&#123;&#x27;one&#x27;: 1, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(chain_map.maps)</span><br><span class=\"line\"><span class=\"comment\"># [&#123;&#x27;one&#x27;: 1, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;]</span></span><br><span class=\"line\"><span class=\"comment\"># 修改a中的值,chain_map会同步更新</span></span><br><span class=\"line\">a[<span class=\"string\">&quot;one&quot;</span>] = <span class=\"number\">10</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(chain_map)</span><br><span class=\"line\"><span class=\"comment\"># ChainMap(&#123;&#x27;one&#x27;: 10, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\"><span class=\"comment\"># 通过chain_map修改，a同步更新 </span></span><br><span class=\"line\">chain_map[<span class=\"string\">&quot;one&quot;</span>] = <span class=\"number\">20</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(a)</span><br><span class=\"line\"><span class=\"comment\"># &#123;&#x27;one&#x27;: 20, &#x27;two&#x27;: 2&#125;</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"new-child方法\"><a href=\"#new-child方法\" class=\"headerlink\" title=\"new_child方法\"></a><code>new_child</code>方法</h2><p><code>new_child(m=None, **kwargs)</code>：返回一个新的<code>ChainMap</code>，包含一个新的map，后面跟着当前实例中的所有<code>map</code>。如果指定了<code>m</code>，它将成为映射列表前面的新映射；如果未指定，则使用一个空字典，因此调用<code>.new_child()</code> 等效于\t<code>ChainMap(&#123;&#125;, *d.maps)</code>。如果指定了任何关键字参数，则更新传递的map或新的空字典。此方法用于创建子上下文，可以在不更改任何父映射中的值的情况下进行更新。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(chain_map)</span><br><span class=\"line\"><span class=\"comment\"># ChainMap(&#123;&#x27;one&#x27;: 20, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\">new_chain_map1 = chain_map.new_child()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(new_chain_map1)</span><br><span class=\"line\"><span class=\"comment\"># ChainMap(&#123;&#125;, &#123;&#x27;one&#x27;: 20, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\">c = <span class=\"built_in\">dict</span>(five=<span class=\"number\">5</span>)</span><br><span class=\"line\">new_chain_map2 = chain_map.new_child(c) </span><br><span class=\"line\"><span class=\"built_in\">print</span>(new_chain_map2)</span><br><span class=\"line\"><span class=\"comment\"># ChainMap(&#123;&#x27;five&#x27;: 5&#125;, &#123;&#x27;one&#x27;: 20, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"parents\"><a href=\"#parents\" class=\"headerlink\" title=\"parents\"></a>parents</h2><p><code>parents</code>属性：返回一个新的<code>ChainMap</code>，其中包含当前实例中除第一个映射外的所有映射。这对于跳过搜索中的第一个<code>map</code>非常有用。用例类似于嵌套作用域中使用的nonlocal关键字。这些用例也与内置super()函数的用例并行。对<code>d.parents</code>的引用相当于:<code>ChainMap(* d.maps[1:])</code>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(chain_map)</span><br><span class=\"line\"><span class=\"comment\"># ChainMap(&#123;&#x27;one&#x27;: 20, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\">new_chain_map3 = chain_map.parents</span><br><span class=\"line\"><span class=\"built_in\">print</span>(new_chain_map3)</span><br><span class=\"line\"><span class=\"comment\"># ChainMap(&#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br></pre></td></tr></table></figure>\n\n<p>注意，<code>ChainMap()</code>的迭代顺序是通过<strong>扫描从后到前的映射</strong>来确定的:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(chain_map)</span><br><span class=\"line\"><span class=\"comment\"># ChainMap(&#123;&#x27;one&#x27;: 20, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\">keys = <span class=\"built_in\">list</span>(chain_map)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(keys)</span><br><span class=\"line\"><span class=\"comment\"># [&#x27;three&#x27;, &#x27;four&#x27;, &#x27;one&#x27;, &#x27;two&#x27;]</span></span><br></pre></td></tr></table></figure>\n\n<p><code>ChainMap</code>类只对链中**的第一个映射进行更新(写入和删除)**，而查找将搜索整个链。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(chain_map)</span><br><span class=\"line\"><span class=\"comment\"># ChainMap(&#123;&#x27;one&#x27;: 20, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\">keys = <span class=\"built_in\">list</span>(chain_map)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(keys)</span><br><span class=\"line\"><span class=\"comment\"># [&#x27;three&#x27;, &#x27;four&#x27;, &#x27;one&#x27;, &#x27;two&#x27;]</span></span><br><span class=\"line\">chain_map[<span class=\"string\">&quot;three&quot;</span>] = <span class=\"number\">300</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(chain_map)</span><br><span class=\"line\"><span class=\"comment\"># ChainMap(&#123;&#x27;one&#x27;: 20, &#x27;two&#x27;: 2, &#x27;three&#x27;: 300&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br></pre></td></tr></table></figure>\n\n<p>因此，如果需要深度写入和删除，需要创建一个子类来更新链中更深的键。上面的例子中，修改字典中的<code>key=&#39;three&#39;</code>时，因为第一个字典没有对应的，所以新增了一个<code>map</code>。如果想修改第二个字典，可以通过下面的方式实现。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> ChainMap</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">DeepChainMap</span>(<span class=\"title class_ inherited__\">ChainMap</span>):</span><br><span class=\"line\">    <span class=\"string\">&#x27;Variant of ChainMap that allows direct updates to inner scopes&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__setitem__</span>(<span class=\"params\">self, key, value</span>):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> mapping <span class=\"keyword\">in</span> self.maps:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> key <span class=\"keyword\">in</span> mapping:</span><br><span class=\"line\">                mapping[key] = value</span><br><span class=\"line\">                <span class=\"keyword\">return</span></span><br><span class=\"line\">        self.maps[<span class=\"number\">0</span>][key] = value</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__delitem__</span>(<span class=\"params\">self, key</span>):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> mapping <span class=\"keyword\">in</span> self.maps:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> key <span class=\"keyword\">in</span> mapping:</span><br><span class=\"line\">                <span class=\"keyword\">del</span> mapping[key]</span><br><span class=\"line\">                <span class=\"keyword\">return</span></span><br><span class=\"line\">        <span class=\"keyword\">raise</span> KeyError(key)</span><br><span class=\"line\">        </span><br><span class=\"line\">a = <span class=\"built_in\">dict</span>(one=<span class=\"number\">1</span>, two=<span class=\"number\">2</span>)</span><br><span class=\"line\">b = <span class=\"built_in\">dict</span>(three=<span class=\"number\">3</span>, four=<span class=\"number\">4</span>)</span><br><span class=\"line\">chain_map = ChainMap(a, b)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(chain_map) <span class=\"comment\"># ChainMap(&#123;&#x27;one&#x27;: 1, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\"></span><br><span class=\"line\">chain_map[<span class=\"string\">&quot;three&quot;</span>] = <span class=\"number\">300</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(chain_map)</span><br><span class=\"line\"><span class=\"comment\"># ChainMap(&#123;&#x27;one&#x27;: 1, &#x27;two&#x27;: 2, &#x27;three&#x27;: 300&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\"></span><br><span class=\"line\">c = <span class=\"built_in\">dict</span>(one=<span class=\"number\">1</span>, two=<span class=\"number\">2</span>)</span><br><span class=\"line\">d = <span class=\"built_in\">dict</span>(three=<span class=\"number\">3</span>, four=<span class=\"number\">4</span>)</span><br><span class=\"line\">deep_chain_map = DeepChainMap(c, d)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(deep_chain_map)</span><br><span class=\"line\"><span class=\"comment\"># DeepChainMap(&#123;&#x27;one&#x27;: 1, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\"></span><br><span class=\"line\">deep_chain_map[<span class=\"string\">&quot;three&quot;</span>] = <span class=\"number\">300</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(deep_chain_map)</span><br><span class=\"line\"><span class=\"comment\"># DeepChainMap(&#123;&#x27;one&#x27;: 1, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 300, &#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">del</span> deep_chain_map[<span class=\"string\">&#x27;three&#x27;</span>]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(deep_chain_map)</span><br><span class=\"line\"><span class=\"comment\"># DeepChainMap(&#123;&#x27;one&#x27;: 1, &#x27;two&#x27;: 2&#125;, &#123;&#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>1、<code>ChainMap</code>可接受多个映射，然后在逻辑上使它们表现为一个单独的映射结构；它只是维护了一个记录底层映射关系的列表，然后去重定义常用的字典操作；</p>\n<p>2、如果有重复的键，会采用第一个映射中对应的值；</p>\n<p>3、修改<code>ChainMap</code>映射结构，会同时作用在自己和原始字典结构上；</p>\n<p>4、可以使用字典的<code>update()</code>方法，来替代上面的合并方案；但是这就需要创建一个新的字典对象(或者修改原字典，破坏了原始数据)，并且原始字典做了修改，并不会反映到新建的字典上；</p>\n<p>5、<code>ChainMap</code>使用的就是原始字典，因此原字典变，它也会改变。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p><a href=\"https://docs.python.org/3/library/collections.html#chainmap-examples-and-recipes\">https://docs.python.org/3/library/collections.html#chainmap-examples-and-recipes</a></p>\n<p><a href=\"https://cloud.tencent.com/developer/article/1597933\">https://cloud.tencent.com/developer/article/1597933</a></p>\n","tags":["Python"]},{"title":"Python中的Iterables vs Iterators","url":"/2023/04/23/Python%E4%B8%AD%E7%9A%84Iterables-vs-Iterators/","content":"<h1 id=\"Python中的Iterables-vs-Iterators\"><a href=\"#Python中的Iterables-vs-Iterators\" class=\"headerlink\" title=\"Python中的Iterables vs Iterators\"></a>Python中的Iterables vs Iterators</h1><p>术语<code>iterable</code>和<code>iterator</code>经常(错误地)互换使用，以描述支持迭代的对象，即允许迭代其元素的对象。实际上，<code>Python</code>中的迭代器(<code>iterators</code>)和可迭代对象(<code>iterables</code>)是两个截然不同的概念，通常会引起混淆，尤其是对新手而言。</p>\n<span id=\"more\"></span>\n\n<h2 id=\"理解Python中的迭代\"><a href=\"#理解Python中的迭代\" class=\"headerlink\" title=\"理解Python中的迭代\"></a>理解Python中的迭代</h2><p>在编写代码时，经常需要多次重复部分代码块。要达到这个目的，可以通过下面任意方法实现：</p>\n<ul>\n<li><p>按顺序重复代码块的多次</p>\n</li>\n<li><p>将代码块放入循环中，根据需要运行多少次即可</p>\n</li>\n</ul>\n<p>第一种方法的缺点是编写重复的代码，这是<strong>难以维护</strong>和不可扩展的。例如，下面的代码将打印三次问候语:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Hello!&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Hello!&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Hello!&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<p>虽然这个代码可以正常运行，但是如果您决定更新代码以打印“Hello, World!”而不仅仅是“Hello!”。在这种情况下，将不得不更新问候消息三次，这将增加维护的负担。如果使用更大更复杂的代码，它可能成为维护人员的噩梦。</p>\n<p>使用循环将是解决问题和避免可维护性问题的更好方法。循环允许您根据需要经常运行一段代码。考虑一下如何使用<code>while</code>循环编写上面的示例：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>times = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">while</span> times &lt; <span class=\"number\">3</span>:</span><br><span class=\"line\"><span class=\"meta\">... </span>    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Hello!&quot;</span>)</span><br><span class=\"line\"><span class=\"meta\">... </span>    times += <span class=\"number\">1</span></span><br><span class=\"line\">...</span><br><span class=\"line\">Hello!</span><br><span class=\"line\">Hello!</span><br><span class=\"line\">Hello!</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Python中的迭代协议\"><a href=\"#Python中的迭代协议\" class=\"headerlink\" title=\"Python中的迭代协议\"></a>Python中的迭代协议</h2><p>迭代协议被Python中的所有迭代工具所使用，并由各种对象类型(如for循环（for-loops）、推导式（comprehensions）、映射(maps)等实现。本质上，该协议由两种对象类型组成，即<code>iterable</code>和<code>iterator</code>。</p>\n<ul>\n<li>可迭代（<code>iterable</code>）对象是你在其元素上迭代的对象</li>\n<li>迭代器(<code>iterator</code>)对象是在迭代过程中产生值的对象</li>\n</ul>\n<h2 id=\"Iterable是什么？\"><a href=\"#Iterable是什么？\" class=\"headerlink\" title=\"Iterable是什么？\"></a>Iterable是什么？</h2><p>在<code>Python</code>中，<code>Iterable</code>是一个实现<code>__iter__()</code>方法的对象，并返回一个迭代器对象或一个实现<code>__getitem__()</code>方法的对象(并且在索引耗尽时应该引发IndexError)。内置可迭代对象包括<code>Lists</code>, <code>Sets</code>和<code>string</code>，因为这样的序列可以在<code>for</code>循环中迭代。</p>\n<p>请注意，在最近的<code>Python</code>版本中，实现<code>Iterables</code>的首选方式是通过实现<code>__iter__()</code>方法。<code>__getitem__()</code>方法是在现代迭代器之前使用的一种遗留功能。但是Python仍然认为实现<code>__getitem__()</code>方法的对象是<code>Iterables</code>。这意味着如果没有定义<code>__iter__()</code>， Python解释器将使用<code>__getitem__()</code>。要了解更多细节，您可以参考<a href=\"https://peps.python.org/pep-0234/\">PEP-234</a>。</p>\n<p>总而言之，Python中的<code>Iterable</code>是以下任何对象：</p>\n<ul>\n<li>可以遍历(例如，可以遍历字符串的字符或文件的行)</li>\n<li>实现<code>__iter__()</code>方法(或<code>__getitem__</code>)，因此可以使用返回<code>Iterator</code>的<code>iter()</code>来调用它</li>\n<li>可以出现在<code>for</code>循环的右侧(<code>for i in myIterable:</code>)</li>\n</ul>\n<h2 id=\"Iterator是什么？\"><a href=\"#Iterator是什么？\" class=\"headerlink\" title=\"Iterator是什么？\"></a>Iterator是什么？</h2><p>另一方面，<code>Python</code>中的迭代器是一个以下面方式实现<code>__next__()</code>方法的对象：</p>\n<ul>\n<li>返回可迭代对象的下一个值，并且更新迭代器的状态，使其指向下一个值</li>\n<li>当可迭代对象的元素耗尽时引发<code>StopIteration</code>异常</li>\n</ul>\n<p>此外，<code>Iterator</code>本身也是一个<code>Iterable</code>，因为它还必须实现<code>__iter__()</code>方法，在该方法中它只返回<code>self</code>。</p>\n<blockquote>\n<p><em>Every Iterator is also an Iterable, but not every Iterable is an Iterator</em></p>\n</blockquote>\n<h2 id=\"Python迭代器和迭代器的操作\"><a href=\"#Python迭代器和迭代器的操作\" class=\"headerlink\" title=\"Python迭代器和迭代器的操作\"></a>Python迭代器和迭代器的操作</h2><p>正如我们已经提到的，<code>Iterables</code>的内置对象类型之一是<code>Python List</code>。现在让我们假设我们有以下整数列表，如下所示:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>my_lst = [<span class=\"number\">5</span>, <span class=\"number\">10</span>, <span class=\"number\">15</span>]</span><br></pre></td></tr></table></figure>\n\n<p>由于<code>my_lst</code>是一个可迭代对象，我们可以运行<code>iter()</code>方法来从可迭代对象中获取一个迭代器对象:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>my_iter = <span class=\"built_in\">iter</span>(my_lst)</span><br></pre></td></tr></table></figure>\n\n<p>我们可以验证<code>my_iter</code>是<code>list_iterator</code>类型</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">type</span>(my_iter)</span><br><span class=\"line\">list_iterator</span><br></pre></td></tr></table></figure>\n\n<p>现在，由于<code>my_iter</code>是一个迭代器，因此它实现了<code>__next__()</code>方法，该方法将返回列表<code>iterable</code>的下一个元素:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; my_iter.__next__()</span><br><span class=\"line\">5</span><br></pre></td></tr></table></figure>\n\n<p>一旦迭代器返回<code>__next__()</code>调用后的next值，它应该改变它的状态，使它现在指向下一个元素:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; my_iter.__next__()</span><br><span class=\"line\">10</span><br></pre></td></tr></table></figure>\n\n<p>请注意，<code>next(iter_name)</code>也是一个有效的语法，相当于<code>iter_name.__next__()</code>：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; next(my_iter)</span><br><span class=\"line\">15</span><br></pre></td></tr></table></figure>\n\n<p>现在我们到达了最后一个元素，下一次调用<code>__next__()</code>方法应该引发<code>StopIteration</code>，这是实现<code>__next__()</code>方法的迭代器必须满足的要求。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>my_iter.__next__()</span><br><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File <span class=\"string\">&quot;&lt;stdin&gt;&quot;</span>, line <span class=\"number\">1</span>, <span class=\"keyword\">in</span> &lt;module&gt;</span><br><span class=\"line\">StopIteration</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>在本文中，我们讨论了<code>Python</code>中的迭代协议，以及迭代对象和迭代器是如何参与迭代的。此外，我们还讨论了可迭代对象和迭代器的主要特性，并介绍了它们的主要区别。最后，我们展示了<code>Iterable</code>和<code>Iterator</code>对象是如何工作的。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p><a href=\"https://towardsdatascience.com/python-iterables-vs-iterators-688907fd755f\">https://towardsdatascience.com/python-iterables-vs-iterators-688907fd755f</a></p>\n<p><a href=\"https://realpython.com/python-iterators-iterables/\">https://realpython.com/python-iterators-iterables/</a></p>\n","tags":["Python"]},{"title":"Pytorch显存分析","url":"/2023/02/09/Pytorch%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90/","content":"<h1 id=\"Pytorch显存分析\"><a href=\"#Pytorch显存分析\" class=\"headerlink\" title=\"Pytorch显存分析\"></a>Pytorch显存分析</h1><p>在深度学习中，常见的错误是CUDA out of memory，这是因为显存放不下那么多模型参数和中间变量。通过及时清空中间变量，优化代码，减少batch等能够减少显存溢出。</p>\n<span id=\"more\"></span>\n\n<p><a href=\"https://pytorch.org/\">PyTorch</a>在进行深度学习训练的时候，有4大部分的显存开销，分别是</p>\n<ul>\n<li>模型参数(parameters)</li>\n<li>模型参数的梯度(gradients)</li>\n<li>优化器状态(optimizer states)</li>\n<li>中间激活值(intermediate activations) 或者叫中间结果(intermediate results)。</li>\n</ul>\n<p>其中activation占绝对大头，50%以上(有些地方说训练过程中占用显存最大的是计算图，把activation看作计算图的节点的话，也没错)优化器占的大小只是weight的2倍(Adam),SGD啥的话甚至1倍。</p>\n<h2 id=\"深度学习训练过程\"><a href=\"#深度学习训练过程\" class=\"headerlink\" title=\"深度学习训练过程\"></a>深度学习训练过程</h2><p>模型定义：定义了模型的网络结构，产生模型参数；</p>\n<p>while(你想训练):</p>\n<ol>\n<li>前向传播：执行模型的前向传播，产生中间激活值；</li>\n<li>后向传播：执行模型的后向传播，产生梯度；</li>\n<li>梯度更新：执行模型参数的更新，第一次执行的时候产生优化器状态。</li>\n</ol>\n<p>在模型定义完之后，1~3循环执行。</p>\n<h2 id=\"Torch机制\"><a href=\"#Torch机制\" class=\"headerlink\" title=\"Torch机制\"></a>Torch机制</h2><h3 id=\"CUDA-context-开销\"><a href=\"#CUDA-context-开销\" class=\"headerlink\" title=\"CUDA context 开销\"></a>CUDA context 开销</h3><p>在第一次执行CUDA操作时，使用GPU所需要创建维护设备间工作的一些相关信息。</p>\n<blockquote>\n<p>PyTorch will <strong>create the CUDA context in the very first CUDA operation</strong>, which can use ~600-1000MB of GPU memory depending on the CUDA version as well as the used device.<br>PyTorch itself will allocate the needed memory and will use an internal cache mechanism. You can read more about it <a href=\"https://pytorch.org/docs/stable/notes/cuda.html\">here</a>.</p>\n</blockquote>\n<blockquote>\n<ol>\n<li>You could rebuild PyTorch and remove libraries shipping with device code, such as cuDNN. While this would yield a performance hit, no cuDNN kernels will be loaded and stored in the CUDA context.</li>\n<li>Yes, you can use <code>PYTORCH_NO_CUDA_MEMORY_CACHING=1</code> to disable the cache.</li>\n<li>PyTorch needs to load its own kernels as well as device code from other libs (cuDNN, cublas, NCCL, etc.), which might not be the case for PyCUDA. It might be possible to share some driver-related code in the context, but I don’t know how much memory savings would be expected and haven’t experimented with it.</li>\n</ol>\n</blockquote>\n<p>显存的值跟CUDA的版本，Pytorch的版本以及所使用的设备都是有关系的。所以只有把任何小的张量放到GPU显存，那么至少会占用1000M左右显存，这部分显存是cuda运行时必须占掉的显存。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br><span class=\"line\">a = torch.tensor([<span class=\"number\">1.0</span>]).cuda()</span><br><span class=\"line\">time.sleep(<span class=\"number\">10</span>)</span><br></pre></td></tr></table></figure>\n\n<p>torch.tensor([1.0]).cuda()分配一个shape为1的tensor，<strong>理论上应该只有4bytes，但用allocated看会有512bytes</strong>。</p>\n<p><strong>在Docker容器中</strong>运行，使用不同的显卡测试，如下：</p>\n<blockquote>\n<p><strong>在1080显卡，pytorch 1.10.0+cu113，cuda11.4中测试，nvidia-smi初始显存为5266MiB，运行代码后为6171MiB。使用6171M-5266M&#x3D;905MiB。</strong></p>\n<p><strong>在RTX2080  Ti上，pytorch 1.10.0+cu113，cuda11.4中测试，初始为3M,，运行后为1476M，使用1476M-3M&#x3D;1473M。</strong></p>\n<p><strong>在3090上，pytorch 1.10.0+cu113，cuda11.4中测试，初始为8101M,，运行后为10290M，使用10290M-8101M&#x3D;2189M。</strong></p>\n</blockquote>\n<p><strong>在win10上运行</strong>，</p>\n<blockquote>\n<p><strong>在3090上，pytorch 1.10.1+cu111，cuda11.6中测试，初始为375M,，运行后为617M，使用617M-375M&#x3D;242M。</strong></p>\n</blockquote>\n<h3 id=\"PyTorch显存分配机制\"><a href=\"#PyTorch显存分配机制\" class=\"headerlink\" title=\"PyTorch显存分配机制\"></a>PyTorch显存分配机制</h3><p>在PyTorch中，显存是按页为单位进行分配的，这可能是CUDA设备的限制。以上面的代码为例，就算我们只想申请4字节的显存，Pytorch也会先向CUDA设备申请2MB的显存到自己的cache区中，然后pytorch再为我们分配512字节或者1024字节的空间。这个在使用torch.cuda.memory_allocated()的时候可以看出来512字节；用torch.cuda.memory_cached()（torch.cuda.memory_cached has been renamed to toch.cuda.memory_reserved)可以看出向CUDA申请的2MB。</p>\n<p>也就是说，PyTorch Allocated memory使用的是PyTorch reserved Memory里的显存，PyTorch reserved Memory则用的是GPU的显存。</p>\n<p>实际上，用nvidia-smi或者gpustat来看Pytorch程序的显存占用不是很合适的。 因为Pytorch的机制是使用缓存分配器来管理缓存分配的(因为这样速度快), 但是在缓存分配器的机制下, <strong>一个Tensor就算被释放了，进程也不会把空闲出来的显存还给GPU，而是等待下一个Tensor来填入这一片被释放的空间(即只要一个Tensor对象在后续不会再被使用，那么PyTorch就会自动回收该Tensor所占用的显存，并以缓冲区的形式继续占用显存，所以在nvidia-smi&#x2F;gpustat中看到的显存并没有减少)</strong></p>\n<p>   <em>即用nvidia-smi&#x2F;gpustat看到的其实是pytorch缓存区&#x2F;缓存分配器的情况</em></p>\n<p> 要是实在看缓冲区不爽的话，也可以用torch.cuda.empty_cache()把它归零，或者加一个环境变量PYTORCH_NO_CUDA_MEMORY_CACHING&#x3D;1，但是程序速度会变慢哦(试过在一个实验里慢了三倍)</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"> </span><br><span class=\"line\">device = torch.device(<span class=\"string\">&#x27;cuda:0&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 定义两个tensor</span></span><br><span class=\"line\">tensor1 = torch.randn(<span class=\"number\">120</span>, <span class=\"number\">3</span>, <span class=\"number\">512</span>, <span class=\"number\">512</span>).<span class=\"built_in\">float</span>().to(device)  </span><br><span class=\"line\"><span class=\"comment\"># 120*3*512*512*4/1000/1000 = 377.48M</span></span><br><span class=\"line\">tensor2 = torch.randn(<span class=\"number\">80</span>, <span class=\"number\">3</span>, <span class=\"number\">512</span>, <span class=\"number\">512</span>).<span class=\"built_in\">float</span>().to(device)  </span><br><span class=\"line\"><span class=\"comment\"># 80*3*512*512*4/1000/1000 = 251.64M</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated()/ <span class=\"number\">1000</span> / <span class=\"number\">1000</span>) <span class=\"comment\"># 629.15MiB</span></span><br><span class=\"line\"><span class=\"comment\"># 初始5308MiB,之后6811MiB，使用6811-5308=1423MiB</span></span><br><span class=\"line\"><span class=\"comment\"># 然后释放，使用del也可以进行释放</span></span><br><span class=\"line\">dummy_tensor_4 = dummy_tensor_4.cpu()</span><br><span class=\"line\">dummy_tensor_5 = dummy_tensor_5.cpu()</span><br><span class=\"line\"><span class=\"comment\"># 这里虽然将上面的显存释放了，但是我们通过Nvidia-smi命令看到显存依然在占用</span></span><br><span class=\"line\">torch.cuda.empty_cache()</span><br><span class=\"line\"><span class=\"comment\"># 只有执行完上面这句，显存才会在Nvidia-smi中释放</span></span><br></pre></td></tr></table></figure>\n\n<p>进程不需要重新向GPU申请显存了，运行速度会快很多，有什么坏处？他不能准确地给出某一个时间点具体的Tensor占用的显存，nvidia-smi<strong>显示的</strong>而是<strong>已经分配到的显存和context开销之和,</strong> <strong>也就是reserved_memory和torch context显存之和</strong>。</p>\n<p>   这也是令很多人在使用PyTorch时对显存占用感到困惑的罪魁祸首！！！</p>\n<h3 id=\"Pytorch释放机制\"><a href=\"#Pytorch释放机制\" class=\"headerlink\" title=\"Pytorch释放机制\"></a>Pytorch释放机制</h3><p>简单总结一下，就是在PyTorch中，只要一个Tensor对象在后续不会再被使用，那么PyTorch就会自动回收该Tensor所占用的显存，并以缓冲区的形式继续占用显存。</p>\n<p>要是实在看缓冲区不爽的话，也可以用torch.cuda.empty_cache()把它减少，或者加一个环境变量PYTORCH_NO_CUDA_MEMORY_CACHING&#x3D;1，但是程序速度会变慢哦，(曾经试过，在我的一个实验里慢了3倍)。</p>\n<p>然后就是选取合适的batch_size就可以开始训练啦，怎么估算多少batch_size可以刚好把咱们的显存占满呢，这里可以通过公式估算一下，（total_gpu_mem - model_used_mem）&#x2F;&#x2F; activation_mem</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">// 参数占用显存大小</span><br><span class=\"line\">params_mem = num_params * (2 if fp16_enabled else 4)</span><br><span class=\"line\">// 梯度占用显存大小</span><br><span class=\"line\">gradients_mem = num_params * (2 if fp16_enabled else 4)</span><br><span class=\"line\">// 优化器占用显存大小</span><br><span class=\"line\">optimizer_mem = num_params * (16 if fp16_enabled else 8)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"训练过程示例\"><a href=\"#训练过程示例\" class=\"headerlink\" title=\"训练过程示例\"></a>训练过程示例</h2><h3 id=\"模型定义\"><a href=\"#模型定义\" class=\"headerlink\" title=\"模型定义\"></a>模型定义</h3><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\">model = torch.nn.Linear(<span class=\"number\">1024</span>,<span class=\"number\">1024</span>, bias=<span class=\"literal\">False</span>).cuda() </span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated()) </span><br></pre></td></tr></table></figure>\n\n<p>输出4194304，刚好等于1024×1024×4</p>\n<h3 id=\"前向计算过程\"><a href=\"#前向计算过程\" class=\"headerlink\" title=\"前向计算过程\"></a>前向计算过程</h3><p>结论：显存增加等于每一层模型产生的结果的显存之和，且跟batch_size成正比。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">inputs = torch.tensor([<span class=\"number\">1.0</span>]*<span class=\"number\">1024</span>).cuda() </span><br><span class=\"line\"><span class=\"comment\"># shape = (1024)  数据占用显存1024*4=4096</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated()) </span><br><span class=\"line\"><span class=\"comment\"># 4194304(模型占用显存) + 4096（中间结果）=4198400</span></span><br><span class=\"line\">outputs = model(inputs) <span class=\"comment\"># outputs shape(1024) # 中间结果（1024*4=4096）</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated()) <span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\">#总共分配的4194304(模型参数占用) + 4096（输入） +4096（中间结果）　=4202496</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"反向传播过程\"><a href=\"#反向传播过程\" class=\"headerlink\" title=\"反向传播过程\"></a>反向传播过程</h3><p>后向传播会将模型的中间激活值给消耗并释放掉掉，并为每一个模型中的参数计算其对应的梯度。在第一次执行的时候，会为模型参数分配对应的用来存储梯度的空间。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">loss = <span class=\"built_in\">sum</span>(outputs) <span class=\"comment\">#  + 512 (torch allocate分配最小单位)</span></span><br><span class=\"line\">temp = torch.cuda.memory_allocated() <span class=\"comment\"># 4202496+512=4203008</span></span><br><span class=\"line\">loss.backward() <span class=\"comment\"># + 4194304（模型参数的梯度，1024*1024*4）</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated()) <span class=\"comment\"># 4203008+4194304=8397312</span></span><br></pre></td></tr></table></figure>\n\n<p>第一次执行时显存增加: 4194304字节 - 激活值大小</p>\n<p>第二次以后执行显存减少: 激活值大小</p>\n<p>Note: 由于这个中间值被赋给了outputs，所以后面在后向传播的时候会发现，这个outputs的显存没有被释放掉。但是当层数变深的时候，就能明显看到变化了。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"comment\"># 模型初始化</span></span><br><span class=\"line\">linear1 = torch.nn.Linear(<span class=\"number\">1024</span>,<span class=\"number\">1024</span>, bias=<span class=\"literal\">False</span>).cuda() <span class=\"comment\"># + 4194304</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated())</span><br><span class=\"line\">linear2 = torch.nn.Linear(<span class=\"number\">1024</span>, <span class=\"number\">1</span>, bias=<span class=\"literal\">False</span>).cuda() <span class=\"comment\"># + 4096</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输入定义</span></span><br><span class=\"line\">inputs = torch.tensor([[<span class=\"number\">1.0</span>]*<span class=\"number\">1024</span>]*<span class=\"number\">1024</span>).cuda() <span class=\"comment\"># shape = (1024,1024) # + 4194304</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 前向传播，正常来说，这里增加的中间结果有linear1的结果4194304，</span></span><br><span class=\"line\"><span class=\"comment\"># linear2的结果4096以及sum的结果512</span></span><br><span class=\"line\"><span class=\"comment\"># 实际上增加的是4194304 + 512！！！难道是sum和linear2操作合并了？？？？？？？</span></span><br><span class=\"line\">loss = <span class=\"built_in\">sum</span>(linear2(linear1(inputs))) </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 后向传播，中间值的显存释放，增加的显存是每个参数的梯度，linear1：4194304，linear2:4096</span></span><br><span class=\"line\">loss.backward() <span class=\"comment\"># memory - （4194304）+ 4194304 + 4096  </span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated()) <span class=\"comment\"># 由于释放一部分，再增加一部分，实际上增加4096</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 再来一次~</span></span><br><span class=\"line\">loss = <span class=\"built_in\">sum</span>(linear2(linear1(inputs))) <span class=\"comment\"># shape = (1) # memory + 4194304  (512没了，因为loss的ref还在)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated())</span><br><span class=\"line\">loss.backward() <span class=\"comment\"># memory - 4194304</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated())</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>讨论：从原理上讲，element-wise fusion 是可以省的呀，z&#x3D;linear1(x), w&#x3D;linear2(z), loss&#x3D;sum(w)，那么 dloss&#x2F;dw&#x3D;ones_like(w)，dloss&#x2F;dz&#x3D;linear2.weight.T @ dloss&#x2F;dw，整个过程中是不需要保存w的。但 pytorch 是否做了 fusion，就需要做 profile 来分析了。</p>\n</blockquote>\n<h2 id=\"参数更新\"><a href=\"#参数更新\" class=\"headerlink\" title=\"参数更新\"></a>参数更新</h2><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">optimizer.step() <span class=\"comment\"># 第一次增加8388608，第二次就不增不减了哦</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.max_memory_allocated()) <span class=\"comment\"># = torch.memory_allocated + 8388608</span></span><br></pre></td></tr></table></figure>\n\n<p>第一次执行时，会为每一个参数初始化<strong>其优化器状态</strong>，对于这里的AdamW而言，每一个参数需要4*2&#x3D;8个字节。第二次开始，不会再额外分配显存。</p>\n<p>显存开销:</p>\n<p>第一次: 增加8388608字节</p>\n<p>第二次及以后: 无增减</p>\n<p>由于计算机计算的特性，有一些计算操作在计算过程中是会带来额外的显存开销的。但是这种开销在torch.memory_allocated中是不能被察觉的。比如在AdamW在进行某一层的更新的时候，会带来2倍该层参数量大小的临时额外开销。这个在max_memory_allocated中可以看到。在本例中就是8388608字节。</p>\n<h2 id=\"节省显存\"><a href=\"#节省显存\" class=\"headerlink\" title=\"节省显存\"></a>节省显存</h2><p>即使只创建了个只有一个元素的tensor,它照样会显示用了几百兆的显存。</p>\n<p><strong>除非在编译的时候去掉一些cuda的功能模块。自己编译pytorch，能降低不少cuda context的显存开销。</strong></p>\n<p>在分析PyTorch的显存时候，一定要使用torch.cuda里的显存分析函数，我用的最多的是torch.cuda.memory_allocated()和torch.cuda.max_memory_allocated()，前者可以精准地反馈当前进程中Torch.Tensor所占用的GPU显存，后者则可以告诉我们到调用函数为止所达到的最大的显存占用字节数。还有像torch.cuda.memory_reserved()这样的函数则是查看当前进程所分配的显存缓冲区是多少的。</p>\n<p>nvidia-smi 里看到的占用</p>\n<p>&#x3D; CUDA 上下文 + pytorch 缓存区</p>\n<p>&#x3D; CUDA 上下文 + 未使用缓存 + 已使用缓存</p>\n<ul>\n<li>Pytorch 内部有自己的缓存管理系统，能够加速显存分配。</li>\n<li>使用 torch.cuda.memory_allocated() 可以看到当前模型实际占用的显存。</li>\n<li>使用 torch.cuda.memory_reserved() 可以看到Pytorch总共占用的显存</li>\n<li>使用 torch.cuda.empty_cache() 清空未使用的缓存，但是已经使用的是不能释放的。</li>\n</ul>\n<p>只有一种情况需要使用 torch.cuda.empty_cache()，就是当你想要释放缓存以便让其他人也可以一起使用当前显卡，否则不需要调用这个方法。</p>\n<p><code>reserved</code> memory contains the <code>allocated</code> and cached memory.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">import torch</span><br><span class=\"line\">a = torch.zeros((1024, 1024)).cuda() # 实际大小4M=1024*1024*4</span><br><span class=\"line\">torch.cuda.memory_allocated() / 1024 / 1024  # 4.0 张量占用</span><br><span class=\"line\">torch.cuda.memory_reserved() / 1024 / 1024  # 20.0  缓存区</span><br><span class=\"line\"># nvidia-smi 上看占用1073M</span><br><span class=\"line\"></span><br><span class=\"line\"># 删除临时变量</span><br><span class=\"line\">del a</span><br><span class=\"line\">torch.cuda.memory_allocated() / 1024 / 1024  # 0.0 张量占用</span><br><span class=\"line\">torch.cuda.memory_reserved() / 1024 / 1024  # 20.0  缓存区</span><br><span class=\"line\"># 可以看到缓存区仍然占用了20M，nvidia-smi里也保持1073M的占用</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># 清空缓存区</span><br><span class=\"line\">torch.cuda.empty_cache()</span><br><span class=\"line\">torch.cuda.memory_allocated() / 1024 / 1024  # 0.0 张量占用</span><br><span class=\"line\">torch.cuda.memory_reserved() / 1024 / 1024  # 0.0  缓存区</span><br><span class=\"line\"># nvidia-smi里的占用回到了1053M</span><br></pre></td></tr></table></figure>\n\n\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">loss = loss_fn(out, batch_gt_tensor)</span><br><span class=\"line\">loss.backward()</span><br><span class=\"line\"># added lines</span><br><span class=\"line\">del batch_input_data</span><br><span class=\"line\">del batch_gt_data</span><br><span class=\"line\">torch.cuda.empty_cache()</span><br><span class=\"line\">optimizer.step()</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"不同框架的上下文\"><a href=\"#不同框架的上下文\" class=\"headerlink\" title=\"不同框架的上下文\"></a>不同框架的上下文</h2><p> creating an empty tensor in pytorch costs 800MB of GPU memory,</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">import torch; torch.zeros((), device=&#x27;cuda&#x27;)</span><br></pre></td></tr></table></figure>\n\n<p>or 170MB to run something similar in tensorflow:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">import tensorflow as tf; tf.constant([])</span><br></pre></td></tr></table></figure>\n\n<p>or 175MB in cupy</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">import cupy; cupy.array([])</span><br></pre></td></tr></table></figure>\n\n<p>or 170MB in pycuda</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">import pycuda.driver as cuda; import pycuda.autoinit; cuda.mem_alloc(1)</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>the context occupying memory is by design of the CUDA &#x2F; NVIDIA software stack. It’s non-actionable because there is nothing we can technically do about it.</p>\n<p>You could reduce the CUDA context size by removing kernels e.g. via dropping libraries such as MAGMA or cuDNN, if these are not used.<br>Besides that I’m not aware of reducing the operator set in the framework itself.</p>\n<p>it’s because of the large number of kernels. There isn’t an easy way to avoid loading them: that would require separating some kernels into separate shared libraries. Building multiple libraries would be a big engineering challenge because of inter-dependencies between the libraries (i.e the main library depends on kernels in secondary libraries; those libraries depend on code in the main library).</p>\n<p>Another strategy is to compile the kernels on-the-fly when needed. I think some of the other packages use that approach. That can slow down the initial use a lot. We could pre-compile and ship the most commonly used kernels and only compile the other kernels on-demand. Again, a big engineering challenge.</p>\n</blockquote>\n<p>CUDA11.7版本，可以设置 <code>CUDA_MODULE_LOADING=LAZY</code></p>\n<blockquote>\n<p>The initialization will create the CUDA context loading all kernels for your GPU architecture and is thus expected. The size of the context depends on the CUDA version, your GPU, the number of kernels in loaded CUDA libs as well as native PyTorch kernels.<br>You could update to CUDA 11.7 and enable lazy module loading via <code>CUDA_MODULE_LOADING=LAZY</code> which will load kernels only if they are needed and will thus reduce the context size.</p>\n<p>Yes, the env variable takes effect in CUDA 11.7+ and won’t change anything in CUDA 10.2. The context size reduction between PyTorch 1.9.0 and 1.12.0 would come from loading some modules lazily via the framework directly as well as a reduction in the number of kernels.</p>\n<p>延迟加载:延迟内核从主机加载到GPU，直到内核被调用。这也只加载已使用的内核，这可能会显著节省设备端内存。这也将加载延迟从应用程序的开始推迟到第一次调用内核的时候——总的二进制加载延迟通常会显著减少，但也会转移到应用程序的后期。<br>要启用这个特性，在启动进程之前设置环境变量CUDA_MODULE_LOADING&#x3D;LAZY。<br>注意，这个特性只兼容CUDA版本&gt;&#x3D; 11.7编译的库。</p>\n</blockquote>\n<p><a href=\"https://zhuanlan.zhihu.com/p/424512257\">https://zhuanlan.zhihu.com/p/424512257</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/486360176\">https://zhuanlan.zhihu.com/p/486360176</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/486360176%EF%BC%88%E8%B5%9E%EF%BC%81%EF%BC%89\">https://zhuanlan.zhihu.com/p/486360176（赞！）</a></p>\n<p><a href=\"https://github.com/pytorch/pytorch/blob/master/c10/cuda/CUDACachingAllocator.cpp\">https://github.com/pytorch/pytorch/blob/master/c10/cuda/CUDACachingAllocator.cpp</a></p>\n<p><a href=\"https://discuss.pytorch.org/t/how-do-i-create-torch-tensor-without-any-wasted-storage-space-baggage/131134/10\">https://discuss.pytorch.org/t/how-do-i-create-torch-tensor-without-any-wasted-storage-space-baggage/131134/10</a></p>\n<p><a href=\"https://www.cnblogs.com/devilmaycry812839668/p/15571390.html\">https://www.cnblogs.com/devilmaycry812839668/p/15571390.html</a></p>\n<p><a href=\"https://github.com/pytorch/pytorch/issues/20532\">https://github.com/pytorch/pytorch/issues/20532</a></p>\n<p><a href=\"https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__CTX.html#group__CUDA__CTX_1g0651954dfb9788173e60a9af7201e65a\">https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__CTX.html#group__CUDA__CTX_1g0651954dfb9788173e60a9af7201e65a</a></p>\n<p><a href=\"https://github.com/pytorch/pytorch/issues/55277\">https://github.com/pytorch/pytorch/issues/55277</a></p>\n<p><a href=\"https://discuss.pytorch.org/t/reduce-gpu-memory-blocked-by-context/142587\">https://discuss.pytorch.org/t/reduce-gpu-memory-blocked-by-context/142587</a></p>\n"},{"title":"Vision-Language Pretraining &#58 Current Trends and the Future","url":"/2023/01/15/vision-language/","content":"<p>由<a href=\"https://www.iro.umontreal.ca/~agrawal/\">Aishwarya Agrawal</a> (DeepMind，蒙特利尔大学，Mila)， <a href=\"http://www.damienteney.info/\">Damien Teney</a> (Idiap研究所)和<a href=\"http://www.aidanematzadeh.me/\">Aida Nematzadeh</a> (DeepMind)编写的ACL 2022教程。</p>\n<span id=\"more\"></span>\n\n<p>幻灯片：</p>\n<p>• Part 1: <a href=\"https://drive.google.com/file/d/1e0oLeQ2QrGcyP5OdINKhVDAjrCxtIIPV/view?usp=sharing\">Vision-language landscape before the pretraining era</a>.<br>• Part 2: <a href=\"https://vlp-tutorial-acl2022.github.io/part2_pretraining_models_aida.pdf\">Modern vision-language pretraining</a>.<br>• Part 3: <a href=\"https://vlp-tutorial-acl2022.github.io/Part3_BeyondStatisticalLearning_Teney.pdf\">Beyond statistical learning</a>.</p>\n<p>本教程的目标是概述解决多模态问题所需的要素，特别是视觉和语言。我们还将讨论这一领域的一些悬而未决的问题和有前景的未来方向。</p>\n<p>在过去的几年里，人们对构建多模态(视觉-语言)模型越来越感兴趣，这种模型是在更大但更嘈杂的数据集上预训练的，其中两种模态(例如，图像和文本)彼此不精确地对应(例如，ViLBERT和CLIP)。给定一个任务(例如视觉问题回答)，这些模型通常会在特定于任务的监督数据集上进行微调。除了更大的预训练数据集外，transformer架构和应用于两种模态的自注意力是最近的预训练模型在下游任务中令人印象深刻的表现的原因。这种方法很有吸引力，原因有几个：首先，预训练数据集通常是从网络自动爬取的，提供了可以忽略不计的收集成本的庞大数据集。其次，我们可以训练大模型一次，并在各种任务中复用它们。最后，这些预训练方法的性能优于或与之前的特定任务模型相当。一个有趣的问题是，除了良好的任务表现外，这些预训练的模型是否能够更好地捕捉两种模态之间的对齐。在本教程中，我们关注最近的视觉语言预训练范式。我们的目标是在多模态预训练领域之前，首先提供图像语言数据集、基准和建模创新的背景。接下来，我们将讨论用于视觉-语言预训练的不同类型模型，重点介绍它们的优缺点。最后，我们讨论了通过统计学习进行视觉-语言预训练的局限性，以及因果建模等替代方法的必要性。</p>\n<h2 id=\"教程类型\"><a href=\"#教程类型\" class=\"headerlink\" title=\"教程类型\"></a>教程类型</h2><p>这是一个前沿教程，重点讨论视觉语言预训练的新趋势：是否最新的模型导致更好的表示，以及它们如何有助于下游任务。我们计划主要讨论2018年及之后的最新论文，但也将包括2018年之前在当前视觉语言范式中发挥关键作用的有影响力的论文。</p>\n<h2 id=\"受众\"><a href=\"#受众\" class=\"headerlink\" title=\"受众\"></a>受众</h2><p>我们希望目标受众是对视觉与语言交叉感兴趣的研究者，如语言基础或基础交流研究者。本教程也对初中生谁开始他们的职业生涯感兴趣。熟悉最近的架构(如变压器)是有用的，但不是参加教程所必需的。</p>\n<h1 id=\"Vision-Language-landscape-before-the-Pretraining-Era\"><a href=\"#Vision-Language-landscape-before-the-Pretraining-Era\" class=\"headerlink\" title=\"Vision-Language landscape before the Pretraining Era\"></a>Vision-Language landscape before the Pretraining Era</h1><h2 id=\"大纲\"><a href=\"#大纲\" class=\"headerlink\" title=\"大纲\"></a>大纲</h2><ul>\n<li>Common VL tasks</li>\n<li>Task specific datasets and models</li>\n<li>What did the community gain?</li>\n<li>Open problems and avenues for future research</li>\n</ul>\n<h2 id=\"Common-VL-tasks\"><a href=\"#Common-VL-tasks\" class=\"headerlink\" title=\"Common VL tasks\"></a>Common VL tasks</h2><p>（1）图像检索</p>\n<p>● High level similarity<br>● Easy evaluation (recall@K)</p>\n<p><img src=\"/2023/01/15/vision-language/image1.PNG\"></p>\n<p>(2) Grounding Referring Expressions</p>\n<p><img src=\"/2023/01/15/vision-language/image2.PNG\"></p>\n<p>● Spatial localization<br>● Finer grained grounding<br>● Easy evaluation (precision@1)</p>\n<p>(3) Image Captioning</p>\n<p>● Language generation (in addition to visual recognition)<br>● Difficult automatic evaluation (BLEU, CIDEr)</p>\n<p><img src=\"/2023/01/15/vision-language/image3.PNG\"></p>\n<p>(4) Visual Question Answering</p>\n<p>● Elicit specific information from images<br>● Relatively easier evaluation (accuracy using string matching)</p>\n<p><img src=\"/2023/01/15/vision-language/image4.PNG\"></p>\n<p>(4 ) Visual Dialog</p>\n<p><img src=\"/2023/01/15/vision-language/image5.PNG\"></p>\n<ul>\n<li>Context modelling in addition to grounding </li>\n<li>Difficult to evaluate free form answers (retrieval metric used)</li>\n</ul>\n<p>Why vision and language?</p>\n<ul>\n<li>Intuitive:<ul>\n<li>Humans learn in multimodal settings</li>\n</ul>\n</li>\n<li>Applications:<ul>\n<li>Aid to visually impaired users</li>\n<li>Online shopping and organizing photos</li>\n<li>Grounded virtual assistants</li>\n</ul>\n</li>\n<li>Scientific:<ul>\n<li>Visual recognition</li>\n<li>Language understanding</li>\n<li>Grounding language into vision</li>\n<li>Compositional reasoning</li>\n<li>Commonsense reasoning</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"任务特定的数据和模型\"><a href=\"#任务特定的数据和模型\" class=\"headerlink\" title=\"任务特定的数据和模型\"></a>任务特定的数据和模型</h2><p>（1）Basic skeleton of most VL models (Image Retrieval)</p>\n<img src=\"/2023/01/15/vision-language/work\\个人博客\\blog\\source\\_posts\\vision-language\\image6.PNG\" style=\"zoom: 67%;\">\n\n<p>(2)Basic skeleton of most VL models (Grounding Referring Expressions)</p>\n<img src=\"/2023/01/15/vision-language/work\\个人博客\\blog\\source\\_posts\\vision-language\\image7.PNG\" style=\"zoom:67%;\">\n\n<p>(3) Basic skeleton of most VL models (VQA)</p>\n<img src=\"/2023/01/15/vision-language/img8.PNG\" style=\"zoom:67%;\">\n\n<p>(4) Basic skeleton of most VL models (Image Captioning)</p>\n<p><img src=\"/2023/01/15/vision-language/img9.PNG\"></p>\n<p>(5) Basic skeleton of most VL models (Visual Dialog)</p>\n<img src=\"/2023/01/15/vision-language/img10.PNG\" style=\"zoom:67%;\">\n\n<h3 id=\"VL-数据集\"><a href=\"#VL-数据集\" class=\"headerlink\" title=\"VL 数据集\"></a>VL 数据集</h3><ul>\n<li><p>Image Retrieval: <a href=\"https://shannon.cs.illinois.edu/DenotationGraph/\">Flickr</a><em>,</em> <a href=\"https://cocodataset.org/#download\">COCO</a></p>\n</li>\n<li><p>Grounding Referring Expression: <a href=\"https://github.com/lichengunc/refer\">RefCOCO</a>，<a href=\"https://ai.stanford.edu/~yukez/visual7w/\">Visual7W</a></p>\n</li>\n<li><p>Image Captioning: <em><a href=\"https://cocodataset.org/#download\">COCO</a></em></p>\n</li>\n<li><p><em>Visual Question Answering:</em> <em><a href=\"https://visualqa.org/\">VQA v1</a>，</em> *<a href=\"https://visualqa.org/\">VQA v2</a>*， <em><a href=\"https://visualgenome.org/\">Visual Genome</a></em>, <em><a href=\"https://cs.stanford.edu/people/dorarad/gqa/index.html\">GQA</a></em></p>\n</li>\n<li><p>Visual Dialog:<em><a href=\"https://visualdialog.org/\">Visual Dialog</a>，</em> <em><a href=\"https://github.com/GuessWhatGame/guesswhat\">GuessWhat?!</a></em></p>\n</li>\n</ul>\n<h3 id=\"任务1：图像描述（Image-Captioning）\"><a href=\"#任务1：图像描述（Image-Captioning）\" class=\"headerlink\" title=\"任务1：图像描述（Image Captioning）\"></a>任务1：图像描述（Image Captioning）</h3><p>Captioning datasets：COCO [Lin et al., 2014]</p>\n<p><img src=\"/2023/01/15/vision-language/img11.PNG\"></p>\n<p>COCO的 全称是Common Objects in COntext，是微软团队提供的一个可以用来进行图像识别的数据集。MS COCO数据集中的图像分为训练、验证和测试集。</p>\n<ul>\n<li>120k训练集+验证集[vs 1k（Pascal）,31k(Flikr)]</li>\n<li>图像包括91类目标，2.5M的标注样例</li>\n<li>图像描述任务的标注benchmark</li>\n</ul>\n<p>评价指标</p>\n<ul>\n<li>自动评估<ul>\n<li>基于n-gram重叠的度量（BLEU, Rouge, METEOR, CIDEr [Chen et al., 2015]）</li>\n<li>基于场景图的度量（SPICE [Anderson et al., 2016]）</li>\n</ul>\n</li>\n<li>人工评估</li>\n</ul>\n<h3 id=\"Neural-Image-Caption-NIC-CVPR-2015\"><a href=\"#Neural-Image-Caption-NIC-CVPR-2015\" class=\"headerlink\" title=\"Neural Image Caption (NIC) (CVPR 2015)\"></a><strong>Neural Image Caption (NIC) (CVPR 2015)</strong></h3><p>论文：<a href=\"https://arxiv.org/abs/1411.4555\">Show and Tell: A Neural Image Caption Generator</a></p>\n<p>NIC，我们的模型，是基于端到端的神经网络，由视觉CNN和语言生成RNN组成。它从输入图像中生成完整的自然语言句子，如下图的例子所示</p>\n<img src=\"/2023/01/15/vision-language/img12.PNG\" style=\"zoom:67%;\">\n\n<p>模型结构：LSTM模型结合CNN图像嵌入器和词嵌入。LSTM单元之间展开的连接显示为蓝色，它们对应循环连接。所有LSTM单元共享相同的参数。</p>\n<img src=\"/2023/01/15/vision-language/img13.PNG\" style=\"zoom:67%;\">\n\n<p>这些是模型产生的标题，你可以看到，它基本上正确地获得了场景的要点，但当涉及到具体的细节时，它可能不是很准确。</p>\n<p><img src=\"/2023/01/15/vision-language/img14.PNG\"></p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p><a href=\"https://aclanthology.org/2022.acl-tutorials.7.pdf\">Vision-Language Pretraining: Current Trends and the Future</a></p>\n","tags":["NLP","CV"]},{"title":"满江红·暮雨初收","url":"/2023/04/08/%E6%BB%A1%E6%B1%9F%E7%BA%A2%C2%B7%E6%9A%AE%E9%9B%A8%E5%88%9D%E6%94%B6/","content":"<blockquote class=\"song\">\n<p>\n            <tt>满江红·暮雨初收</tt><br>\n            <at>〔宋代〕 柳永</at><br>暮雨初收，长川静、征帆夜落。<br>临岛屿、蓼烟疏淡，苇风萧索。<br>几许渔人飞短艇，尽载灯火归村落。<br>遣行客、当此念回程，伤漂泊。<br>桐江好，烟漠漠。<br>波似染，山如削。<br>绕严陵滩畔，鹭飞鱼跃。<br>游宦区区成底事，平生况有云泉约。<br>归去来、一曲仲宣吟，从军乐。<br>\n </p>\n</blockquote>\n"}]