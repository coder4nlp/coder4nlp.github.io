[{"title":"ERNIE-layout","url":"/2023/02/01/ERNIE-layout/","content":"<h1 id=\"ERNIE-Layout-Layout-Knowledge-Enhanced-Pre-training-for-Visually-rich-Document-Understanding\"><a href=\"#ERNIE-Layout-Layout-Knowledge-Enhanced-Pre-training-for-Visually-rich-Document-Understanding\" class=\"headerlink\" title=\"ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding\"></a>ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding</h1><h2 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h2><p>近年来，在富文档理解方面，已经见证了预训练技术的兴起和成功。然而，现有的大多数方法缺乏对以布局为中心的知识的系统挖掘和利用，导致性能不佳。论文提出了ERNIE-Layout，这是一种新颖的文档预训练解决方案，在整个工作流程中增强布局知识，以学习更好的表示方式，结合文本、布局和图像的特征。具体来说，我们首先在序列化阶段对输入序列进行重新排列，然后提出相关的预训练任务——阅读顺序预测，学习文档的正确阅读顺序。为了提高模型的布局意识，我们在多模态transformer中集成了空间感知解耦注意力，在预训练阶段集成了区域替换预测任务。实验结果表明ERNIE-Layout在各种下游任务上实现了卓越的性能，在关键信息提取、文档图像分类和文档问答数据集上达到新的技术水平。代码和模型可以在PaddleNLP上公开获取。</p>\n<span id=\"more\"></span>\n\n<p>VrDU：Visually-rich Document Understanding，NLU：Natural Language Understanding</p>\n<h2 id=\"问题及方案\"><a href=\"#问题及方案\" class=\"headerlink\" title=\"问题及方案\"></a>问题及方案</h2><h3 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h3><p>文档格式的多样性和复杂性对任务提出了新的挑战，理想的模型需要充分利用文本、布局甚至视觉信息，像人类一样充分理解视觉丰富的文档。</p>\n<p>VrDU的早期工作常采用单模态或浅多模态融合方法，这些方法是针对特定任务的，需要大量标注数据。最近预训练语言模型已经席卷了这个领域。LayoutLM、 LayoutLMv2、以及一些先进的文档预训练方法相继提出，并在各种VrDU任务中取得了巨大成功。与流行的单模态或视觉语言（Vision-Language）框架不同，<strong>文档理解模型的独特性在于如何利用布局知识。</strong>然而，现有的文档预训练解决方案通常会陷入将二维坐标作为一维位置扩展的陷阱，赋予模型布局感知能力。考虑到VrDU的特点，我们认为以布局为中心的知识应该从两个方面进行系统的挖掘和利用：</p>\n<p>（1）一方面，布局隐式地反映了文档的正确阅读顺序，而以往的方法都是将光学字符识别(OCR)的结果进行多路复用，大致按照从上到下、从左到右的方式排列token。对于具有复杂布局的文档(例如，表格、表单、多列模板)，这与人类的阅读习惯不一致，并导致下游任务的性能不佳。（2）另一方面，<strong>布局实际上是语言和语言之外的第三种形式</strong>，而目前的模型通常将布局作为一种特殊的位置特征，例如嵌入在输入层中的布局(LayoutLM)或注意层中的偏向项(LayoutLMv2)。布局与文本&#x2F;图像之间缺乏跨模态交互可能会限制模型学习布局在语义表达中的作用。</p>\n<h3 id=\"方案\"><a href=\"#方案\" class=\"headerlink\" title=\"方案\"></a>方案</h3><p>（1）首先，我们在序列化阶段采用现成的基于布局的文档解析器，为每个输入文档生成合适的阅读顺序，使模型接收到的输入序列比使用粗略的光栅扫描顺序更符合人类的阅读习惯。</p>\n<p>（2）然后，每个文本&#x2F;视觉token都配备了其位置嵌入和布局嵌入，并送到堆叠的多模态transformer层。受DeBERTa的解耦注意力启发， 我们提出了一种空间感知的解耦注意力机制，其中token之间的注意力权重是根据它们的隐藏状态和相对位置使用解耦矩阵计算的。最后，布局不仅作为输入token的二维位置属性，而且为语义相似度的计算提供了一个空间视角。</p>\n<h2 id=\"模型结构\"><a href=\"#模型结构\" class=\"headerlink\" title=\"模型结构\"></a>模型结构</h2><p>Ernie-layout 整体采用 Transformer Encoder 架构。</p>\n<p><img src=\"/2023/02/01/ERNIE-layout/image9.PNG\"></p>\n<h3 id=\"序列模块\"><a href=\"#序列模块\" class=\"headerlink\" title=\"序列模块\"></a>序列模块</h3><p>用Document-Parse作为先验知识告诉模型阅读的顺序（a layout-knowledge enhanced pre-training approach），将每个token加入阅读顺序的特征。PPL被广泛用于度量语言模型的性能。通过Document-Parser序列化的输入序列的PPL比光栅扫描顺序的PPL低。</p>\n<h3 id=\"输入表示\"><a href=\"#输入表示\" class=\"headerlink\" title=\"输入表示\"></a>输入表示</h3><p>ERNIE-Layout的输入序列包括文本部分和视觉部分，每个部分的表示是其模态特征和布局嵌入的组合。</p>\n<p><strong>文本嵌入</strong>（Text Embedding）。序列化模块之后的文档token用作文本序列。</p>\n<p>在BERT-Style模型的预处理之后，两个特殊标记[CLS]和[SEP]分别附加在文本序列的开头和结尾。最后，token序列T的文本嵌入表示为:<br>$$<br>T&#x3D;E_{tk}(T)+E_{1p}(T)+E_{tp}(T)<br>$$<br>这里$E_{tk}$、$E_{1p}$、$E_{tp}$分别是token embedding、1D position embedding以及token type  embedding。其中采用可学习的 position_embeddings 。position_ids通过 OCR 工具获得。采用 [Layout-Parser](<a href=\"https://github.com/Layout-Parser/\">https://github.com/Layout-Parser/</a> layout-parser) 对图片中的文本内容，根据阅读顺序进行排序，安排对应的 position_ids。</p>\n<p><strong>视觉嵌入（Visual Embedding）</strong>用 Faster-RCNN 当作encoder，图片先resize成224×224 ，然后池化得到7x7的feature,之后flaten成视觉序列，特征线性映射到text embedding 同样的维度。同样地，此外还键入position embedding和token type embedding。<br>$$<br>V&#x3D;F_{vs}(V)+E_{1p}(T)+E_{tp}(T)<br>$$</p>\n<p><strong>Layout Embedding</strong>。对于每个文本token，OCR工具提供包含边界框宽度和高度的2D坐标$(x_0,y_0,x_1,y_1,w,h)$，$(x_0,y_0)$表示左上角的坐标，$(x_1,y_1)$表示右下角的坐标。$w&#x3D;x_1-x_0$，$h&#x3D;y_1-y_0$，所有坐标规范化到$[0,1000]$，使用两个嵌入层表示横坐标和纵坐标。<br>$$<br>L&#x3D;E_{2x}(x_0,x_1,W)+E_{2y}(y_0,y_1,H)<br>$$<br>$E_{2x}$表示x轴嵌入层，$E_{2y}$表示y轴嵌入层。</p>\n<p>为了获得ERNIE-Layout最终的输入，我们将每个文本和视觉嵌入和它们对应的布局嵌入整合到一起。最终的序列长度是$N+HW$，文本和视觉与相关的Layout Embedding相加后concat。<br>$$<br>H&#x3D;[T+L;V+L]<br>$$</p>\n<h3 id=\"多模态Transformer\"><a href=\"#多模态Transformer\" class=\"headerlink\" title=\"多模态Transformer\"></a>多模态Transformer</h3><p>在最终的输入表示中，文本和视觉token被拼接在一起，Transformer的自注意机制支持它们的层感知跨模式交互。但是，作为一种独特的模态，在计算注意力权重时需要考虑布局特征，并明确考虑布局特征与内容(统称文字和图像)之间的紧密性。受DeBERTa 解耦注意力的启发，其中token之间的注意力权重是使用其内容上的解耦矩阵计算的。</p>\n<p>以1D位置为例，token $i$和token $j$的相对距离为$\\delta_{1p}$如下：<br>$$<br>y&#x3D; \\begin{cases}<br>0,\\quad &amp; i-j\\leq -k \\<br>2k-1,\\quad &amp;i-j\\ge k \\<br>i-j +k,\\quad &amp; others<br>\\end{cases}<br>$$<br>2D位置的相对距离同理。</p>\n<p>而后计算上下文-上下文，上下文-1D 位置信息, 上下文-2D 位置信息对应的 attention 权重：<br>$$<br>A_{ij}^{ct,ct}&#x3D;Q_i^{ct}K_l^{ct} \\<br>A_{ij}^{ct,1p}&#x3D;Q_i^{ct}K_{\\delta_{1p}(i,j)}^{1p} + {K_{j}^{ct}Q_{\\delta_{1p}(j,i)}^{1p}}^{\\top} \\<br>A_{ij}^{ct,2x}&#x3D;Q_i^{ct}K_{\\delta_{2x}(i,j)}^{2x} + {K_{j}^{ct}Q_{\\delta_{2x}(j,i)}^{2x}}^{\\top} \\<br>A_{ij}^{ct,2y}&#x3D;Q_i^{ct}K_{\\delta_{2y}(i,j)}^{2y} + {K_{j}^{ct}Q_{\\delta_{2y}(j,i)}^{2y}}^{\\top} \\<br>$$<br>最后，将所有这些注意得分进行汇总，得到注意矩阵$\\hat A $。通过缩放和归一化操作，空间感知解耦注意力的输出为：<br>$$<br>\\hat A_{ij}&#x3D;A_{ij}^{ct,ct}+A_{ij}^{ct,1p}+A_{ij}^{ct,2x}+A_{ij}^{ct,2y} \\<br>H_{out}&#x3D;softmax (\\frac{\\hat A}{\\sqrt 3d})<br>$$</p>\n<p><strong>issue 可以确认，paddlenlp 开源的 ernie-layoutx 为论文的降级版，其中的注意力模块、输入embedding模块等均与论文描述的不同。</strong></p>\n<blockquote>\n<p>ernie-layout代码中的实现:<a href=\"https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/transformers/ernie_layout/modeling.py#L315\">https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/transformers/ernie_layout/modeling.py#L315</a><br>这个attention和layoutlmv2一样的吧？</p>\n<p>考虑到商用，目前为降级开源，当前开出版本仍旧好于LayoutXLM，后续会适当的时候对外开源spatial-aware disentangled attention版本，如商业有需求请联系<a href=\"https://ai.baidu.com/tech/nlp/Textanalysis\">https://ai.baidu.com/tech/nlp/Textanalysis</a></p>\n</blockquote>\n<h2 id=\"预训练任务\"><a href=\"#预训练任务\" class=\"headerlink\" title=\"预训练任务\"></a>预训练任务</h2><p>ERNIE-Layout采用了四种预训练任务，包括新提出的阅读顺序预测（reading order prediction）、区域替换预测（replaced region prediction tasks）任务和传统的masked视觉语言建模、文本-图像对齐任务。</p>\n<ul>\n<li><p><strong>Reading Order Prediction：</strong> 希望注意矩阵能携带关于阅读顺序的知识，通过这种方式，我们赋予$\\hat A_{ij}$一个额外的含义，即第j个token是第i个token的下一个token的概率。此外，ground truth是一个0-1矩阵G，其中1表示两个token之间存在阅读顺序关系，反之亦然。对于最后一个token，下一个token是它自己。在预训练阶段，使用交叉熵损失函数。<br>$$<br>L_{ROP}&#x3D;-\\sum_{0\\le i &lt; N}\\sum_{0\\le j &lt; N}G_{ij}log(\\hat {A_{ij}})<br>$$</p>\n</li>\n<li><p><strong>Replaced Region Prediction：</strong> 在视觉编码器中，每个文档图像被处理成一个固定长度HW的序列。为了使模型能够感知图像块和文本之间的细粒度对应，在布局知识的帮助下，我们提出了区域替换预测(RRP)。具体来说，随机选取10%的图像块，用另一个图像中的块替换，处理后的图像由视觉编码器编码并输入到多模态 transformer中。然后，使用 transformer输出的[CLS]向量来预测哪些补丁被替换。所以这个任务的损失是：<br>$$<br>L_{RRP}&#x3D;-\\sum_{0 \\le i &lt;HW}[G_ilog(P_i)+(1-G_i)*log(1-P_i)]<br>$$<br>$G_i$是替换的图像块，$P_i$是规范化的概率。</p>\n</li>\n<li><p><strong>Masked Visual-Language Modeling</strong> ：类似 MLM（masked language modeling），MVLM（masked visual-language modeling）目标是根据masked文本上下文和整个多模态线索恢复masked文本token。</p>\n</li>\n<li><p><strong>Text-Image Alignment</strong> ：除了图像侧跨模态任务RRP，我们还采用了文本-图像对齐(TIA,Text-Image Alignment)作为文本侧任务，帮助模型学习图像区域与边界框坐标之间的空间对应关系。在这里，一些文本行是随机选择的，它们对应的区域覆盖在文档图像上。然后，引入分类层来预测每个文本标记是否被覆盖。</p>\n</li>\n</ul>\n<p>预训练任务的最终目标是<br>$$<br>L&#x3D;L_{ROP}+L_{RRP}+L_{MVLM}+L_{TIA}<br>$$</p>\n<h2 id=\"实验设置\"><a href=\"#实验设置\" class=\"headerlink\" title=\"实验设置\"></a>实验设置</h2><p>ERNIE-Layout有24个transformer层，1024个隐藏单元和16个注意头。文本token的最大序列长度为512，视觉token的最大序列长度为49。transformer初始化自RoBERTa large，视觉编码器采用Faster-RCNN 作为初始化模型。其余参数随机初始化。我们使用Adam作为优化器，学习率为1e-4，权值衰减为0.01。学习率在前10%的步骤中线性升温，然后线性衰减到0。ERNIE-Layout在24个Tesla A100 gpu上训练了20个epoch，batch大小为576。</p>\n<h2 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h2><p>为了实验的公平性，我们只使用序列化来重新排列预训练数据的读取顺序，这意味着ERNIE-Layout在微调阶段接收到的输入与比较方法中输入是一样的。</p>\n<h3 id=\"Key-Information-Extraction\"><a href=\"#Key-Information-Extraction\" class=\"headerlink\" title=\"Key Information Extraction\"></a>Key Information Extraction</h3><p>ERNIE-Layout在FUNSD, CORD, Kleister-NDA上实现STOA，并在SROIE上实现了具有竞争力的性能。值得一提的是，在FUNSD中，ERNIE-Layout较之前的最佳结果获得了7.98%的显著稳定改善(标准差为0.0011)。以上现象足以验证我们在文档预训练模型中挖掘和利用布局知识的设计理念的有效性。</p>\n<p><img src=\"/2023/02/01/ERNIE-layout/image1.PNG\"></p>\n<p><img src=\"/2023/02/01/ERNIE-layout/image7.PNG\"></p>\n<h3 id=\"与最新的LayoutLMv3对比一下\"><a href=\"#与最新的LayoutLMv3对比一下\" class=\"headerlink\" title=\"与最新的LayoutLMv3对比一下\"></a>与最新的LayoutLMv3对比一下</h3><p><img src=\"/2023/02/01/ERNIE-layout/image8.PNG\"></p>\n<center><strong>LayoutLMv3 Large与ERNIE-layout Large 对比</strong></center>\n\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>FUNSD</th>\n<th>CORD</th>\n<th>DocVQA</th>\n<th>RVL-CDIP</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>LayoutLMv3 large</strong></td>\n<td>92.08</td>\n<td><strong>97.46</strong></td>\n<td>83.37</td>\n<td>95.93</td>\n</tr>\n<tr>\n<td><strong>ERNIE-layout large</strong></td>\n<td><strong>93.12</strong></td>\n<td>97.21</td>\n<td><strong>88.41</strong></td>\n<td><strong>96.27</strong></td>\n</tr>\n</tbody></table>\n<h3 id=\"Document-Question-Answering\"><a href=\"#Document-Question-Answering\" class=\"headerlink\" title=\"Document Question Answering\"></a>Document Question Answering</h3><p>表4列出了平均归一化Levenshtein相似度(ANLS, Average Normalized Levenshtein Similarity) DocVQA评分。</p>\n<p>请注意，LayoutLMv2(#7)是基于UniLMv2(#3)开发的，该模型具有强大的问答能力，甚至在任务上击败了多模型LayoutLM(#4)。不幸的是，UniLMv2没有公开任何预训练代码或预训练模型，我们只能使用RoBERTa的参数来初始化我们的ERNIE-Layout。然而，我们感到惊讶的是ERNIE-Layout带来了令人兴奋的性能改进，(几乎是LayoutLMv2增加的两倍)。此外，我们使用模型集成在DocVQA排行榜上获得了第一名。</p>\n<p><img src=\"/2023/02/01/ERNIE-layout/image2.PNG\"></p>\n<h3 id=\"Document-Image-Classification\"><a href=\"#Document-Image-Classification\" class=\"headerlink\" title=\"Document Image Classification\"></a>Document Image Classification</h3><p>与这些重点关注多模态语义理解的关键信息提取或文档问答任务不同，文档图像分类需要对文本内容和文档布局的宏观感知。尽管我们的预训练任务关注的是细粒度的跨模态匹配，ERNIE-Layout仍然刷新跨粒度任务的最佳性能。</p>\n<img src=\"/2023/02/01/ERNIE-layout/image3.PNG\" style=\"zoom:67%;\">\n\n\n\n<h2 id=\"预训练分析\"><a href=\"#预训练分析\" class=\"headerlink\" title=\"预训练分析\"></a>预训练分析</h2><h3 id=\"预训练任务的分析\"><a href=\"#预训练任务的分析\" class=\"headerlink\" title=\"预训练任务的分析\"></a>预训练任务的分析</h3><p>在这个实验中，我们从基本的MVLM任务开始实现基线模型(#1)，并集成新的任务逐步直到最终模型包含所有四个训练任务(#5)。从表6中，我们观察到RRP带来了0.95%的改善FUNSD，展示了细粒度跨模态的相互作用。当加入ROP时，FUNSD的性能进一步提高1.3%。我们认为ROP有助于模型学习阅读顺序知识的更好表示。</p>\n<p><img src=\"/2023/02/01/ERNIE-layout/image4.PNG\"></p>\n<h3 id=\"注意力机制的有效性\"><a href=\"#注意力机制的有效性\" class=\"headerlink\" title=\"注意力机制的有效性\"></a>注意力机制的有效性</h3><p>LayoutLMv2 最初提出了空间感知的自注意，在注意力计算中考虑布局特征，后续许多方法都遵循这一思路。从表6中，我们发现采用这种机制可以提高下游任务的性能(#4 v.s. #6)。与此同时，将注意力分散到位置和内容部分是获得进一步性能提升的另一个有效解决方案(#5 v.s. #6)。</p>\n<h3 id=\"序列化模块的有效性\"><a href=\"#序列化模块的有效性\" class=\"headerlink\" title=\"序列化模块的有效性\"></a>序列化模块的有效性</h3><p>在这里，我们将探讨使用不同的序列化模块对下游VrDU任务的影响。如表7所示，使用基于布局知识的序列化模块(#2，#3)，模型可以获得更好的性能(即使没有解耦注意力)。我们将这种改进归因于这样一个事实:尽管序列化没有用于微调数据集，但在预训练后，模型有能力理解文档的正确阅读顺序。</p>\n<img src=\"/2023/02/01/ERNIE-layout/image5.PNG\" style=\"zoom:67%;\">\n\n\n\n<p>如图，一个复杂文档布局，使用raster-scanning order序列化为“… Session Chair: Session Chair: Session Chair: Tuula Hakkarainen …”，而使用Document-Parser序列化为：“… Session Chair: Tuula wz Session Chair: Frank Markert …”，这更符合人类的阅读习惯。</p>\n<img src=\"/2023/02/01/ERNIE-layout/image6.PNG\" style=\"zoom:67%;\">\n\n<p>使用Document-Parser序列化后，文档的PPL大幅下降。</p>\n","tags":["NLP"]},{"title":"Hugging Face的Datasets 库","url":"/2023/01/10/Hugging-Face%E7%9A%84Datasets-%E5%BA%93/","content":"<p><code>Datasets</code>是一个轻量级库，提供两个主要特性:</p>\n<span id=\"more\"></span>\n\n<ul>\n<li>用于许多公开数据集的单行<strong>dataloaders</strong>：只需一行程序即可下载和预处理<code>HuggingFace</code>数据集中心提供的任何数量的主要公开数据集(图像数据集、音频数据集、467种语言和方言的文本数据集等)。使用一个简单的命令，如<code>squad_dataset = load_dataset(&quot;squad&quot;)</code>，即可获取任何这些数据集，准备在数据加载器中使用，用于训练&#x2F;评估ML模型(<code>Numpy/Pandas/PyTorch/TensorFlow/JAX</code>)</li>\n<li>高效的数据预处理：简单，快速和可重复的数据预处理，用于公开数据集以及您自己的<code>CSV</code>, <code>JSON</code>，<code>text</code>，<code>PNG</code>，<code>JPEG</code>，<code>WAV</code>,，<code>MP3</code>, <code>Parquet</code>等本地数据集。使用<code>processed_dataset = dataset.map(process_example)</code>这样的简单命令，可以有效地为ML模型评估和训练准备数据集。</li>\n</ul>\n<p>Datasets旨在让社区轻松添加和共享新的数据集。</p>\n<p><code>Datasets</code>有许多其他有趣的特性:</p>\n<ul>\n<li><p>在大型数据集上发展：数据集自然地将用户从RAM内存限制中解放出来，所有数据集都使用高效的<code>zero-serialization cost backend</code>(Apache Arrow)进行内存映射。</p>\n</li>\n<li><p>智能缓存：永远不要等待您的数据处理多次。</p>\n</li>\n<li><p>轻量级且快速，使用透明的<code>python API</code>(多处理&#x2F;缓存&#x2F;内存映射)。</p>\n</li>\n<li><p>内置与<code>NumPy</code>, <code>Pandas</code>, <code>PyTorch</code>, <code>Tensorflow 2</code>和<code>JAX</code>的互操作性。</p>\n</li>\n<li><p>对音频和图像数据的原生支持。</p>\n</li>\n<li><p>启用流模式以节省磁盘空间并立即开始遍历数据集。</p>\n<p><code>Datasets</code>起源于很棒的<a href=\"https://github.com/tensorflow/datasets\">TensorFlow Datasets</a> 的一个分支，HuggingFace团队想要深切感谢<code>TensorFlow Datasets</code>团队构建这个惊人的库。关于<code>Datasets</code>和<code>tfds</code>之间差异的更多详细信息，可以在<a href=\"https://github.com/huggingface/datasets#main-differences-between--datasets-and-tfds\">Main differences between 🤗 Datasets and <code>tfds</code></a>部分中找到。</p>\n</li>\n</ul>\n<h1 id=\"从hub加载一个数据\"><a href=\"#从hub加载一个数据\" class=\"headerlink\" title=\"从hub加载一个数据\"></a>从hub加载一个数据</h1><p>找到可复现和可访问的高质量数据集可能很困难。<code>Datasets</code>的主要目标之一是提供一种简单的方法来加载任何格式或类型的数据集。最简单的入门方法是在<a href=\"https://huggingface.co/datasets\">Hugging Face Hub</a> 上发现现有数据集(一个社区驱动的数据集集合，用于自然语言处理、计算机视觉和音频任务)，并使用<code>Datasets</code>下载并生成数据集。</p>\n<p>本教程使用<a href=\"https://huggingface.co/datasets/rotten_tomatoes\">rotten_tomatoes</a>和<a href=\"https://huggingface.co/datasets/PolyAI/minds14\">MInDS-14</a>数据集，但请随意加载任何你想要的数据集并跟随。现在前往Hub，为您的任务找到一个数据集!</p>\n<h2 id=\"加载一个数据\"><a href=\"#加载一个数据\" class=\"headerlink\" title=\"加载一个数据\"></a>加载一个数据</h2><p>在花时间下载数据集之前，快速获取关于数据集的一些一般信息通常是有帮助的。数据集的信息存储在 <a href=\"https://huggingface.co/docs/datasets/v2.6.1/en/package_reference/main_classes#datasets.DatasetInfo\">DatasetInfo</a>中，可以包括数据集描述、特征和数据集大小等信息。</p>\n<p>使用<a href=\"https://huggingface.co/docs/datasets/v2.6.1/en/package_reference/loading_methods#datasets.load_dataset_builder\">load_dataset_builder()</a>函数加载数据集构建器，并检查数据集的属性，而不是下载数据集:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">from datasets import load_dataset_builder</span><br><span class=\"line\">ds_builder = load_dataset_builder(&quot;rotten_tomatoes&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">ds_builder.info.description</span><br><span class=\"line\"></span><br><span class=\"line\">ds_builder.info.features</span><br></pre></td></tr></table></figure>\n\n<p>如果你对数据集满意，那么用<code>load_dataset()</code>加载它:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">from datasets import load_dataset</span><br><span class=\"line\">dataset = load_dataset(&quot;rotten_tomatoes&quot;, split=&quot;train&quot;)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"划分\"><a href=\"#划分\" class=\"headerlink\" title=\"划分\"></a>划分</h2><p>一个划分是数据集的特定子集(如<code>train</code>和<code>test</code>)。使用<code>get_dataset_split_names()</code>函数列出数据集的拆分名称:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> datasets <span class=\"keyword\">import</span> get_dataset_split_names</span><br><span class=\"line\"></span><br><span class=\"line\">get_dataset_split_names(<span class=\"string\">&quot;rotten_tomatoes&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<p>然后你可以用<code>split</code>参数加载一个特定的划分。加载一个数据集划分返回一个<code>Dataset</code>对象:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> datasets <span class=\"keyword\">import</span> load_dataset</span><br><span class=\"line\"></span><br><span class=\"line\">dataset = load_dataset(<span class=\"string\">&quot;rotten_tomatoes&quot;</span>, split=<span class=\"string\">&quot;train&quot;</span>)</span><br><span class=\"line\">dataset</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">Dataset(&#123;</span><br><span class=\"line\">    features: [&#x27;text&#x27;, &#x27;label&#x27;],</span><br><span class=\"line\">    num_rows: 8530</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>如果你没有指定<code>split</code>，<code>Datasets</code>会返回一个<code>DatasetDict</code>对象:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">from datasets import load_dataset</span><br><span class=\"line\"></span><br><span class=\"line\">dataset = load_dataset(&quot;rotten_tomatoes&quot;)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">DatasetDict(&#123;</span><br><span class=\"line\">    train: Dataset(&#123;</span><br><span class=\"line\">        features: [&#x27;text&#x27;, &#x27;label&#x27;],</span><br><span class=\"line\">        num_rows: 8530</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">    validation: Dataset(&#123;</span><br><span class=\"line\">        features: [&#x27;text&#x27;, &#x27;label&#x27;],</span><br><span class=\"line\">        num_rows: 1066</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">    test: Dataset(&#123;</span><br><span class=\"line\">        features: [&#x27;text&#x27;, &#x27;label&#x27;],</span><br><span class=\"line\">        num_rows: 1066</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n\n\n\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p><a href=\"https://huggingface.co/docs/datasets/load_hub\">https://huggingface.co/docs/datasets/load_hub</a></p>\n<p><a href=\"https://huggingface.co/docs/datasets/index\">https://huggingface.co/docs/datasets/index</a></p>\n"},{"title":"Python中的ChainMap","url":"/2023/04/23/Python%E4%B8%AD%E7%9A%84ChainMap/","content":"<h1 id=\"ChainMap介绍\"><a href=\"#ChainMap介绍\" class=\"headerlink\" title=\"ChainMap介绍\"></a>ChainMap介绍</h1><p><code>collections.ChainMap</code>用于快速链接多个映射，并将它们视为一个单元。它通常比创建一个新字典并运行多次<code>update()</code>调用要快得多。</p>\n<p><code>ChainMap</code>的作用：</p>\n<ul>\n<li><code>ChainMap</code>将多个字典或其他映射组合在一起，以创建一个可更新的单一视图。如果没有指定映射，则提供一个空字典，以便新链始终至少有一个映射。</li>\n<li>底层映射存储在一个列表中。该列表是公共的，可以使用maps属性访问或更新。</li>\n<li>查找时，依次搜索底层映射，直到找到一个键。<strong>但是，写、更新和删除只对第一个映射进行操作！！！</strong></li>\n<li><code>ChainMap</code>通过引用合并底层映射。因此，如果某个底层映射更新了，这些更改将反映在<code>ChainMap</code>中。</li>\n<li>支持所有常用的字典方法。</li>\n</ul>\n<span id=\"more\"></span>\n\n<p>下面介绍<code>ChainMap</code>中常见的属性和方法。</p>\n<h2 id=\"maps\"><a href=\"#maps\" class=\"headerlink\" title=\"maps\"></a>maps</h2><p><code>maps</code>属性：用户可更新的映射列表。列表从最先搜索到最后搜索。它是唯一存储的状态，可以通过修改来更改要搜索的映射。<strong>该列表应始终包含至少一个映射</strong>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> ChainMap</span><br><span class=\"line\">a = <span class=\"built_in\">dict</span>(one=<span class=\"number\">1</span>, two=<span class=\"number\">2</span>)</span><br><span class=\"line\">b = <span class=\"built_in\">dict</span>(three=<span class=\"number\">3</span>, four=<span class=\"number\">4</span>)</span><br><span class=\"line\">chain_map = ChainMap(a, b)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(chain_map)</span><br><span class=\"line\"><span class=\"comment\"># ChainMap(&#123;&#x27;one&#x27;: 1, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(chain_map.maps)</span><br><span class=\"line\"><span class=\"comment\"># [&#123;&#x27;one&#x27;: 1, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;]</span></span><br><span class=\"line\"><span class=\"comment\"># 修改a中的值,chain_map会同步更新</span></span><br><span class=\"line\">a[<span class=\"string\">&quot;one&quot;</span>] = <span class=\"number\">10</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(chain_map)</span><br><span class=\"line\"><span class=\"comment\"># ChainMap(&#123;&#x27;one&#x27;: 10, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\"><span class=\"comment\"># 通过chain_map修改，a同步更新 </span></span><br><span class=\"line\">chain_map[<span class=\"string\">&quot;one&quot;</span>] = <span class=\"number\">20</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(a)</span><br><span class=\"line\"><span class=\"comment\"># &#123;&#x27;one&#x27;: 20, &#x27;two&#x27;: 2&#125;</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"new-child方法\"><a href=\"#new-child方法\" class=\"headerlink\" title=\"new_child方法\"></a><code>new_child</code>方法</h2><p><code>new_child(m=None, **kwargs)</code>：返回一个新的<code>ChainMap</code>，包含一个新的map，后面跟着当前实例中的所有<code>map</code>。如果指定了<code>m</code>，它将成为映射列表前面的新映射；如果未指定，则使用一个空字典，因此调用<code>.new_child()</code> 等效于\t<code>ChainMap(&#123;&#125;, *d.maps)</code>。如果指定了任何关键字参数，则更新传递的map或新的空字典。此方法用于创建子上下文，可以在不更改任何父映射中的值的情况下进行更新。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(chain_map)</span><br><span class=\"line\"><span class=\"comment\"># ChainMap(&#123;&#x27;one&#x27;: 20, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\">new_chain_map1 = chain_map.new_child()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(new_chain_map1)</span><br><span class=\"line\"><span class=\"comment\"># ChainMap(&#123;&#125;, &#123;&#x27;one&#x27;: 20, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\">c = <span class=\"built_in\">dict</span>(five=<span class=\"number\">5</span>)</span><br><span class=\"line\">new_chain_map2 = chain_map.new_child(c) </span><br><span class=\"line\"><span class=\"built_in\">print</span>(new_chain_map2)</span><br><span class=\"line\"><span class=\"comment\"># ChainMap(&#123;&#x27;five&#x27;: 5&#125;, &#123;&#x27;one&#x27;: 20, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"parents\"><a href=\"#parents\" class=\"headerlink\" title=\"parents\"></a>parents</h2><p><code>parents</code>属性：返回一个新的<code>ChainMap</code>，其中包含当前实例中除第一个映射外的所有映射。这对于跳过搜索中的第一个<code>map</code>非常有用。用例类似于嵌套作用域中使用的nonlocal关键字。这些用例也与内置super()函数的用例并行。对<code>d.parents</code>的引用相当于:<code>ChainMap(* d.maps[1:])</code>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(chain_map)</span><br><span class=\"line\"><span class=\"comment\"># ChainMap(&#123;&#x27;one&#x27;: 20, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\">new_chain_map3 = chain_map.parents</span><br><span class=\"line\"><span class=\"built_in\">print</span>(new_chain_map3)</span><br><span class=\"line\"><span class=\"comment\"># ChainMap(&#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br></pre></td></tr></table></figure>\n\n<p>注意，<code>ChainMap()</code>的迭代顺序是通过<strong>扫描从后到前的映射</strong>来确定的:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(chain_map)</span><br><span class=\"line\"><span class=\"comment\"># ChainMap(&#123;&#x27;one&#x27;: 20, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\">keys = <span class=\"built_in\">list</span>(chain_map)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(keys)</span><br><span class=\"line\"><span class=\"comment\"># [&#x27;three&#x27;, &#x27;four&#x27;, &#x27;one&#x27;, &#x27;two&#x27;]</span></span><br></pre></td></tr></table></figure>\n\n<p><code>ChainMap</code>类只对链中**的第一个映射进行更新(写入和删除)**，而查找将搜索整个链。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(chain_map)</span><br><span class=\"line\"><span class=\"comment\"># ChainMap(&#123;&#x27;one&#x27;: 20, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\">keys = <span class=\"built_in\">list</span>(chain_map)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(keys)</span><br><span class=\"line\"><span class=\"comment\"># [&#x27;three&#x27;, &#x27;four&#x27;, &#x27;one&#x27;, &#x27;two&#x27;]</span></span><br><span class=\"line\">chain_map[<span class=\"string\">&quot;three&quot;</span>] = <span class=\"number\">300</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(chain_map)</span><br><span class=\"line\"><span class=\"comment\"># ChainMap(&#123;&#x27;one&#x27;: 20, &#x27;two&#x27;: 2, &#x27;three&#x27;: 300&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br></pre></td></tr></table></figure>\n\n<p>因此，如果需要深度写入和删除，需要创建一个子类来更新链中更深的键。上面的例子中，修改字典中的<code>key=&#39;three&#39;</code>时，因为第一个字典没有对应的，所以新增了一个<code>map</code>。如果想修改第二个字典，可以通过下面的方式实现。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> ChainMap</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">DeepChainMap</span>(<span class=\"title class_ inherited__\">ChainMap</span>):</span><br><span class=\"line\">    <span class=\"string\">&#x27;Variant of ChainMap that allows direct updates to inner scopes&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__setitem__</span>(<span class=\"params\">self, key, value</span>):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> mapping <span class=\"keyword\">in</span> self.maps:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> key <span class=\"keyword\">in</span> mapping:</span><br><span class=\"line\">                mapping[key] = value</span><br><span class=\"line\">                <span class=\"keyword\">return</span></span><br><span class=\"line\">        self.maps[<span class=\"number\">0</span>][key] = value</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__delitem__</span>(<span class=\"params\">self, key</span>):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> mapping <span class=\"keyword\">in</span> self.maps:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> key <span class=\"keyword\">in</span> mapping:</span><br><span class=\"line\">                <span class=\"keyword\">del</span> mapping[key]</span><br><span class=\"line\">                <span class=\"keyword\">return</span></span><br><span class=\"line\">        <span class=\"keyword\">raise</span> KeyError(key)</span><br><span class=\"line\">        </span><br><span class=\"line\">a = <span class=\"built_in\">dict</span>(one=<span class=\"number\">1</span>, two=<span class=\"number\">2</span>)</span><br><span class=\"line\">b = <span class=\"built_in\">dict</span>(three=<span class=\"number\">3</span>, four=<span class=\"number\">4</span>)</span><br><span class=\"line\">chain_map = ChainMap(a, b)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(chain_map) <span class=\"comment\"># ChainMap(&#123;&#x27;one&#x27;: 1, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\"></span><br><span class=\"line\">chain_map[<span class=\"string\">&quot;three&quot;</span>] = <span class=\"number\">300</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(chain_map)</span><br><span class=\"line\"><span class=\"comment\"># ChainMap(&#123;&#x27;one&#x27;: 1, &#x27;two&#x27;: 2, &#x27;three&#x27;: 300&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\"></span><br><span class=\"line\">c = <span class=\"built_in\">dict</span>(one=<span class=\"number\">1</span>, two=<span class=\"number\">2</span>)</span><br><span class=\"line\">d = <span class=\"built_in\">dict</span>(three=<span class=\"number\">3</span>, four=<span class=\"number\">4</span>)</span><br><span class=\"line\">deep_chain_map = DeepChainMap(c, d)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(deep_chain_map)</span><br><span class=\"line\"><span class=\"comment\"># DeepChainMap(&#123;&#x27;one&#x27;: 1, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 3, &#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\"></span><br><span class=\"line\">deep_chain_map[<span class=\"string\">&quot;three&quot;</span>] = <span class=\"number\">300</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(deep_chain_map)</span><br><span class=\"line\"><span class=\"comment\"># DeepChainMap(&#123;&#x27;one&#x27;: 1, &#x27;two&#x27;: 2&#125;, &#123;&#x27;three&#x27;: 300, &#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">del</span> deep_chain_map[<span class=\"string\">&#x27;three&#x27;</span>]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(deep_chain_map)</span><br><span class=\"line\"><span class=\"comment\"># DeepChainMap(&#123;&#x27;one&#x27;: 1, &#x27;two&#x27;: 2&#125;, &#123;&#x27;four&#x27;: 4&#125;)</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>1、<code>ChainMap</code>可接受多个映射，然后在逻辑上使它们表现为一个单独的映射结构；它只是维护了一个记录底层映射关系的列表，然后去重定义常用的字典操作；</p>\n<p>2、如果有重复的键，会采用第一个映射中对应的值；</p>\n<p>3、修改<code>ChainMap</code>映射结构，会同时作用在自己和原始字典结构上；</p>\n<p>4、可以使用字典的<code>update()</code>方法，来替代上面的合并方案；但是这就需要创建一个新的字典对象(或者修改原字典，破坏了原始数据)，并且原始字典做了修改，并不会反映到新建的字典上；</p>\n<p>5、<code>ChainMap</code>使用的就是原始字典，因此原字典变，它也会改变。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p><a href=\"https://docs.python.org/3/library/collections.html#chainmap-examples-and-recipes\">https://docs.python.org/3/library/collections.html#chainmap-examples-and-recipes</a></p>\n<p><a href=\"https://cloud.tencent.com/developer/article/1597933\">https://cloud.tencent.com/developer/article/1597933</a></p>\n","tags":["Python"]},{"title":"Python中的Iterables vs Iterators","url":"/2023/04/23/Python%E4%B8%AD%E7%9A%84Iterables-vs-Iterators/","content":"<h1 id=\"Python中的Iterables-vs-Iterators\"><a href=\"#Python中的Iterables-vs-Iterators\" class=\"headerlink\" title=\"Python中的Iterables vs Iterators\"></a>Python中的Iterables vs Iterators</h1><p>术语<code>iterable</code>和<code>iterator</code>经常(错误地)互换使用，以描述支持迭代的对象，即允许迭代其元素的对象。实际上，<code>Python</code>中的迭代器(<code>iterators</code>)和可迭代对象(<code>iterables</code>)是两个截然不同的概念，通常会引起混淆，尤其是对新手而言。</p>\n<span id=\"more\"></span>\n\n<h2 id=\"理解Python中的迭代\"><a href=\"#理解Python中的迭代\" class=\"headerlink\" title=\"理解Python中的迭代\"></a>理解Python中的迭代</h2><p>在编写代码时，经常需要多次重复部分代码块。要达到这个目的，可以通过下面任意方法实现：</p>\n<ul>\n<li><p>按顺序重复代码块的多次</p>\n</li>\n<li><p>将代码块放入循环中，根据需要运行多少次即可</p>\n</li>\n</ul>\n<p>第一种方法的缺点是编写重复的代码，这是<strong>难以维护</strong>和不可扩展的。例如，下面的代码将打印三次问候语:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Hello!&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Hello!&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Hello!&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<p>虽然这个代码可以正常运行，但是如果您决定更新代码以打印“Hello, World!”而不仅仅是“Hello!”。在这种情况下，将不得不更新问候消息三次，这将增加维护的负担。如果使用更大更复杂的代码，它可能成为维护人员的噩梦。</p>\n<p>使用循环将是解决问题和避免可维护性问题的更好方法。循环允许您根据需要经常运行一段代码。考虑一下如何使用<code>while</code>循环编写上面的示例：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>times = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">while</span> times &lt; <span class=\"number\">3</span>:</span><br><span class=\"line\"><span class=\"meta\">... </span>    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Hello!&quot;</span>)</span><br><span class=\"line\"><span class=\"meta\">... </span>    times += <span class=\"number\">1</span></span><br><span class=\"line\">...</span><br><span class=\"line\">Hello!</span><br><span class=\"line\">Hello!</span><br><span class=\"line\">Hello!</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Python中的迭代协议\"><a href=\"#Python中的迭代协议\" class=\"headerlink\" title=\"Python中的迭代协议\"></a>Python中的迭代协议</h2><p>迭代协议被Python中的所有迭代工具所使用，并由各种对象类型(如for循环（for-loops）、推导式（comprehensions）、映射(maps)等实现。本质上，该协议由两种对象类型组成，即<code>iterable</code>和<code>iterator</code>。</p>\n<ul>\n<li>可迭代（<code>iterable</code>）对象是你在其元素上迭代的对象</li>\n<li>迭代器(<code>iterator</code>)对象是在迭代过程中产生值的对象</li>\n</ul>\n<h2 id=\"Iterable是什么？\"><a href=\"#Iterable是什么？\" class=\"headerlink\" title=\"Iterable是什么？\"></a>Iterable是什么？</h2><p>在<code>Python</code>中，<code>Iterable</code>是一个实现<code>__iter__()</code>方法的对象，并返回一个迭代器对象或一个实现<code>__getitem__()</code>方法的对象(并且在索引耗尽时应该引发IndexError)。内置可迭代对象包括<code>Lists</code>, <code>Sets</code>和<code>string</code>，因为这样的序列可以在<code>for</code>循环中迭代。</p>\n<p>请注意，在最近的<code>Python</code>版本中，实现<code>Iterables</code>的首选方式是通过实现<code>__iter__()</code>方法。<code>__getitem__()</code>方法是在现代迭代器之前使用的一种遗留功能。但是Python仍然认为实现<code>__getitem__()</code>方法的对象是<code>Iterables</code>。这意味着如果没有定义<code>__iter__()</code>， Python解释器将使用<code>__getitem__()</code>。要了解更多细节，您可以参考<a href=\"https://peps.python.org/pep-0234/\">PEP-234</a>。</p>\n<p>总而言之，Python中的<code>Iterable</code>是以下任何对象：</p>\n<ul>\n<li>可以遍历(例如，可以遍历字符串的字符或文件的行)</li>\n<li>实现<code>__iter__()</code>方法(或<code>__getitem__</code>)，因此可以使用返回<code>Iterator</code>的<code>iter()</code>来调用它</li>\n<li>可以出现在<code>for</code>循环的右侧(<code>for i in myIterable:</code>)</li>\n</ul>\n<h2 id=\"Iterator是什么？\"><a href=\"#Iterator是什么？\" class=\"headerlink\" title=\"Iterator是什么？\"></a>Iterator是什么？</h2><p>另一方面，<code>Python</code>中的迭代器是一个以下面方式实现<code>__next__()</code>方法的对象：</p>\n<ul>\n<li>返回可迭代对象的下一个值，并且更新迭代器的状态，使其指向下一个值</li>\n<li>当可迭代对象的元素耗尽时引发<code>StopIteration</code>异常</li>\n</ul>\n<p>此外，<code>Iterator</code>本身也是一个<code>Iterable</code>，因为它还必须实现<code>__iter__()</code>方法，在该方法中它只返回<code>self</code>。</p>\n<blockquote>\n<p><em>Every Iterator is also an Iterable, but not every Iterable is an Iterator</em></p>\n</blockquote>\n<h2 id=\"Python迭代器和迭代器的操作\"><a href=\"#Python迭代器和迭代器的操作\" class=\"headerlink\" title=\"Python迭代器和迭代器的操作\"></a>Python迭代器和迭代器的操作</h2><p>正如我们已经提到的，<code>Iterables</code>的内置对象类型之一是<code>Python List</code>。现在让我们假设我们有以下整数列表，如下所示:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>my_lst = [<span class=\"number\">5</span>, <span class=\"number\">10</span>, <span class=\"number\">15</span>]</span><br></pre></td></tr></table></figure>\n\n<p>由于<code>my_lst</code>是一个可迭代对象，我们可以运行<code>iter()</code>方法来从可迭代对象中获取一个迭代器对象:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>my_iter = <span class=\"built_in\">iter</span>(my_lst)</span><br></pre></td></tr></table></figure>\n\n<p>我们可以验证<code>my_iter</code>是<code>list_iterator</code>类型</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"built_in\">type</span>(my_iter)</span><br><span class=\"line\">list_iterator</span><br></pre></td></tr></table></figure>\n\n<p>现在，由于<code>my_iter</code>是一个迭代器，因此它实现了<code>__next__()</code>方法，该方法将返回列表<code>iterable</code>的下一个元素:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; my_iter.__next__()</span><br><span class=\"line\">5</span><br></pre></td></tr></table></figure>\n\n<p>一旦迭代器返回<code>__next__()</code>调用后的next值，它应该改变它的状态，使它现在指向下一个元素:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; my_iter.__next__()</span><br><span class=\"line\">10</span><br></pre></td></tr></table></figure>\n\n<p>请注意，<code>next(iter_name)</code>也是一个有效的语法，相当于<code>iter_name.__next__()</code>：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; next(my_iter)</span><br><span class=\"line\">15</span><br></pre></td></tr></table></figure>\n\n<p>现在我们到达了最后一个元素，下一次调用<code>__next__()</code>方法应该引发<code>StopIteration</code>，这是实现<code>__next__()</code>方法的迭代器必须满足的要求。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>my_iter.__next__()</span><br><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File <span class=\"string\">&quot;&lt;stdin&gt;&quot;</span>, line <span class=\"number\">1</span>, <span class=\"keyword\">in</span> &lt;module&gt;</span><br><span class=\"line\">StopIteration</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>在本文中，我们讨论了<code>Python</code>中的迭代协议，以及迭代对象和迭代器是如何参与迭代的。此外，我们还讨论了可迭代对象和迭代器的主要特性，并介绍了它们的主要区别。最后，我们展示了<code>Iterable</code>和<code>Iterator</code>对象是如何工作的。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p><a href=\"https://towardsdatascience.com/python-iterables-vs-iterators-688907fd755f\">https://towardsdatascience.com/python-iterables-vs-iterators-688907fd755f</a></p>\n<p><a href=\"https://realpython.com/python-iterators-iterables/\">https://realpython.com/python-iterators-iterables/</a></p>\n","tags":["Python"]},{"title":"Pytorch显存分析","url":"/2023/02/09/Pytorch%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90/","content":"<h1 id=\"Pytorch显存分析\"><a href=\"#Pytorch显存分析\" class=\"headerlink\" title=\"Pytorch显存分析\"></a>Pytorch显存分析</h1><p>在深度学习中，常见的错误是CUDA out of memory，这是因为显存放不下那么多模型参数和中间变量。通过及时清空中间变量，优化代码，减少batch等能够减少显存溢出。</p>\n<span id=\"more\"></span>\n\n<p><a href=\"https://pytorch.org/\">PyTorch</a>在进行深度学习训练的时候，有4大部分的显存开销，分别是</p>\n<ul>\n<li>模型参数(parameters)</li>\n<li>模型参数的梯度(gradients)</li>\n<li>优化器状态(optimizer states)</li>\n<li>中间激活值(intermediate activations) 或者叫中间结果(intermediate results)。</li>\n</ul>\n<p>其中activation占绝对大头，50%以上(有些地方说训练过程中占用显存最大的是计算图，把activation看作计算图的节点的话，也没错)优化器占的大小只是weight的2倍(Adam),SGD啥的话甚至1倍。</p>\n<h2 id=\"深度学习训练过程\"><a href=\"#深度学习训练过程\" class=\"headerlink\" title=\"深度学习训练过程\"></a>深度学习训练过程</h2><p>模型定义：定义了模型的网络结构，产生模型参数；</p>\n<p>while(你想训练):</p>\n<ol>\n<li>前向传播：执行模型的前向传播，产生中间激活值；</li>\n<li>后向传播：执行模型的后向传播，产生梯度；</li>\n<li>梯度更新：执行模型参数的更新，第一次执行的时候产生优化器状态。</li>\n</ol>\n<p>在模型定义完之后，1~3循环执行。</p>\n<h2 id=\"Torch机制\"><a href=\"#Torch机制\" class=\"headerlink\" title=\"Torch机制\"></a>Torch机制</h2><h3 id=\"CUDA-context-开销\"><a href=\"#CUDA-context-开销\" class=\"headerlink\" title=\"CUDA context 开销\"></a>CUDA context 开销</h3><p>在第一次执行CUDA操作时，使用GPU所需要创建维护设备间工作的一些相关信息。</p>\n<blockquote>\n<p>PyTorch will <strong>create the CUDA context in the very first CUDA operation</strong>, which can use ~600-1000MB of GPU memory depending on the CUDA version as well as the used device.<br>PyTorch itself will allocate the needed memory and will use an internal cache mechanism. You can read more about it <a href=\"https://pytorch.org/docs/stable/notes/cuda.html\">here</a>.</p>\n</blockquote>\n<blockquote>\n<ol>\n<li>You could rebuild PyTorch and remove libraries shipping with device code, such as cuDNN. While this would yield a performance hit, no cuDNN kernels will be loaded and stored in the CUDA context.</li>\n<li>Yes, you can use <code>PYTORCH_NO_CUDA_MEMORY_CACHING=1</code> to disable the cache.</li>\n<li>PyTorch needs to load its own kernels as well as device code from other libs (cuDNN, cublas, NCCL, etc.), which might not be the case for PyCUDA. It might be possible to share some driver-related code in the context, but I don’t know how much memory savings would be expected and haven’t experimented with it.</li>\n</ol>\n</blockquote>\n<p>显存的值跟CUDA的版本，Pytorch的版本以及所使用的设备都是有关系的。所以只有把任何小的张量放到GPU显存，那么至少会占用1000M左右显存，这部分显存是cuda运行时必须占掉的显存。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br><span class=\"line\">a = torch.tensor([<span class=\"number\">1.0</span>]).cuda()</span><br><span class=\"line\">time.sleep(<span class=\"number\">10</span>)</span><br></pre></td></tr></table></figure>\n\n<p>torch.tensor([1.0]).cuda()分配一个shape为1的tensor，<strong>理论上应该只有4bytes，但用allocated看会有512bytes</strong>。</p>\n<p><strong>在Docker容器中</strong>运行，使用不同的显卡测试，如下：</p>\n<blockquote>\n<p><strong>在1080显卡，pytorch 1.10.0+cu113，cuda11.4中测试，nvidia-smi初始显存为5266MiB，运行代码后为6171MiB。使用6171M-5266M&#x3D;905MiB。</strong></p>\n<p><strong>在RTX2080  Ti上，pytorch 1.10.0+cu113，cuda11.4中测试，初始为3M,，运行后为1476M，使用1476M-3M&#x3D;1473M。</strong></p>\n<p><strong>在3090上，pytorch 1.10.0+cu113，cuda11.4中测试，初始为8101M,，运行后为10290M，使用10290M-8101M&#x3D;2189M。</strong></p>\n</blockquote>\n<p><strong>在win10上运行</strong>，</p>\n<blockquote>\n<p><strong>在3090上，pytorch 1.10.1+cu111，cuda11.6中测试，初始为375M,，运行后为617M，使用617M-375M&#x3D;242M。</strong></p>\n</blockquote>\n<h3 id=\"PyTorch显存分配机制\"><a href=\"#PyTorch显存分配机制\" class=\"headerlink\" title=\"PyTorch显存分配机制\"></a>PyTorch显存分配机制</h3><p>在PyTorch中，显存是按页为单位进行分配的，这可能是CUDA设备的限制。以上面的代码为例，就算我们只想申请4字节的显存，Pytorch也会先向CUDA设备申请2MB的显存到自己的cache区中，然后pytorch再为我们分配512字节或者1024字节的空间。这个在使用torch.cuda.memory_allocated()的时候可以看出来512字节；用torch.cuda.memory_cached()（torch.cuda.memory_cached has been renamed to toch.cuda.memory_reserved)可以看出向CUDA申请的2MB。</p>\n<p>也就是说，PyTorch Allocated memory使用的是PyTorch reserved Memory里的显存，PyTorch reserved Memory则用的是GPU的显存。</p>\n<p>实际上，用nvidia-smi或者gpustat来看Pytorch程序的显存占用不是很合适的。 因为Pytorch的机制是使用缓存分配器来管理缓存分配的(因为这样速度快), 但是在缓存分配器的机制下, <strong>一个Tensor就算被释放了，进程也不会把空闲出来的显存还给GPU，而是等待下一个Tensor来填入这一片被释放的空间(即只要一个Tensor对象在后续不会再被使用，那么PyTorch就会自动回收该Tensor所占用的显存，并以缓冲区的形式继续占用显存，所以在nvidia-smi&#x2F;gpustat中看到的显存并没有减少)</strong></p>\n<p>   <em>即用nvidia-smi&#x2F;gpustat看到的其实是pytorch缓存区&#x2F;缓存分配器的情况</em></p>\n<p> 要是实在看缓冲区不爽的话，也可以用torch.cuda.empty_cache()把它归零，或者加一个环境变量PYTORCH_NO_CUDA_MEMORY_CACHING&#x3D;1，但是程序速度会变慢哦(试过在一个实验里慢了三倍)</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"> </span><br><span class=\"line\">device = torch.device(<span class=\"string\">&#x27;cuda:0&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 定义两个tensor</span></span><br><span class=\"line\">tensor1 = torch.randn(<span class=\"number\">120</span>, <span class=\"number\">3</span>, <span class=\"number\">512</span>, <span class=\"number\">512</span>).<span class=\"built_in\">float</span>().to(device)  </span><br><span class=\"line\"><span class=\"comment\"># 120*3*512*512*4/1000/1000 = 377.48M</span></span><br><span class=\"line\">tensor2 = torch.randn(<span class=\"number\">80</span>, <span class=\"number\">3</span>, <span class=\"number\">512</span>, <span class=\"number\">512</span>).<span class=\"built_in\">float</span>().to(device)  </span><br><span class=\"line\"><span class=\"comment\"># 80*3*512*512*4/1000/1000 = 251.64M</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated()/ <span class=\"number\">1000</span> / <span class=\"number\">1000</span>) <span class=\"comment\"># 629.15MiB</span></span><br><span class=\"line\"><span class=\"comment\"># 初始5308MiB,之后6811MiB，使用6811-5308=1423MiB</span></span><br><span class=\"line\"><span class=\"comment\"># 然后释放，使用del也可以进行释放</span></span><br><span class=\"line\">dummy_tensor_4 = dummy_tensor_4.cpu()</span><br><span class=\"line\">dummy_tensor_5 = dummy_tensor_5.cpu()</span><br><span class=\"line\"><span class=\"comment\"># 这里虽然将上面的显存释放了，但是我们通过Nvidia-smi命令看到显存依然在占用</span></span><br><span class=\"line\">torch.cuda.empty_cache()</span><br><span class=\"line\"><span class=\"comment\"># 只有执行完上面这句，显存才会在Nvidia-smi中释放</span></span><br></pre></td></tr></table></figure>\n\n<p>进程不需要重新向GPU申请显存了，运行速度会快很多，有什么坏处？他不能准确地给出某一个时间点具体的Tensor占用的显存，nvidia-smi<strong>显示的</strong>而是<strong>已经分配到的显存和context开销之和,</strong> <strong>也就是reserved_memory和torch context显存之和</strong>。</p>\n<p>   这也是令很多人在使用PyTorch时对显存占用感到困惑的罪魁祸首！！！</p>\n<h3 id=\"Pytorch释放机制\"><a href=\"#Pytorch释放机制\" class=\"headerlink\" title=\"Pytorch释放机制\"></a>Pytorch释放机制</h3><p>简单总结一下，就是在PyTorch中，只要一个Tensor对象在后续不会再被使用，那么PyTorch就会自动回收该Tensor所占用的显存，并以缓冲区的形式继续占用显存。</p>\n<p>要是实在看缓冲区不爽的话，也可以用torch.cuda.empty_cache()把它减少，或者加一个环境变量PYTORCH_NO_CUDA_MEMORY_CACHING&#x3D;1，但是程序速度会变慢哦，(曾经试过，在我的一个实验里慢了3倍)。</p>\n<p>然后就是选取合适的batch_size就可以开始训练啦，怎么估算多少batch_size可以刚好把咱们的显存占满呢，这里可以通过公式估算一下，（total_gpu_mem - model_used_mem）&#x2F;&#x2F; activation_mem</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">// 参数占用显存大小</span><br><span class=\"line\">params_mem = num_params * (2 if fp16_enabled else 4)</span><br><span class=\"line\">// 梯度占用显存大小</span><br><span class=\"line\">gradients_mem = num_params * (2 if fp16_enabled else 4)</span><br><span class=\"line\">// 优化器占用显存大小</span><br><span class=\"line\">optimizer_mem = num_params * (16 if fp16_enabled else 8)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"训练过程示例\"><a href=\"#训练过程示例\" class=\"headerlink\" title=\"训练过程示例\"></a>训练过程示例</h2><h3 id=\"模型定义\"><a href=\"#模型定义\" class=\"headerlink\" title=\"模型定义\"></a>模型定义</h3><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\">model = torch.nn.Linear(<span class=\"number\">1024</span>,<span class=\"number\">1024</span>, bias=<span class=\"literal\">False</span>).cuda() </span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated()) </span><br></pre></td></tr></table></figure>\n\n<p>输出4194304，刚好等于1024×1024×4</p>\n<h3 id=\"前向计算过程\"><a href=\"#前向计算过程\" class=\"headerlink\" title=\"前向计算过程\"></a>前向计算过程</h3><p>结论：显存增加等于每一层模型产生的结果的显存之和，且跟batch_size成正比。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">inputs = torch.tensor([<span class=\"number\">1.0</span>]*<span class=\"number\">1024</span>).cuda() </span><br><span class=\"line\"><span class=\"comment\"># shape = (1024)  数据占用显存1024*4=4096</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated()) </span><br><span class=\"line\"><span class=\"comment\"># 4194304(模型占用显存) + 4096（中间结果）=4198400</span></span><br><span class=\"line\">outputs = model(inputs) <span class=\"comment\"># outputs shape(1024) # 中间结果（1024*4=4096）</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated()) <span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\">#总共分配的4194304(模型参数占用) + 4096（输入） +4096（中间结果）　=4202496</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"反向传播过程\"><a href=\"#反向传播过程\" class=\"headerlink\" title=\"反向传播过程\"></a>反向传播过程</h3><p>后向传播会将模型的中间激活值给消耗并释放掉掉，并为每一个模型中的参数计算其对应的梯度。在第一次执行的时候，会为模型参数分配对应的用来存储梯度的空间。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">loss = <span class=\"built_in\">sum</span>(outputs) <span class=\"comment\">#  + 512 (torch allocate分配最小单位)</span></span><br><span class=\"line\">temp = torch.cuda.memory_allocated() <span class=\"comment\"># 4202496+512=4203008</span></span><br><span class=\"line\">loss.backward() <span class=\"comment\"># + 4194304（模型参数的梯度，1024*1024*4）</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated()) <span class=\"comment\"># 4203008+4194304=8397312</span></span><br></pre></td></tr></table></figure>\n\n<p>第一次执行时显存增加: 4194304字节 - 激活值大小</p>\n<p>第二次以后执行显存减少: 激活值大小</p>\n<p>Note: 由于这个中间值被赋给了outputs，所以后面在后向传播的时候会发现，这个outputs的显存没有被释放掉。但是当层数变深的时候，就能明显看到变化了。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"comment\"># 模型初始化</span></span><br><span class=\"line\">linear1 = torch.nn.Linear(<span class=\"number\">1024</span>,<span class=\"number\">1024</span>, bias=<span class=\"literal\">False</span>).cuda() <span class=\"comment\"># + 4194304</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated())</span><br><span class=\"line\">linear2 = torch.nn.Linear(<span class=\"number\">1024</span>, <span class=\"number\">1</span>, bias=<span class=\"literal\">False</span>).cuda() <span class=\"comment\"># + 4096</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输入定义</span></span><br><span class=\"line\">inputs = torch.tensor([[<span class=\"number\">1.0</span>]*<span class=\"number\">1024</span>]*<span class=\"number\">1024</span>).cuda() <span class=\"comment\"># shape = (1024,1024) # + 4194304</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 前向传播，正常来说，这里增加的中间结果有linear1的结果4194304，</span></span><br><span class=\"line\"><span class=\"comment\"># linear2的结果4096以及sum的结果512</span></span><br><span class=\"line\"><span class=\"comment\"># 实际上增加的是4194304 + 512！！！难道是sum和linear2操作合并了？？？？？？？</span></span><br><span class=\"line\">loss = <span class=\"built_in\">sum</span>(linear2(linear1(inputs))) </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 后向传播，中间值的显存释放，增加的显存是每个参数的梯度，linear1：4194304，linear2:4096</span></span><br><span class=\"line\">loss.backward() <span class=\"comment\"># memory - （4194304）+ 4194304 + 4096  </span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated()) <span class=\"comment\"># 由于释放一部分，再增加一部分，实际上增加4096</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 再来一次~</span></span><br><span class=\"line\">loss = <span class=\"built_in\">sum</span>(linear2(linear1(inputs))) <span class=\"comment\"># shape = (1) # memory + 4194304  (512没了，因为loss的ref还在)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated())</span><br><span class=\"line\">loss.backward() <span class=\"comment\"># memory - 4194304</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.memory_allocated())</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>讨论：从原理上讲，element-wise fusion 是可以省的呀，z&#x3D;linear1(x), w&#x3D;linear2(z), loss&#x3D;sum(w)，那么 dloss&#x2F;dw&#x3D;ones_like(w)，dloss&#x2F;dz&#x3D;linear2.weight.T @ dloss&#x2F;dw，整个过程中是不需要保存w的。但 pytorch 是否做了 fusion，就需要做 profile 来分析了。</p>\n</blockquote>\n<h2 id=\"参数更新\"><a href=\"#参数更新\" class=\"headerlink\" title=\"参数更新\"></a>参数更新</h2><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">optimizer.step() <span class=\"comment\"># 第一次增加8388608，第二次就不增不减了哦</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(torch.cuda.max_memory_allocated()) <span class=\"comment\"># = torch.memory_allocated + 8388608</span></span><br></pre></td></tr></table></figure>\n\n<p>第一次执行时，会为每一个参数初始化<strong>其优化器状态</strong>，对于这里的AdamW而言，每一个参数需要4*2&#x3D;8个字节。第二次开始，不会再额外分配显存。</p>\n<p>显存开销:</p>\n<p>第一次: 增加8388608字节</p>\n<p>第二次及以后: 无增减</p>\n<p>由于计算机计算的特性，有一些计算操作在计算过程中是会带来额外的显存开销的。但是这种开销在torch.memory_allocated中是不能被察觉的。比如在AdamW在进行某一层的更新的时候，会带来2倍该层参数量大小的临时额外开销。这个在max_memory_allocated中可以看到。在本例中就是8388608字节。</p>\n<h2 id=\"节省显存\"><a href=\"#节省显存\" class=\"headerlink\" title=\"节省显存\"></a>节省显存</h2><p>即使只创建了个只有一个元素的tensor,它照样会显示用了几百兆的显存。</p>\n<p><strong>除非在编译的时候去掉一些cuda的功能模块。自己编译pytorch，能降低不少cuda context的显存开销。</strong></p>\n<p>在分析PyTorch的显存时候，一定要使用torch.cuda里的显存分析函数，我用的最多的是torch.cuda.memory_allocated()和torch.cuda.max_memory_allocated()，前者可以精准地反馈当前进程中Torch.Tensor所占用的GPU显存，后者则可以告诉我们到调用函数为止所达到的最大的显存占用字节数。还有像torch.cuda.memory_reserved()这样的函数则是查看当前进程所分配的显存缓冲区是多少的。</p>\n<p>nvidia-smi 里看到的占用</p>\n<p>&#x3D; CUDA 上下文 + pytorch 缓存区</p>\n<p>&#x3D; CUDA 上下文 + 未使用缓存 + 已使用缓存</p>\n<ul>\n<li>Pytorch 内部有自己的缓存管理系统，能够加速显存分配。</li>\n<li>使用 torch.cuda.memory_allocated() 可以看到当前模型实际占用的显存。</li>\n<li>使用 torch.cuda.memory_reserved() 可以看到Pytorch总共占用的显存</li>\n<li>使用 torch.cuda.empty_cache() 清空未使用的缓存，但是已经使用的是不能释放的。</li>\n</ul>\n<p>只有一种情况需要使用 torch.cuda.empty_cache()，就是当你想要释放缓存以便让其他人也可以一起使用当前显卡，否则不需要调用这个方法。</p>\n<p><code>reserved</code> memory contains the <code>allocated</code> and cached memory.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">import torch</span><br><span class=\"line\">a = torch.zeros((1024, 1024)).cuda() # 实际大小4M=1024*1024*4</span><br><span class=\"line\">torch.cuda.memory_allocated() / 1024 / 1024  # 4.0 张量占用</span><br><span class=\"line\">torch.cuda.memory_reserved() / 1024 / 1024  # 20.0  缓存区</span><br><span class=\"line\"># nvidia-smi 上看占用1073M</span><br><span class=\"line\"></span><br><span class=\"line\"># 删除临时变量</span><br><span class=\"line\">del a</span><br><span class=\"line\">torch.cuda.memory_allocated() / 1024 / 1024  # 0.0 张量占用</span><br><span class=\"line\">torch.cuda.memory_reserved() / 1024 / 1024  # 20.0  缓存区</span><br><span class=\"line\"># 可以看到缓存区仍然占用了20M，nvidia-smi里也保持1073M的占用</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># 清空缓存区</span><br><span class=\"line\">torch.cuda.empty_cache()</span><br><span class=\"line\">torch.cuda.memory_allocated() / 1024 / 1024  # 0.0 张量占用</span><br><span class=\"line\">torch.cuda.memory_reserved() / 1024 / 1024  # 0.0  缓存区</span><br><span class=\"line\"># nvidia-smi里的占用回到了1053M</span><br></pre></td></tr></table></figure>\n\n\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">loss = loss_fn(out, batch_gt_tensor)</span><br><span class=\"line\">loss.backward()</span><br><span class=\"line\"># added lines</span><br><span class=\"line\">del batch_input_data</span><br><span class=\"line\">del batch_gt_data</span><br><span class=\"line\">torch.cuda.empty_cache()</span><br><span class=\"line\">optimizer.step()</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"不同框架的上下文\"><a href=\"#不同框架的上下文\" class=\"headerlink\" title=\"不同框架的上下文\"></a>不同框架的上下文</h2><p> creating an empty tensor in pytorch costs 800MB of GPU memory,</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">import torch; torch.zeros((), device=&#x27;cuda&#x27;)</span><br></pre></td></tr></table></figure>\n\n<p>or 170MB to run something similar in tensorflow:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">import tensorflow as tf; tf.constant([])</span><br></pre></td></tr></table></figure>\n\n<p>or 175MB in cupy</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">import cupy; cupy.array([])</span><br></pre></td></tr></table></figure>\n\n<p>or 170MB in pycuda</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">import pycuda.driver as cuda; import pycuda.autoinit; cuda.mem_alloc(1)</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>the context occupying memory is by design of the CUDA &#x2F; NVIDIA software stack. It’s non-actionable because there is nothing we can technically do about it.</p>\n<p>You could reduce the CUDA context size by removing kernels e.g. via dropping libraries such as MAGMA or cuDNN, if these are not used.<br>Besides that I’m not aware of reducing the operator set in the framework itself.</p>\n<p>it’s because of the large number of kernels. There isn’t an easy way to avoid loading them: that would require separating some kernels into separate shared libraries. Building multiple libraries would be a big engineering challenge because of inter-dependencies between the libraries (i.e the main library depends on kernels in secondary libraries; those libraries depend on code in the main library).</p>\n<p>Another strategy is to compile the kernels on-the-fly when needed. I think some of the other packages use that approach. That can slow down the initial use a lot. We could pre-compile and ship the most commonly used kernels and only compile the other kernels on-demand. Again, a big engineering challenge.</p>\n</blockquote>\n<p>CUDA11.7版本，可以设置 <code>CUDA_MODULE_LOADING=LAZY</code></p>\n<blockquote>\n<p>The initialization will create the CUDA context loading all kernels for your GPU architecture and is thus expected. The size of the context depends on the CUDA version, your GPU, the number of kernels in loaded CUDA libs as well as native PyTorch kernels.<br>You could update to CUDA 11.7 and enable lazy module loading via <code>CUDA_MODULE_LOADING=LAZY</code> which will load kernels only if they are needed and will thus reduce the context size.</p>\n<p>Yes, the env variable takes effect in CUDA 11.7+ and won’t change anything in CUDA 10.2. The context size reduction between PyTorch 1.9.0 and 1.12.0 would come from loading some modules lazily via the framework directly as well as a reduction in the number of kernels.</p>\n<p>延迟加载:延迟内核从主机加载到GPU，直到内核被调用。这也只加载已使用的内核，这可能会显著节省设备端内存。这也将加载延迟从应用程序的开始推迟到第一次调用内核的时候——总的二进制加载延迟通常会显著减少，但也会转移到应用程序的后期。<br>要启用这个特性，在启动进程之前设置环境变量CUDA_MODULE_LOADING&#x3D;LAZY。<br>注意，这个特性只兼容CUDA版本&gt;&#x3D; 11.7编译的库。</p>\n</blockquote>\n<p><a href=\"https://zhuanlan.zhihu.com/p/424512257\">https://zhuanlan.zhihu.com/p/424512257</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/486360176\">https://zhuanlan.zhihu.com/p/486360176</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/486360176%EF%BC%88%E8%B5%9E%EF%BC%81%EF%BC%89\">https://zhuanlan.zhihu.com/p/486360176（赞！）</a></p>\n<p><a href=\"https://github.com/pytorch/pytorch/blob/master/c10/cuda/CUDACachingAllocator.cpp\">https://github.com/pytorch/pytorch/blob/master/c10/cuda/CUDACachingAllocator.cpp</a></p>\n<p><a href=\"https://discuss.pytorch.org/t/how-do-i-create-torch-tensor-without-any-wasted-storage-space-baggage/131134/10\">https://discuss.pytorch.org/t/how-do-i-create-torch-tensor-without-any-wasted-storage-space-baggage/131134/10</a></p>\n<p><a href=\"https://www.cnblogs.com/devilmaycry812839668/p/15571390.html\">https://www.cnblogs.com/devilmaycry812839668/p/15571390.html</a></p>\n<p><a href=\"https://github.com/pytorch/pytorch/issues/20532\">https://github.com/pytorch/pytorch/issues/20532</a></p>\n<p><a href=\"https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__CTX.html#group__CUDA__CTX_1g0651954dfb9788173e60a9af7201e65a\">https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__CTX.html#group__CUDA__CTX_1g0651954dfb9788173e60a9af7201e65a</a></p>\n<p><a href=\"https://github.com/pytorch/pytorch/issues/55277\">https://github.com/pytorch/pytorch/issues/55277</a></p>\n<p><a href=\"https://discuss.pytorch.org/t/reduce-gpu-memory-blocked-by-context/142587\">https://discuss.pytorch.org/t/reduce-gpu-memory-blocked-by-context/142587</a></p>\n"},{"title":"Vision-Language Pretraining &#58 Current Trends and the Future","url":"/2023/01/15/vision-language/","content":"<p>由<a href=\"https://www.iro.umontreal.ca/~agrawal/\">Aishwarya Agrawal</a> (DeepMind，蒙特利尔大学，Mila)， <a href=\"http://www.damienteney.info/\">Damien Teney</a> (Idiap研究所)和<a href=\"http://www.aidanematzadeh.me/\">Aida Nematzadeh</a> (DeepMind)编写的ACL 2022教程。</p>\n<span id=\"more\"></span>\n\n<p>幻灯片：</p>\n<p>• Part 1: <a href=\"https://drive.google.com/file/d/1e0oLeQ2QrGcyP5OdINKhVDAjrCxtIIPV/view?usp=sharing\">Vision-language landscape before the pretraining era</a>.<br>• Part 2: <a href=\"https://vlp-tutorial-acl2022.github.io/part2_pretraining_models_aida.pdf\">Modern vision-language pretraining</a>.<br>• Part 3: <a href=\"https://vlp-tutorial-acl2022.github.io/Part3_BeyondStatisticalLearning_Teney.pdf\">Beyond statistical learning</a>.</p>\n<p>本教程的目标是概述解决多模态问题所需的要素，特别是视觉和语言。我们还将讨论这一领域的一些悬而未决的问题和有前景的未来方向。</p>\n<p>在过去的几年里，人们对构建多模态(视觉-语言)模型越来越感兴趣，这种模型是在更大但更嘈杂的数据集上预训练的，其中两种模态(例如，图像和文本)彼此不精确地对应(例如，ViLBERT和CLIP)。给定一个任务(例如视觉问题回答)，这些模型通常会在特定于任务的监督数据集上进行微调。除了更大的预训练数据集外，transformer架构和应用于两种模态的自注意力是最近的预训练模型在下游任务中令人印象深刻的表现的原因。这种方法很有吸引力，原因有几个：首先，预训练数据集通常是从网络自动爬取的，提供了可以忽略不计的收集成本的庞大数据集。其次，我们可以训练大模型一次，并在各种任务中复用它们。最后，这些预训练方法的性能优于或与之前的特定任务模型相当。一个有趣的问题是，除了良好的任务表现外，这些预训练的模型是否能够更好地捕捉两种模态之间的对齐。在本教程中，我们关注最近的视觉语言预训练范式。我们的目标是在多模态预训练领域之前，首先提供图像语言数据集、基准和建模创新的背景。接下来，我们将讨论用于视觉-语言预训练的不同类型模型，重点介绍它们的优缺点。最后，我们讨论了通过统计学习进行视觉-语言预训练的局限性，以及因果建模等替代方法的必要性。</p>\n<h2 id=\"教程类型\"><a href=\"#教程类型\" class=\"headerlink\" title=\"教程类型\"></a>教程类型</h2><p>这是一个前沿教程，重点讨论视觉语言预训练的新趋势：是否最新的模型导致更好的表示，以及它们如何有助于下游任务。我们计划主要讨论2018年及之后的最新论文，但也将包括2018年之前在当前视觉语言范式中发挥关键作用的有影响力的论文。</p>\n<h2 id=\"受众\"><a href=\"#受众\" class=\"headerlink\" title=\"受众\"></a>受众</h2><p>我们希望目标受众是对视觉与语言交叉感兴趣的研究者，如语言基础或基础交流研究者。本教程也对初中生谁开始他们的职业生涯感兴趣。熟悉最近的架构(如变压器)是有用的，但不是参加教程所必需的。</p>\n<h1 id=\"Vision-Language-landscape-before-the-Pretraining-Era\"><a href=\"#Vision-Language-landscape-before-the-Pretraining-Era\" class=\"headerlink\" title=\"Vision-Language landscape before the Pretraining Era\"></a>Vision-Language landscape before the Pretraining Era</h1><h2 id=\"大纲\"><a href=\"#大纲\" class=\"headerlink\" title=\"大纲\"></a>大纲</h2><ul>\n<li>Common VL tasks</li>\n<li>Task specific datasets and models</li>\n<li>What did the community gain?</li>\n<li>Open problems and avenues for future research</li>\n</ul>\n<h2 id=\"Common-VL-tasks\"><a href=\"#Common-VL-tasks\" class=\"headerlink\" title=\"Common VL tasks\"></a>Common VL tasks</h2><p>（1）图像检索</p>\n<p>● High level similarity<br>● Easy evaluation (recall@K)</p>\n<p><img src=\"/2023/01/15/vision-language/image1.PNG\"></p>\n<p>(2) Grounding Referring Expressions</p>\n<p><img src=\"/2023/01/15/vision-language/image2.PNG\"></p>\n<p>● Spatial localization<br>● Finer grained grounding<br>● Easy evaluation (precision@1)</p>\n<p>(3) Image Captioning</p>\n<p>● Language generation (in addition to visual recognition)<br>● Difficult automatic evaluation (BLEU, CIDEr)</p>\n<p><img src=\"/2023/01/15/vision-language/image3.PNG\"></p>\n<p>(4) Visual Question Answering</p>\n<p>● Elicit specific information from images<br>● Relatively easier evaluation (accuracy using string matching)</p>\n<p><img src=\"/2023/01/15/vision-language/image4.PNG\"></p>\n<p>(4 ) Visual Dialog</p>\n<p><img src=\"/2023/01/15/vision-language/image5.PNG\"></p>\n<ul>\n<li>Context modelling in addition to grounding </li>\n<li>Difficult to evaluate free form answers (retrieval metric used)</li>\n</ul>\n<p>Why vision and language?</p>\n<ul>\n<li>Intuitive:<ul>\n<li>Humans learn in multimodal settings</li>\n</ul>\n</li>\n<li>Applications:<ul>\n<li>Aid to visually impaired users</li>\n<li>Online shopping and organizing photos</li>\n<li>Grounded virtual assistants</li>\n</ul>\n</li>\n<li>Scientific:<ul>\n<li>Visual recognition</li>\n<li>Language understanding</li>\n<li>Grounding language into vision</li>\n<li>Compositional reasoning</li>\n<li>Commonsense reasoning</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"任务特定的数据和模型\"><a href=\"#任务特定的数据和模型\" class=\"headerlink\" title=\"任务特定的数据和模型\"></a>任务特定的数据和模型</h2><p>（1）Basic skeleton of most VL models (Image Retrieval)</p>\n<img src=\"/2023/01/15/vision-language/work\\个人博客\\blog\\source\\_posts\\vision-language\\image6.PNG\" style=\"zoom: 67%;\">\n\n<p>(2)Basic skeleton of most VL models (Grounding Referring Expressions)</p>\n<img src=\"/2023/01/15/vision-language/work\\个人博客\\blog\\source\\_posts\\vision-language\\image7.PNG\" style=\"zoom:67%;\">\n\n<p>(3) Basic skeleton of most VL models (VQA)</p>\n<img src=\"/2023/01/15/vision-language/img8.PNG\" style=\"zoom:67%;\">\n\n<p>(4) Basic skeleton of most VL models (Image Captioning)</p>\n<p><img src=\"/2023/01/15/vision-language/img9.PNG\"></p>\n<p>(5) Basic skeleton of most VL models (Visual Dialog)</p>\n<img src=\"/2023/01/15/vision-language/img10.PNG\" style=\"zoom:67%;\">\n\n<h3 id=\"VL-数据集\"><a href=\"#VL-数据集\" class=\"headerlink\" title=\"VL 数据集\"></a>VL 数据集</h3><ul>\n<li><p>Image Retrieval: <a href=\"https://shannon.cs.illinois.edu/DenotationGraph/\">Flickr</a><em>,</em> <a href=\"https://cocodataset.org/#download\">COCO</a></p>\n</li>\n<li><p>Grounding Referring Expression: <a href=\"https://github.com/lichengunc/refer\">RefCOCO</a>，<a href=\"https://ai.stanford.edu/~yukez/visual7w/\">Visual7W</a></p>\n</li>\n<li><p>Image Captioning: <em><a href=\"https://cocodataset.org/#download\">COCO</a></em></p>\n</li>\n<li><p><em>Visual Question Answering:</em> <em><a href=\"https://visualqa.org/\">VQA v1</a>，</em> *<a href=\"https://visualqa.org/\">VQA v2</a>*， <em><a href=\"https://visualgenome.org/\">Visual Genome</a></em>, <em><a href=\"https://cs.stanford.edu/people/dorarad/gqa/index.html\">GQA</a></em></p>\n</li>\n<li><p>Visual Dialog:<em><a href=\"https://visualdialog.org/\">Visual Dialog</a>，</em> <em><a href=\"https://github.com/GuessWhatGame/guesswhat\">GuessWhat?!</a></em></p>\n</li>\n</ul>\n<h3 id=\"任务1：图像描述（Image-Captioning）\"><a href=\"#任务1：图像描述（Image-Captioning）\" class=\"headerlink\" title=\"任务1：图像描述（Image Captioning）\"></a>任务1：图像描述（Image Captioning）</h3><p>Captioning datasets：COCO [Lin et al., 2014]</p>\n<p><img src=\"/2023/01/15/vision-language/img11.PNG\"></p>\n<p>COCO的 全称是Common Objects in COntext，是微软团队提供的一个可以用来进行图像识别的数据集。MS COCO数据集中的图像分为训练、验证和测试集。</p>\n<ul>\n<li>120k训练集+验证集[vs 1k（Pascal）,31k(Flikr)]</li>\n<li>图像包括91类目标，2.5M的标注样例</li>\n<li>图像描述任务的标注benchmark</li>\n</ul>\n<p>评价指标</p>\n<ul>\n<li>自动评估<ul>\n<li>基于n-gram重叠的度量（BLEU, Rouge, METEOR, CIDEr [Chen et al., 2015]）</li>\n<li>基于场景图的度量（SPICE [Anderson et al., 2016]）</li>\n</ul>\n</li>\n<li>人工评估</li>\n</ul>\n<h3 id=\"Neural-Image-Caption-NIC-CVPR-2015\"><a href=\"#Neural-Image-Caption-NIC-CVPR-2015\" class=\"headerlink\" title=\"Neural Image Caption (NIC) (CVPR 2015)\"></a><strong>Neural Image Caption (NIC) (CVPR 2015)</strong></h3><p>论文：<a href=\"https://arxiv.org/abs/1411.4555\">Show and Tell: A Neural Image Caption Generator</a></p>\n<p>NIC，我们的模型，是基于端到端的神经网络，由视觉CNN和语言生成RNN组成。它从输入图像中生成完整的自然语言句子，如下图的例子所示</p>\n<img src=\"/2023/01/15/vision-language/img12.PNG\" style=\"zoom:67%;\">\n\n<p>模型结构：LSTM模型结合CNN图像嵌入器和词嵌入。LSTM单元之间展开的连接显示为蓝色，它们对应循环连接。所有LSTM单元共享相同的参数。</p>\n<img src=\"/2023/01/15/vision-language/img13.PNG\" style=\"zoom:67%;\">\n\n<p>这些是模型产生的标题，你可以看到，它基本上正确地获得了场景的要点，但当涉及到具体的细节时，它可能不是很准确。</p>\n<p><img src=\"/2023/01/15/vision-language/img14.PNG\"></p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p><a href=\"https://aclanthology.org/2022.acl-tutorials.7.pdf\">Vision-Language Pretraining: Current Trends and the Future</a></p>\n","tags":["NLP","CV"]},{"title":"满江红·暮雨初收","url":"/2023/04/08/%E6%BB%A1%E6%B1%9F%E7%BA%A2%C2%B7%E6%9A%AE%E9%9B%A8%E5%88%9D%E6%94%B6/","content":"<blockquote class=\"song\">\n<p>\n            <tt>满江红·暮雨初收</tt><br>\n            <at>〔宋代〕 柳永</at><br>暮雨初收，长川静、征帆夜落。<br>临岛屿、蓼烟疏淡，苇风萧索。<br>几许渔人飞短艇，尽载灯火归村落。<br>遣行客、当此念回程，伤漂泊。<br>桐江好，烟漠漠。<br>波似染，山如削。<br>绕严陵滩畔，鹭飞鱼跃。<br>游宦区区成底事，平生况有云泉约。<br>归去来、一曲仲宣吟，从军乐。<br>\n </p>\n</blockquote>\n"},{"title":"einops库介绍","url":"/2023/08/22/einops%E5%BA%93%E4%BB%8B%E7%BB%8D/","content":"<h1 id=\"einops库介绍\"><a href=\"#einops库介绍\" class=\"headerlink\" title=\"einops库介绍\"></a>einops库介绍</h1><h2 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h2><p>einops是一个灵活且强大的操作张量的库，支持numpy，pytorch，TensorFlow，jax等。下面介绍一些三个常用的函数：rearrange，reduce, repeat。</p>\n<h2 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h2><p>使用pip进行安装</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">pip install einops</span><br></pre></td></tr></table></figure>\n\n<span id=\"more\"></span>\n\n<h2 id=\"rearrange\"><a href=\"#rearrange\" class=\"headerlink\" title=\"rearrange\"></a>rearrange</h2><p>用于对张量维度的重新组织排序，可替换Pytorch中的reshape，view，transpose和permute等操作。</p>\n<p>def rearrange(inputs, pattern, **axes_lengths) 转换inputs </p>\n<ul>\n<li>inputs (tensor): 表示输入的张量</li>\n<li>pattern (str): 表示张量维度变换的映射关系</li>\n<li>**axes_lengths: 表示按照指定的规格形式进行变换</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">x = torch.rand(<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>) <span class=\"comment\"># # shape:[2,3,4,5]</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;tensor([[[[7.3583e-01, 8.8132e-01, 7.5042e-01, 6.0073e-02, 5.1658e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [9.2195e-01, 1.7267e-01, 3.5516e-01, 1.7312e-01, 5.6702e-02],</span></span><br><span class=\"line\"><span class=\"string\">          [4.1464e-01, 7.8160e-01, 1.9855e-01, 2.5987e-01, 4.8159e-02],</span></span><br><span class=\"line\"><span class=\"string\">          [6.4739e-01, 3.2817e-01, 7.8965e-01, 8.3885e-01, 9.7713e-01]],</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">         [[5.6104e-01, 9.0705e-01, 6.2796e-01, 4.8434e-01, 1.3008e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [4.8028e-01, 8.4640e-01, 2.2299e-01, 1.6646e-01, 3.2246e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [5.6090e-01, 2.2387e-01, 5.5233e-01, 9.8023e-01, 8.2715e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [2.8145e-04, 3.3426e-01, 3.3481e-01, 2.5597e-01, 3.8871e-01]],</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">         [[6.0828e-01, 6.6627e-01, 8.4232e-01, 2.3180e-01, 4.4164e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [8.8813e-01, 3.4384e-01, 2.9303e-01, 7.4433e-01, 7.7608e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [9.4652e-01, 4.3120e-01, 5.6858e-01, 3.1444e-02, 3.0593e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [6.5291e-01, 4.1902e-01, 8.8847e-01, 3.5140e-01, 9.6655e-01]]],</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        [[[6.3584e-01, 7.0123e-01, 3.2156e-01, 4.8972e-01, 9.2589e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [6.6355e-01, 7.4288e-01, 2.3929e-01, 7.5589e-02, 6.2182e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [2.9493e-01, 4.8045e-01, 5.0630e-01, 4.5764e-02, 7.1761e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [6.0717e-02, 7.6304e-01, 6.6417e-01, 2.2472e-01, 4.7651e-01]],</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">         [[1.4037e-01, 1.3692e-02, 6.0491e-01, 3.8362e-02, 3.6310e-02],</span></span><br><span class=\"line\"><span class=\"string\">          [4.9173e-01, 5.2488e-01, 5.3634e-01, 9.5493e-01, 8.2945e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [9.7051e-01, 5.5878e-01, 4.4233e-01, 9.7438e-01, 9.8430e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [2.3281e-01, 2.8178e-01, 1.8327e-01, 7.3072e-01, 2.9559e-01]],</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">         [[5.0112e-01, 2.3173e-01, 3.5388e-01, 2.8602e-01, 6.9028e-03],</span></span><br><span class=\"line\"><span class=\"string\">          [2.3983e-01, 9.9609e-01, 2.8969e-01, 3.2394e-01, 3.5144e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [1.8526e-01, 2.8949e-01, 3.5263e-01, 5.9341e-01, 8.0384e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [9.2112e-01, 2.1679e-01, 7.9035e-01, 3.9351e-01, 2.3241e-01]]]])&quot;&quot;&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 将x的一些维度进行转换</span></span><br><span class=\"line\">y = x.transpose(<span class=\"number\">2</span>, <span class=\"number\">1</span>) <span class=\"comment\"># shape:[2,4,3,5] transpose只有两个参数，transpose(1,2)和transpose(2,1)是相同的。</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;tensor([[[[7.3583e-01, 8.8132e-01, 7.5042e-01, 6.0073e-02, 5.1658e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [5.6104e-01, 9.0705e-01, 6.2796e-01, 4.8434e-01, 1.3008e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [6.0828e-01, 6.6627e-01, 8.4232e-01, 2.3180e-01, 4.4164e-01]],</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">         [[9.2195e-01, 1.7267e-01, 3.5516e-01, 1.7312e-01, 5.6702e-02],</span></span><br><span class=\"line\"><span class=\"string\">          [4.8028e-01, 8.4640e-01, 2.2299e-01, 1.6646e-01, 3.2246e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [8.8813e-01, 3.4384e-01, 2.9303e-01, 7.4433e-01, 7.7608e-01]],</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">         [[4.1464e-01, 7.8160e-01, 1.9855e-01, 2.5987e-01, 4.8159e-02],</span></span><br><span class=\"line\"><span class=\"string\">          [5.6090e-01, 2.2387e-01, 5.5233e-01, 9.8023e-01, 8.2715e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [9.4652e-01, 4.3120e-01, 5.6858e-01, 3.1444e-02, 3.0593e-01]],</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">         [[6.4739e-01, 3.2817e-01, 7.8965e-01, 8.3885e-01, 9.7713e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [2.8145e-04, 3.3426e-01, 3.3481e-01, 2.5597e-01, 3.8871e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [6.5291e-01, 4.1902e-01, 8.8847e-01, 3.5140e-01, 9.6655e-01]]],</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        [[[6.3584e-01, 7.0123e-01, 3.2156e-01, 4.8972e-01, 9.2589e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [1.4037e-01, 1.3692e-02, 6.0491e-01, 3.8362e-02, 3.6310e-02],</span></span><br><span class=\"line\"><span class=\"string\">          [5.0112e-01, 2.3173e-01, 3.5388e-01, 2.8602e-01, 6.9028e-03]],</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">         [[6.6355e-01, 7.4288e-01, 2.3929e-01, 7.5589e-02, 6.2182e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [4.9173e-01, 5.2488e-01, 5.3634e-01, 9.5493e-01, 8.2945e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [2.3983e-01, 9.9609e-01, 2.8969e-01, 3.2394e-01, 3.5144e-01]],</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">         [[2.9493e-01, 4.8045e-01, 5.0630e-01, 4.5764e-02, 7.1761e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [9.7051e-01, 5.5878e-01, 4.4233e-01, 9.7438e-01, 9.8430e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [1.8526e-01, 2.8949e-01, 3.5263e-01, 5.9341e-01, 8.0384e-01]],</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">         [[6.0717e-02, 7.6304e-01, 6.6417e-01, 2.2472e-01, 4.7651e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [2.3281e-01, 2.8178e-01, 1.8327e-01, 7.3072e-01, 2.9559e-01],</span></span><br><span class=\"line\"><span class=\"string\">          [9.2112e-01, 2.1679e-01, 7.9035e-01, 3.9351e-01, 2.3241e-01]]]])&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"comment\"># 使用rearrange操作</span></span><br><span class=\"line\">y = rearrange(x, <span class=\"string\">&#x27;b c h w -&gt; b h w c&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"reduce\"><a href=\"#reduce\" class=\"headerlink\" title=\"reduce\"></a>reduce</h2><p>def repeat(inputs, pattern,  **axes_lengths)</p>\n<ul>\n<li>inputs (tensor): 表示输入的张量</li>\n<li>pattern (str): 表示张量按照某个维度复制的映射关系</li>\n<li>**axes_lengths: 表示按照指定的规格形式进行复制</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">x = np.random.randn(<span class=\"number\">100</span>, <span class=\"number\">32</span>, <span class=\"number\">64</span>)</span><br><span class=\"line\"><span class=\"comment\"># perform max-reduction on the first axis</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>y = reduce(x, <span class=\"string\">&#x27;t b c -&gt; b c&#x27;</span>, <span class=\"string\">&#x27;max&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># same as previous, but with clearer axes meaning</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>y = reduce(x, <span class=\"string\">&#x27;time batch channel -&gt; batch channel&#x27;</span>, <span class=\"string\">&#x27;max&#x27;</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n"},{"title":"Pytorch中的data和detach对比","url":"/2023/10/15/Pytorch%E4%B8%AD%E7%9A%84data%E5%92%8Cdetach%E5%AF%B9%E6%AF%94/","content":"<h2 id=\"1-data的使用\"><a href=\"#1-data的使用\" class=\"headerlink\" title=\"1 .data的使用\"></a>1 <code>.data</code>的使用</h2><p><code>.data</code>是从变量（Variable）中获取张量的主要方法。</p>\n<p>在Pytorch 0.4之前，被<code>torch.autograd.Variable</code>包裹的<code>Tensor</code>才能实现反向传播。</p>\n<p> <code>Varibale</code>包含的几个属性</p>\n<ul>\n<li><code>data</code>：存储了Tensor，是本体数据；</li>\n<li><code>grad</code>：保存&#x2F;累加data的梯度，本身是个Variable而非Tensor，与data形状一致；</li>\n<li><code>grad_fn</code>：指向Function对象，用于反向传播的梯度计算之用；</li>\n<li><code>requires_grad(bool)</code>：表示是否需要求梯度，默认为false；</li>\n<li><code>volatile(bool)</code>：如果某一个variable的volatile属性被设为True，那么所有依赖它的节点volatile属性都为True。volatile属性为True的节点不会求导，volatile的优先级比requires_grad高，默认为False。</li>\n</ul>\n<span id=\"more\"></span>\n\n<p>在Pytorch 0.4之后，张量类（<code>torch.Tensor</code>）和变量类(<code>torch.autograd.Variable</code>)合并为张量类。也就是说，<code>torch.Tensor</code>与旧的变量（<code>Variable</code>）一样，能够跟踪历史，并返回一个<code>torch.Tensor</code>类型的对象。这意味着不再需要在代码中到处使用<code>Variable</code>包装器。 <strong><code>torch.tensor</code>包含以下属性</strong></p>\n<ul>\n<li><code>dtype</code> 该张量存储的值类型，可选类型见：torch.dtype；</li>\n<li><code>device</code> 该张量存放的设备类型，cpu&#x2F;gpu；</li>\n<li><code>data</code> 该张量节点存储的值；</li>\n<li><code>requires_grad</code> 表示autograd时是否需要计算此tensor的梯度，默认False；</li>\n<li><code>grad</code> 存储梯度的值，初始为None；</li>\n<li><code>grad_fn</code> 反向传播时，用来计算梯度的函数；</li>\n<li><code>is_leaf </code>该张量节点在计算图中是否为叶子节点；</li>\n</ul>\n<p><code>tensor.data</code>创建一个<code>requires_grad=False</code>的tensor。</p>\n<p><code> .data</code> 返回相同数据张量,而且这个新的张量和原来的张量是共用数据的，一者改变，另一者也会跟着改变，而且新分离得到的张量的require s_grad &#x3D; False, 即不可求导的。（这一点其实detach是一样的）。由于<code>.data</code>不能被<code>autograd</code>追踪，在某些情况下可能是不安全的。</p>\n<p><code>tensor_a</code>初始化为一个<code>requires_grad = True</code>的张量，使用<code>tanh</code>以后的张量<code>tensor_a_out</code>，<code>requires_grad</code>同样是<code>True</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\">tensor_a = torch.tensor([<span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>, <span class=\"number\">3.0</span>], requires_grad = <span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;tensor_a=&#x27;</span>,tensor_a)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;tensor_a.requires_grad=&#x27;</span>,tensor_a.requires_grad)</span><br><span class=\"line\"><span class=\"comment\"># 对tensor_a使用tanh函数</span></span><br><span class=\"line\">tensor_a_out = tensor_a.tanh()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;tensor_a=&#x27;</span>,tensor_a)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;tensor_a_out=&#x27;</span>, tensor_a_out)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;tensor_a_out.requires_grad=&#x27;</span>, tensor_a_out.requires_grad)</span><br></pre></td></tr></table></figure>\n\n<p>输出：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">tensor_a= tensor([1., 2., 3.], requires_grad=True)</span><br><span class=\"line\">tensor_a.requires_grad= True</span><br><span class=\"line\">tensor_a= tensor([1., 2., 3.], requires_grad=True)</span><br><span class=\"line\">tensor_a_out= tensor([0.7616, 0.9640, 0.9951], grad_fn=&lt;TanhBackward0&gt;)</span><br><span class=\"line\">tensor_a_out.requires_grad= True</span><br></pre></td></tr></table></figure>\n\n<p><strong>注意</strong>：在<a href=\"https://so.csdn.net/so/search?q=PyTorch&spm=1001.2101.3001.7020\">PyTorch</a> 中，<code>torch.Tensor</code>是主要的张量类，所有的张量都是<code>torch.Tensor</code>的实例。<code>torch.Tensor</code>是<code>torch.FloatTensor</code>的别名。而<code>torch.tensor</code>是一个函数，返回的是一个tensor。</p>\n<p>使用<code>.data</code>查看<code>tensor_a_out</code>的数据</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 需要走注意的是，通过.data得到的的张量会和原来的张量共享内存，而且是不可求导的，data_tensor_a_out发生了变化，原来的张量tensor_a_out也会发生变化</span></span><br><span class=\"line\">data_tensor_a_out = tensor_a_out.data</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;data_tensor_a_out = &#x27;</span>, data_tensor_a_out)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;data_tensor_a_out.requires_grad = &#x27;</span>, data_tensor_a_out.requires_grad)</span><br></pre></td></tr></table></figure>\n\n<p>输出：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">data_tensor_a_out =  tensor([<span class=\"number\">0.7616</span>, <span class=\"number\">0.9640</span>, <span class=\"number\">0.9951</span>], grad_fn=&lt;TanhBackward0&gt;)</span><br><span class=\"line\">data_tensor_a_out.requires_grad =  <span class=\"literal\">False</span></span><br></pre></td></tr></table></figure>\n\n<p>使用<code>data_ptr()</code>判断两个张量是不是共享内存。<code>data_ptr()</code>返回tensor首元素的内存地址， 常用来判断两个Tensor是不是共享内存</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">rint(<span class=\"string\">&quot;data_tensor_a_out.data_ptr()=&quot;</span>, data_tensor_a_out.data_ptr())</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;tensor_a_out.data_ptr()=&quot;</span>, tensor_a_out.data_ptr())</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;data_tensor_a_out.data_ptr() == tensor_a_out.data_ptr()=&quot;</span>, data_tensor_a_out.data_ptr() == tensor_a_out.data_ptr()) <span class=\"comment\"># True</span></span><br></pre></td></tr></table></figure>\n\n<p>输出：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">data_tensor_a_out.data_ptr()= 121330752</span><br><span class=\"line\">tensor_a_out.data_ptr()= 121330752</span><br><span class=\"line\">data_tensor_a_out.data_ptr() == tensor_a_out.data_ptr()= True</span><br></pre></td></tr></table></figure>\n\n<p>类似地，张量的转置也是共享内存的。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 因为转置操作只是改变了stride信息，tensor_a_out_t和tensor_a_out指向同一个memory</span></span><br><span class=\"line\">tensor_a_out_t = tensor_a_out.t() <span class=\"comment\"># Transpose</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(tensor_a_out.data_ptr() == tensor_a_out_t.data_ptr()) <span class=\"comment\"># True</span></span><br></pre></td></tr></table></figure>\n\n<p>需要走注意的是，通过<code>.data</code>得到的的张量会和原来的张量共享内存，而且是不可求导的，<code>data_tensor_a_out</code>发生了变化，原来的张量<code>tensor_a_out</code>也会发生变化。</p>\n<p>假设不小心修改了<code>data_tensor_a_out</code>的值，导致<code>data_tensor_a_out</code>一起改变</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 将data_tensor_a_out置0</span></span><br><span class=\"line\">data_tensor_a_out.zero_()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;data_tanh_tensor_a = &#x27;</span>, data_tensor_a_out)</span><br><span class=\"line\"><span class=\"comment\"># tensor_a_out也会被置0</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;tensor_a_out = &#x27;</span>, tensor_a_out)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;tensor_a_out.requires_grad=&#x27;</span>, tensor_a_out.requires_grad)</span><br></pre></td></tr></table></figure>\n\n<p>输出：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">data_tensor_a_out =  tensor([<span class=\"number\">0.</span>, <span class=\"number\">0.</span>, <span class=\"number\">0.</span>])</span><br><span class=\"line\">tensor_a_out =  tensor([<span class=\"number\">0.</span>, <span class=\"number\">0.</span>, <span class=\"number\">0.</span>], grad_fn=&lt;TanhBackward0&gt;)</span><br><span class=\"line\">tensor_a_out.requires_grad= <span class=\"literal\">True</span></span><br></pre></td></tr></table></figure>\n\n<p><code>data_tensor_a_out</code>假设不小心置0，导致<code>tensor_a_out</code>也变成0.，那么通过<code>tensor_a_out</code>得到的损失函数就是错的。此外，<code>.data</code>不能被<code>autograd</code>追踪，在某些情况下可能是不安全的。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">loss = tensor_a_out.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;loss:&quot;</span>, loss.item())</span><br><span class=\"line\"><span class=\"comment\"># 反向传播</span></span><br><span class=\"line\">loss.backward()</span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;tensor_a.grad=&quot;</span>, tensor_a.grad)  <span class=\"comment\"># 不会报错,</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;tensor_a_out.grad=&quot;</span>, tensor_a_out.grad) <span class=\"comment\"># 非叶子节点，访问梯度警告</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;data_tensor_a_out.grad=&quot;</span>, data_tensor_a_out.grad) </span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;tensor_a_out.is_leaf=&quot;</span>, tensor_a_out.is_leaf)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;data_tensor_a_out.is_leaf=&quot;</span>, data_tensor_a_out.is_leaf)</span><br></pre></td></tr></table></figure>\n\n<p>输出：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">loss: 0.0</span><br><span class=\"line\">tensor_a.grad= tensor([1., 1., 1.])</span><br><span class=\"line\">test.py:35: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won&#x27;t be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:486.)</span><br><span class=\"line\">  print(&quot;tensor_a_out.grad=&quot;, tensor_a_out.grad) # 非叶子节点，访问梯度警告</span><br><span class=\"line\">tensor_a_out.grad= None</span><br><span class=\"line\">data_tensor_a_out.grad= None</span><br><span class=\"line\">tensor_a_out.is_leaf= False</span><br><span class=\"line\">data_tensor_a_out.is_leaf= True</span><br></pre></td></tr></table></figure>\n\n<p>这里的警告是说：正在访问非叶张量的张量的<code>.grad</code>属性。在<code>autograd.backward()</code>期间不会填充它的<code>.grad</code>属性。如果确实需要非叶子张量的梯度，请在非叶张量上使用<code>.retain_grad()</code>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 在反向传播之前使用，可以保留梯度</span></span><br><span class=\"line\">tensor_a_out.retain_grad()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;tensor_a_out.grad=&quot;</span>, tensor_a_out.grad) </span><br></pre></td></tr></table></figure>\n\n<p>从上面的例子可以看出，由于更改<code>data_ensor_a_out</code>的值，导致原来的张量<code>tensor_a_out</code>的值也跟着改变了，但是这种改变对于<code>autograd</code>是没有察觉的，它依然按照求导规则来求导，导致得出完全错误的导数值却浑然不知。它的风险性就是如果我再任意一个地方更改了某一个张量，求导的时候也没有通知我已经在某处更改了，导致得出的导数值完全不正确，故而风险大。</p>\n<p>注意，<code>requires_grad()</code>为<code>True</code>并不意味着可以输出对应的梯度，还要看<code>is_leaf</code>属性，当<code>is_leaf=false</code>时，也即该变量是非叶张量，则会爆出上面的错误。</p>\n<p>那么<code>is_leaf</code>是如何产生的：由用户初始创建的变量，而不是程序中间产生的结果变量，那么该变量为<strong>叶变量</strong>。</p>\n<h2 id=\"2-tensor-detach-的使用\"><a href=\"#2-tensor-detach-的使用\" class=\"headerlink\" title=\"2 tensor.detach()的使用\"></a>2 tensor.detach()的使用</h2><p><code>Tensor.detach()</code>返回一个与当前图分离的新张量。新张量的<code>requires_grad</code>为<code>False</code>。 需要注意，返回的<code>tensor</code>和原始的<code>tensor</code>共用同一个<code>data tensor</code>，<code>in-place</code>修改会在两个tensor上同时体现。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">detach_tensor_a_out = tensor_a_out.detach()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;detach_tensor_a_out = &#x27;</span>, tensor_a_out)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;detach_tensor_a_out.requires_grad = &#x27;</span>, detach_tensor_a_out.requires_grad)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(detach_tensor_a_out.data_ptr())</span><br><span class=\"line\"><span class=\"built_in\">print</span>(tensor_a_out.data_ptr())</span><br><span class=\"line\"><span class=\"built_in\">print</span>(detach_tensor_a_out.data_ptr() == tensor_a_out.data_ptr()) <span class=\"comment\"># True</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">detach_tensor_a_out.zero_()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;detach_tensor_a_out = &#x27;</span>, detach_tensor_a_out)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;tensor_a_out = &#x27;</span>, tensor_a_out)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;tensor_a_out.requires_grad=&#x27;</span>, tensor_a_out.requires_grad)</span><br><span class=\"line\"></span><br><span class=\"line\">loss = tensor_a_out.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;loss:&quot;</span>, loss.item())</span><br><span class=\"line\"><span class=\"comment\"># 反向传播</span></span><br><span class=\"line\">loss.backward()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;tensor_a.grad=&quot;</span>, tensor_a.grad)  <span class=\"comment\"># 不会报错,</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;tensor_a_out.grad=&quot;</span>, tensor_a_out.grad) <span class=\"comment\"># 非叶子节点，访问梯度警告</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;detach_tensor_a_out.grad=&quot;</span>, detach_tensor_a_out.grad) </span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;tensor_a_out.is_leaf=&quot;</span>, tensor_a_out.is_leaf)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;detach_tensor_a_out.is_leaf=&quot;</span>, detach_tensor_a_out.is_leaf)</span><br></pre></td></tr></table></figure>\n\n\n\n<p>输出</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">detach_tensor_a_out =  tensor([0.7616, 0.9640, 0.9951], grad_fn=&lt;TanhBackward0&gt;)</span><br><span class=\"line\">detach_tensor_a_out.requires_grad =  False</span><br><span class=\"line\">119204992</span><br><span class=\"line\">119204992</span><br><span class=\"line\">True</span><br><span class=\"line\">detach_tensor_a_out =  tensor([0., 0., 0.])</span><br><span class=\"line\">tensor_a_out =  tensor([0., 0., 0.], grad_fn=&lt;TanhBackward0&gt;)</span><br><span class=\"line\">tensor_a_out.requires_grad= True</span><br><span class=\"line\">****************************************************************************************************</span><br><span class=\"line\">loss: 0.0</span><br><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File &quot;test.py&quot;, line 32, in &lt;module&gt;</span><br><span class=\"line\">    loss.backward()</span><br><span class=\"line\">  File &quot;lib/python3.11/site-packages/torch/_tensor.py&quot;, line 487, in backward</span><br><span class=\"line\">    torch.autograd.backward(</span><br><span class=\"line\">  File &quot;lib/python3.11/site-packages/torch/autograd/__init__.py&quot;, line 200, in backward</span><br><span class=\"line\">    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass</span><br><span class=\"line\">RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [3]], which is output 0 of TanhBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).</span><br></pre></td></tr></table></figure>\n\n<p>从上面的例子可以看出，更改分离之后的变量值<code>detach_tensor_a_out</code>，导致原来的张量<code>tensor_a_out</code>的值也跟着改变了，这个时候如果依然按照求导规则来求导，由于<code>tensor_a_out</code>已经更改了，所以不会再继续求导了，而是报错，这样就避免了得出完全牛头不对马嘴的求导结果。</p>\n<h2 id=\"3-参考\"><a href=\"#3-参考\" class=\"headerlink\" title=\"3 参考\"></a>3 参考</h2><ol>\n<li><a href=\"https://blog.csdn.net/QKK612501/article/details/115829190\">https://blog.csdn.net/QKK612501/article/details/115829190</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/505445223\">https://zhuanlan.zhihu.com/p/505445223</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/436599662\">https://zhuanlan.zhihu.com/p/436599662</a></li>\n</ol>\n","tags":["NLP, Pytorch"]},{"title":"git 443问题解决","url":"/2024/01/21/git-443%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/","content":"<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>​\t由于特殊原因，使用Git访问Github时，比如使用<code>git clone，git push...</code>可能常会出现443问题： fatal: unable to access ‘<a href=\"https://github.com/tmp.git/\">https://github.com/tmp.git/</a>‘: Failed to connect to github.com port 443 after 21044 ms: Timed out。443是HTTP协议中的一个端口号，用于安全的HTTPS通信。然后，浏览器是可以通过梯子访问的。这是由于浏览器配置了代理来访问Github，但是git没有使用。</p>\n<h2 id=\"解决\"><a href=\"#解决\" class=\"headerlink\" title=\"解决\"></a>解决</h2><p>通过如下方式配置<code>Git</code>代理</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">git config --global http.proxy &quot;127.0.0.1:port&quot;  </span><br><span class=\"line\">git config --global https.proxy &quot;127.0.0.1:port&quot;</span><br></pre></td></tr></table></figure>\n\n<p><strong>其中的port需要替换为代理的端口号</strong></p>\n","tags":["Git"]},{"title":"vscode配置deepspeed进行debug","url":"/2023/10/17/vscode%E9%85%8D%E7%BD%AEdeepspeed%E8%BF%9B%E8%A1%8Cdebug/","content":"<p>Visual Studio Code的一个关键特性是它强大的调试支持。VS Code的内置调试器有助于加速你的编辑、编译和调试过程。</p>\n<h2 id=\"安装环境\"><a href=\"#安装环境\" class=\"headerlink\" title=\"安装环境\"></a>安装环境</h2><p>首先需要安装<code>Python</code>环境，如果是<code>Vscode</code>远程连接服务器，那么要<code>Python</code>插件安装到远程服务器中。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://code.visualstudio.com/docs/editor/debugging\">Debugging</a></li>\n</ol>\n","tags":["deepspeed"]},{"title":"InternLM-XComposer2-4KHD A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD","url":"/2024/04/17/InternLM-XComposer2-4KHD-md/","content":"<p>论文：<a href=\"https://arxiv.org/abs/2404.06512\">https://arxiv.org/abs/2404.06512</a><br>Github:：<a href=\"https://github.com/InternLM/InternLM-XComposer\">https://github.com/InternLM/InternLM-XComposer</a></p>\n<p>大型视觉-语言模型(LVLM)领域已经取得了重大进展，但由于分辨率有限，在理解细粒度视觉内容方面的挑战阻碍了其进展。最近的努力旨在增强LVLMs的高分辨率理解能力，但它们仍然限制在大约1500 × 1500像素，并限制在相对狭窄的分辨率范围内。本文提出了InternLMXComposer2-4KHD，这是将LVLM分辨率提升到4K高清(3840 × 1600)及以上的开创性探索。同时，考虑到并非所有场景都需要超高分辨率，它支持从336像素到4K标准的多种分辨率，极大地拓宽了其适用范围。具体而言，该研究通过引入一种新颖的扩展：基于自动patch配置的动态解析来推进patch划分范式。它在保持训练图像宽高比的同时，根据预训练的视觉Transformer (ViT) (336 × 336)自动改变patch计数和配置布局，从而实现从336像素到4K标准的动态训练分辨率。我们的研究表明，将训练分辨率扩大到4K高清，可以在不触及潜在改进上限的情况下实现持续的性能增强。InternLM-XComposer2-4KHD表现出卓越的能力，在16个基准测试中的10个测试中匹配甚至超过GPT4V和Gemini Pro。具有7B参数的InternLM-XComposer2-4KHD模型系列在<a href=\"https://github.com/InternLM/InternLM-XComposer%E5%8F%AF%E7%94%A8%E3%80%82\">https://github.com/InternLM/InternLM-XComposer可用。</a></p>\n<span id=\"more\"></span>\n\n<h2 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h2><p>近年来，大型语言模型(LLMs)的进展[10,21,29,39,73,78,91-93]引发了大型视觉-语言模型(LVLMs)的发展。这些模型已被证明可以熟练地完成图像描述[14,17]和视觉问答(VQA)[31, 33, 57, 107]等任务。然而，由于其有限的分辨率，它们很难处理包含精细细节的图像，如图表[68]、表格[87]、文档[70]和信息图[69]。这限制了它们在现实场景中的实际应用。</p>\n<p>最近的进展旨在提高大型视觉-语言模型(LVLM)的分辨率。一些方法[36,48,66,97]涉及直接自适应高分辨率视觉编码器。然而，视觉Transformer (ViT)架构在处理不同分辨率和宽高比的图像时存在不足，从而限制了其有效处理不同输入的能力。或者，一些方法[37,46,50,51,55,59,99]保持视觉编码器的分辨率，将高分辨率图像分割为多个低分辨率块。然而，这些方法受到分辨率不足的限制，通常在1500 × 1500左右，不能满足日常内容的需求，例如网站截图[85]、文档页面[70]和蓝图[69]。此外，它们被限制在一些预定义的高分辨率设置[36,46,48,50,51,55,59,66,97]或有限的分辨率范围[37,99]，从而限制了它们在各种应用程序中的用途。</p>\n<p>本文提出InternLM-XComposer2- 4KHD，一种开创性的模型。首次将大型视觉-语言模型((LVLMs)的分辨率能力扩展到4K高清甚至更高，从而在高分辨率视觉-语言理解方面设立了一个新标准。InternLM-XComposer2-4KHD旨在处理广泛的分辨率，支持从336像素到4K高清的任何纵横比的图像，便于在现实环境中部署。</p>\n<p>InternLM-XComposer2-4KHD遵循patch划分[45,50]范式，并通过合并一个创新的扩展来增强它：自动补丁配置的动态解析。具体来说，将大型视觉语言模型(LVLMs)的分辨率扩展到4K高清甚至更高的标准，远远不仅仅是增加补丁的数量。它涉及一种微妙的方法来克服特定挑战。（1）动态分辨率和自动patch配置：为解决高分辨率训练数据的稀缺问题，该框架引入了一种策略，在自动布局配置的同时动态调整分辨率。在训练过程中，它保持图像的原始宽高比比，同时自适应地改变patch (336 × 336)布局和计数。这导致训练分辨率超过原始图像分辨率，达到4KHD，解决了高分辨率数据的不足。（2）处理patch配置的可变性：尽管动态分辨率训练表面上很简单，但补丁配置的可变性会严重混淆LVLMs。为缓解这一问题，在每行patch token之后引入一个换行标记，以清晰地描述补丁布局，减少训练模糊性，并显著提高性能。（3）超越4K分辨率的推论：我们的观察表明，即使在高达4K分辨率的图像上进行训练，该模型也可以通过处理更高分辨率的图像，在推理过程中实现额外的性能改进。</p>\n<p>此外，将训练分辨率扩大到4K标准可以导致性能的持续改善，突出了甚至超过4K分辨率的训练潜力。这强调了进一步增强模型能力的能力，并为在大型视觉-语言模型领域内推进高分辨率图像处理的前沿提出了一个有希望的道路。</p>\n<p>在跨领域的16个不同基准上评估了InternLM-XComposer2-4KHD，包括5个具有挑战性的HD-OCR数据集(DocVQA[70]、ChartQA[68]、InfographicVQA[69]、TextVQA[87]和OCRBench[58])。与之前的开源LVLM模型和闭源api相比，所提出方法在16个基准中的6个中实现了SOTA结果，尽管只有7B参数，但显示出有竞争力的性能。如图1所示，InternLM-XComposer2-4KHD在10个基准测试中的性能甚至超过了GPT4V[74]和Gemini Pro[90]。值得注意的是，所提出方法在5个HD-OCR数据集上表现出了优异的性能，大大超过了现有的开源LVLM。</p>\n<h2 id=\"相关工作\"><a href=\"#相关工作\" class=\"headerlink\" title=\"相关工作\"></a>相关工作</h2><p>**大型视觉语言模型(LVLMs)**。大型语言模型(LLMs)[6, 9, 10, 23, 39, 41, 73, 76, 78, 91 - 93, 108]由于在文本生成和问答等各种与语言相关的任务中令人印象深刻的表现，受到了极大的关注。在这种热情的支持下，最近出现了大型视觉-语言模型(LVLMs)[4, 7, 16, 18, 19, 25, 28, 32, 47, 74, 77, 102, 110, 113]，将llm与视觉编码器[79,89,109]相结合，以利用语言和视觉模态的互补优势。通过融合文本和视觉表示，LVLMs可以在视觉上下文中建立语言基础，从而能够更全面地理解和生成多模态内容[5,11,14,20,27,51,60,95]。</p>\n<p>用于高分辨率理解的LVLMs。大型视觉-语言模型(LVLMs)通常使用CLIP-ViT作为视觉编码器来完成视觉相关任务。然而，视觉编码器对低分辨率的依赖，如224 × 224或336 × 336像素，限制了其在OCR和文档&#x2F;图表感知等高分辨率任务中的有效性。为了增强对高分辨率的理解，最近的工作主要采用以下策略:(1)高分辨率(HR)视觉编码器或双编码器，以满足HR和低分辨率(LR)输入[36,48,66,97]。例如，Vary[97]引入了一个支持HR输入的新图像编码器，然后将其与原始CLIP视觉编码器的LR嵌入连接起来。类似地，CogAgent[36]和Mini-Gemini[48]也使用不同的视觉编码器分离HR和LR图像，随后使用交叉注意力模块合并它们的特征。所提出方法提供了一个更简化的解决方案，并在不同分辨率和长宽比的输入中显示出优势。(2)裁剪后的图像块[37,46,50,51,59,99,101]。例如，Monkey[50]利用滑动窗口将图像分割为小块，然后使用LoRA微调进行处理。TextMonkey[59]进一步提出了转移窗口注意力和token重采样器来考虑不同补丁之间的联系。这些方法受限于一些预定义的高分辨率设置[36、46、48、50、51、55、59、66、97]或有限的分辨率范围[37、99]。相反，所提出方法设计了一种动态图像划分策略，以支持从336像素缩放到4K分辨率，最大分辨率比以前的方法更大(例如，Monkey[50]的1.5k和UReader的2k[101])。</p>\n<p>用于文档理解的LVLMs。文档理解涉及分析和理解各种数字文档，如数字、表格和学术论文。许多文档理解任务需要模型来处理高分辨率输入、复杂布局、各种纵横比和不同的文档格式。为了增强 LVLMs理解文档的能力，一些工作收集并构建了高质量的文档指令调优数据，包括LLaVAR [112]， mplug docowl[100]和TGDoc[96]。DocPediaDocPedia[30]在频域处理文档输入。之前的一些工作通过为高分辨率输入设计特殊模块，如HR和LR编码器[36,97]或裁剪的图像块[59,99,101]，提高了文档理解能力。InternLMXComposer2-4KHD首次扩展到4K分辨率输入，并在与OCR相关的基准上展示了强大的文档理解能力。此外，该方法还在其他通用LVLM基准上取得了可比的结果，如感知和推理[15,33,57,61]。</p>\n<h2 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h2><h3 id=\"模型结构\"><a href=\"#模型结构\" class=\"headerlink\" title=\"模型结构\"></a>模型结构</h3><p>InternLM-XComposer2- 4khd的模型架构主要遵循InternLM-XComposer2[27]的设计(为了简单，XComposer2在以下)，包括一个轻量级视觉编码器OpenAI ViT-Large&#x2F;14、大型语言模型InternLM2-7B和部分LoRA以实现高效对齐。我们建议读者阅读XComposer2论文以了解更多细节。</p>\n<h3 id=\"高分辨率输入\"><a href=\"#高分辨率输入\" class=\"headerlink\" title=\"高分辨率输入\"></a>高分辨率输入</h3><p>动态图像分割。利用静态输入图像大小来处理高分辨率图像，特别是具有不同纵横比的图像，既不高效也不有效。为了克服这个限制，我们引入了一种动态图像分割方法，如图4所示。该方法战略性地将图像分割为更小的块，同时保持原始图像的长宽比的完整性。</p>\n<p><img src=\"/2024/04/17/InternLM-XComposer2-4KHD-md/xcomposer-1.png\"></p>\n<p>给定最大分区数$H$，大小为$[H, w]$的图像$x$被调整大小并填充为大小为$[ph × 336, pw × 336]$的新图像$\\hat x$。这个过程受以下约束：<br>$$<br>p_w \\times p_h \\le H;p_h&#x3D;[pw \\times h&#x2F;w]<br>$$</p>\n<p>这里$p_w$和$p_h$分别表示每一行和每列中的patch数量。然后，我们将$\\hat x$分割为$p_h \\times p_w$非重叠patches。每个patch是一个$336 \\times 336$大小的小图像，我们将这些patch视为ViT的单个输入。</p>\n<p>接下来，我们使用“HD-H” 来表示H patches约束下的高分辨率设置。例如，<code>HD-9</code>允许多达9个patches，包括一系列的分辨率，如1008×1008, 672×1344, 336×3024等。</p>\n<p><strong>全局-局部格式</strong>。对于每个输入图像，我们将其以两个视图呈现给模型。第一个是全局视图，其中图像被调整为固定大小(在我们的例子中是336 × 336)。这提供了对图像的宏观理解。根据经验，我们发现这对于LVLM正确理解图像至关重要。第二个视图是局部视图。我们使用前面提到的动态图像划分策略将图像划分为图像块，并从每个图像块中提取特征。特征提取后，这些块被重新组装成一个大的特征图。经过简单的token合并过程后，特征映射被平展为最终的局部特征。</p>\n<p><strong>图像2D结构换行指示器。</strong>假设图像具有2D结构，并且图像比例是动态的，那么每行的token数量可以在不同的图像中不同。这种变化可能会混淆LVLM，使其难以确定哪些token属于图像的同一行，哪些标记属于下一行。这种混淆可能会阻碍LVLM理解图像2D结构的能力，这对于理解文档、图表和表格等结构化图像内容至关重要。为解决这个问题，我们在展平之前在图像特征的每行末尾引入了一个可学习的换行符(“\\n”)标记。最后，我们将全局和局部视图连接起来，在它们之间插入一个特殊的“ separate ”标记来区分这两个视图。</p>\n<h3 id=\"预训练\"><a href=\"#预训练\" class=\"headerlink\" title=\"预训练\"></a>预训练</h3><p>在预训练阶段，LLM被冻结，而视觉编码器和部分LoRA都进行微调，以使视觉token与LLM对齐。预训练数据主要遵循XComposer2中的设计，该设计有三个目标：1)通用语义对齐，2)世界知识对齐，3)视觉能力增强。本文专注于高分辨率和结构化图像理解。因此，我们收集了更多的相关数据来增强这一特定能力。如表1所示，我们为此使用了不同的OCR数据集。</p>\n<p><img src=\"/2024/04/17/InternLM-XComposer2-4KHD-md/xcomposer-2.png\"></p>\n<p>在实践中，采用OpenAI CLIP ViT-L-14-336作为视觉编码器。与XComposer2不同，我们保持ViT分辨率为336 × 336，并通过更多的补丁增加输入分辨率。对于动态图像分割策略，我们使用“ HD-25” 作为修饰说明。对于每个图像或块，通过简单的合并操作将图像标记数减少到1&#x2F;4。我们通过通道维度将附近的4个token连接到一个新token中，然后通过MLP将其与LLM对齐。“ separate” 和“ \\n ”token是随机初始化的。对于部分LoRA，我们将LLM解码器块中所有线性层的秩设置为256。我们的训练过程涉及4096的批量大小，跨越两个epoch。在前1%的训练步骤内，学习率线性增加到2 × 10−4。之后，它根据余弦衰减策略减少到0。为了保留视觉编码器的现有知识，我们应用了分层学习率(LLDR)衰减策略，并将衰减因子设置为0.90。</p>\n<h3 id=\"4KHD监督微调\"><a href=\"#4KHD监督微调\" class=\"headerlink\" title=\"4KHD监督微调\"></a>4KHD监督微调</h3><p>在预训练后，赋予模型理解高分辨率图像和解决各种挑战的能力。不同于以往的感知任务(如VQAv2、GQA)，这些任务通常根据图像中明显的目标来回答问题。<strong>与OCR相关的任务依赖于对高分辨率图像中的文本的详细理解。</strong>例如，在InfoVQA中，50%的图像的长边长度超过2000像素。低分辨率的输入会扭曲密集的文本信息，导致模型无法理解。然而，我们观察到上述<strong>感知任务存在分辨率饱和问题</strong>，其中分辨率的影响变得可以忽略不计。</p>\n<p>为解决这个问题，本文提出一种混合分辨率训练策略，以提高训练效率。对于需要高分辨率的任务，我们在训练期间采用“ HD-55 ”设置。这允许输入4K (3840 × 1600)图像，而不需要额外的图像压缩。这些任务称为表2中的HD-OCR QA任务。对于其他任务，我们实现了动态解析策略。图像会被调整到原始大小和“HD25 ”设置指定的大小之间的范围内。这种动态方法增强了LVLM对输入分辨率差异的鲁棒性，从而使LVLM在推理过程中利用更大的分辨率。例如，我们观察到，当LVLM在“ HD25 ”设置下训练时，使用“ HD30 ”设置在大多数与ocr相关的任务上产生了更好的结果。</p>\n<p><img src=\"/2024/04/17/InternLM-XComposer2-4KHD-md/xcomposer-3.png\"></p>\n<p>在实践中，我们以2048的批量大小联合训练所有组件超过在3500步。对来自多个源的数据进行加权采样，权重基于每个源的数据数量。由于“HD55” 设置比“HD-25”多两倍的图像token，我们调整数据加载器来为它们启用不同的批量大小，并相应地调整它们的权重。最大学习率设置为5 × 10−5，每个组件都有自己独特的学习策略。对于视觉编码器，我们将LLDR设置为0.9，这与预训练策略相一致。对于LLM，我们采用固定的学习率比例因子0.2。这减慢了LLM的更新，在保留其原始能力和将其与视觉知识对齐之间实现平衡。</p>\n<h2 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h2><p>在本节中，我们在监督微调后验证了我们的InternLM-XComposer2-4KHD(为简单起见，以下为IXC2-4KHD)的基准性能。</p>\n<h3 id=\"LVLM基准测试结果\"><a href=\"#LVLM基准测试结果\" class=\"headerlink\" title=\"LVLM基准测试结果\"></a>LVLM基准测试结果</h3><p>在表3和表4中，我们比较了IXC2- 4KHD与SOTA开源LVLMs和闭源api的基准列表。本文报告了DocVQA[70]、ChartQA[68]、InfographicVQA[69]、TextVQA[87]、OCRBench[58]、MMStar[15]、MathVista[61]、MMMU[107]、AI2D[42]、MME[31]、MMBench (MMB)[57]、MMBench- chinese (MMBCN)[57]、SEED-Bench Image Part (SEEDI)[45]、QBench-Testset (QBenchT)[98]、MM-Vet[105]、HallusionBench (HallB)[34]的结果。主要在OpenCompass VLMEvalKit[24]上对实验结果进行统一再现。</p>\n<p><img src=\"/2024/04/17/InternLM-XComposer2-4KHD-md/xcomposer-4.png\"></p>\n<p><img src=\"/2024/04/17/InternLM-XComposer2-4KHD-md/xcomposer-5.png\"></p>\n<p><strong>与闭源api的比较</strong>。如表3所示，IXC2-4KHD在各种基准测试中表现出具有竞争力的性能，可以与闭源api相媲美。由于其高分辨率输入，IXC2- 4KHD在DocVQA上取得了90.0%的分数，在ChartQA上取得了81.0%的分数，从而以不小的差距超过了GPT-4V和Gemini-Pro。在具有挑战性的InfographicVQA任务中，所提出模型是第一个接近闭源api性能的开源模型，性能超过之前的开源模型近20%。除了与ocr相关的任务外，IXC2-4KHD是一个通用的大型视觉-语言模态，在语义级任务中表现出色，显示出有竞争力的结果。</p>\n<p><strong>与开源模型的比较。</strong>在相似的模型规模下，与开源LVLMs进行了全面比较。如表4所示，我们的模型显著优于现有的开源模型，在所有基准上都取得了有竞争力的结果。值得注意的是，InternLM-XComposer2系列是唯一在具有挑战性的MMStar基准上取得高于50%分数的方法。</p>\n<p><strong>高分辨率理解评估。</strong>将IXC2-4KHD与专门为高分辨率理解任务设计的模型进行比较。我们在表5中报告了5个高分辨率基准的结果，作为一般的LVLM， IXC2-4KHD在这些任务上显示了卓越的性能，并以很大的优势超过了竞争对手。例如，IXC2-4KHD在InfographicVQA上的得分为68.6%，超过了最近的DocOwl 1.5，提高了17.9%。在OCRBench测试平台上，IXC2-4KHD获得了67.5%的性能增益，比CogAgent高出8.5%。</p>\n<p><img src=\"/2024/04/17/InternLM-XComposer2-4KHD-md/xcomposer-6.png\"></p>\n<h3 id=\"深入研究分辨率\"><a href=\"#深入研究分辨率\" class=\"headerlink\" title=\"深入研究分辨率\"></a>深入研究分辨率</h3><p><strong>高分辨率训练对于HD-OCR任务至关重要。</strong>我们研究了四种分辨率设置：HD-9(最多1561个图像标记，我们只是简单地声明一下)、HD16(2653个标记)、HD-25(4057个标记)和4KHD(8737个标记)。在这里，我们报告了InfoVQA、DocVQA和TextVQA的验证集，ChartQA和AI2D的测试集，MMBench EN-Test，以及SEEDBench的2k子集(我们称它为SEED∗)。在接下来的实验中，我们默认报告上述基准的结果。</p>\n<p>如图5所示，我们注意到随着分辨率的增加，HD-OCR任务的显著改善。例如，该模型在HD-9设置下的InfographicVQA上只获得了50.5%的分数。然而，当我们切换到HD-16设置时，我们观察到性能提高了10.2%。随着分辨率的增加，性能继续提高，即使在4KHD设置下也没有观察到饱和度。由于计算限制，将改进上限的探索推迟到未来的工作。在其他与ocr相关的任务中，提高分辨率所带来的性能增益相对较小。对于与感知相关的基准，性能在四种设置之间的差异可以忽略不计的分辨率上达到饱和。</p>\n<p><img src=\"/2024/04/17/InternLM-XComposer2-4KHD-md/xcomposer-7.png\"></p>\n<p><strong>更高的推理分辨率可以在与文本相关的任务上取得更好的结果。</strong>从我们的实验中得到的一个有趣的观察是，我们的模型在以稍高的分辨率进行推断时，往往会在与文本相关的任务上产生改进的结果。我们将HD-9、HD-16、HD-25的结果列于表6。例如，IXC2-HD9在InfographicVQA上获得了50.5%的分数。当我们使用HD16进行推断时，我们看到性能提高了8.1%，而无需额外的训练。在IXC2-HD16和IXC2-HD25上也观察到了类似的改进。假设训练中使用的动态图像标记长度增强了LVLM的鲁棒性，当图像中的文本在更高分辨率的输入中更”清晰”时，会得到更好的结果。相反，在这种设置下，ChartQA的结果会持续下降。这可能是由于当分辨率改变时，模型对图表结构感到困惑。此外，与图5的观察结果类似，分辨率对感知相关基准的影响似乎非常小。</p>\n<p><img src=\"/2024/04/17/InternLM-XComposer2-4KHD-md/xcomposer-8.png\"></p>\n<p><strong>可视化的结果。</strong>我们提供了图2和图3所示的超高清图像可视化结果。请参阅附录以获取更多结果。</p>\n<p><img src=\"/2024/04/17/InternLM-XComposer2-4KHD-md/xcomposer-9.png\"></p>\n<h3 id=\"高分辨率策略消融实验\"><a href=\"#高分辨率策略消融实验\" class=\"headerlink\" title=\"高分辨率策略消融实验\"></a>高分辨率策略消融实验</h3><p><strong>全局视图的作用。</strong>我们首先考察Global-Local格式中的Global视图的影响。如表7所示，我们发现全局视图对于LVLM准确理解输入图像至关重要。当它被移除时，模型在所有基准上的表现都更差。例如，在没有全局视图的情况下，模型在MMBench测试中的性能下降了−4.4%。全局视图提供了对图像的一般宏观理解，而模型很难从局部视图中的大量token中推导出来。</p>\n<p><img src=\"/2024/04/17/InternLM-XComposer2-4KHD-md/xcomposer-10.png\"></p>\n<p><strong>换行符的作用。</strong>在展开操作之前，我们在图像特征的每行末尾合并一个特殊的换行符。这个token是图像2D结构的指示器。我们在表8中检查了它对HD-9和4KHD策略的影响。当采用固定的高分辨率策略HD-9时，从换行令牌获得的好处很小。这可以归因于LVLM在训练后处理图像比率有限差异的能力。然而，当实现更具挑战性的4KHD (HD-25 + HD-55)策略时，LVLM在没有换行指示符的情况下，在图像比例和token数量方面表现出显著的多样性，在与ocr相关的任务上表现出显著的性能下降。这一发现支持了我们的假设，即当图像标记直接展平为一维序列时，LVLM很难理解图像的形状。换行符可以帮助模型更好地理解图像的结构。</p>\n<p><img src=\"/2024/04/17/InternLM-XComposer2-4KHD-md/xcomposer-11.png\"></p>\n<p><strong>令牌合并策略的影响。</strong>在实践中，我们采用一种简单的合并策略，沿着通道维度连接四个相邻的令牌。我们发现这种方法在有效减少图像token的数量方面是有效的。本文研究了4KHD设置下不同token合并策略的影响。在表9中，我们研究了两个额外的策略：ReSampler[5]和C-Abstrator[12]，它们的默认设置和相同的压缩率0.25，即将具有576个标记的图像减少到144个标记。结果表明，concatenation和C-Abstractor都工作得很好，在大多数基准上得到了相似的结果，这一观察结果也与MM-1[71]中的研究一致，即连接器的影响较小。然而，重采样器的性能比其他方法差，有明显的间隔。这是由于用于收集信息的可学习查询需要大量数据进行训练造成的，预训练数据有点轻量，可以完全收敛。</p>\n<p><img src=\"/2024/04/17/InternLM-XComposer2-4KHD-md/xcomposer-12.png\"></p>\n<h2 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h2><p>本文提出InternLM-Xcomposer2- 4KHD，在与OCR相关的任务上超过了之前的开源模型的性能，并在通用LVLM基准上取得了有竞争力的结果。由于我们的动态分辨率和自动补丁配置，我们的模型支持最高可达4K高清的训练分辨率。集成了一个全局视图块来支持宏观理解，并集成了一个可学习的换行符来处理各种输入图像分辨率。随着HD-OCR任务的训练分辨率增加，模型的性能不断提高。值得注意的是，即使对于4KHD设置，也没有观察到任何性能饱和，而且由于更高分辨率输入的计算负担增加，也没有探索上限。在未来的工作中，我们计划探索精确LVLM训练和推理的有效解决方案，使我们的模型在保持计算效率的同时处理更高的分辨率。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">instances: [&#123;&#x27;samples&#x27;: &#123;&#x27;data_type&#x27;: &#x27;multi&#x27;,</span><br><span class=\"line\">                             &#x27;image&#x27;: tensor([[[[-0.9748, -0.9310, -0.9602,  ..., -0.9456, -0.9602, -0.9164],</span><br><span class=\"line\">                         [-0.9893, -0.9456, -0.9456,  ..., -0.8580, -0.9310, -0.9456],</span><br><span class=\"line\">                         [-0.9602, -0.9748, -0.9748,  ..., -0.9018, -0.9310, -0.9310],</span><br><span class=\"line\">                         ...,</span><br><span class=\"line\">                         [-0.7412, -0.7704, -0.7412,  ..., -0.7120, -0.7266, -0.7266],</span><br><span class=\"line\">                         [-0.7120, -0.7704, -0.7704,  ..., -0.6828, -0.7266, -0.6682],</span><br><span class=\"line\">                         [-0.7120, -0.7266, -0.7558,  ..., -0.6536, -0.6974, -0.6390]],</span><br><span class=\"line\">               </span><br><span class=\"line\">                        [[-0.9117, -0.8666, -0.8967,  ..., -0.8816, -0.8967, -0.8516],</span><br><span class=\"line\">                         [-0.9267, -0.8816, -0.8816,  ..., -0.7916, -0.8666, -0.8816],</span><br><span class=\"line\">                         [-0.8967, -0.9117, -0.9117,  ..., -0.8366, -0.8666, -0.8666],</span><br><span class=\"line\">                         ...,</span><br><span class=\"line\">                         [-0.6715, -0.7016, -0.6715,  ..., -0.6415, -0.6565, -0.6565],</span><br><span class=\"line\">                         [-0.6415, -0.7016, -0.7016,  ..., -0.6115, -0.6565, -0.5965],</span><br><span class=\"line\">                         [-0.6415, -0.6565, -0.6865,  ..., -0.5815, -0.6265, -0.5665]],</span><br><span class=\"line\">               </span><br><span class=\"line\">                        [[-0.6839, -0.6412, -0.6697,  ..., -0.6555, -0.6697, -0.6270],</span><br><span class=\"line\">                         [-0.6981, -0.6555, -0.6555,  ..., -0.5701, -0.6412, -0.6555],</span><br><span class=\"line\">                         [-0.6697, -0.6839, -0.6839,  ..., -0.6128, -0.6412, -0.6412],</span><br><span class=\"line\">                         ...,</span><br><span class=\"line\">                         [-0.4564, -0.4848, -0.4564,  ..., -0.4279, -0.4422, -0.4422],</span><br><span class=\"line\">                         [-0.4279, -0.4848, -0.4848,  ..., -0.3995, -0.4422, -0.3853],</span><br><span class=\"line\">                         [-0.4279, -0.4422, -0.4706,  ..., -0.3711, -0.4137, -0.3568]]],</span><br><span class=\"line\">               </span><br><span class=\"line\">               </span><br><span class=\"line\">                       [[[-0.9748, -0.9310, -0.9602,  ..., -0.9456, -0.9602, -0.9164],</span><br><span class=\"line\">                         [-0.9893, -0.9456, -0.9456,  ..., -0.8580, -0.9310, -0.9456],</span><br><span class=\"line\">                         [-0.9602, -0.9748, -0.9748,  ..., -0.9018, -0.9310, -0.9310],</span><br><span class=\"line\">                         ...,</span><br><span class=\"line\">                         [-0.7412, -0.7704, -0.7412,  ..., -0.7120, -0.7266, -0.7266],</span><br><span class=\"line\">                         [-0.7120, -0.7704, -0.7704,  ..., -0.6828, -0.7266, -0.6682],</span><br><span class=\"line\">                         [-0.7120, -0.7266, -0.7558,  ..., -0.6536, -0.6974, -0.6390]],</span><br><span class=\"line\">               </span><br><span class=\"line\">                        [[-0.9117, -0.8666, -0.8967,  ..., -0.8816, -0.8967, -0.8516],</span><br><span class=\"line\">                         [-0.9267, -0.8816, -0.8816,  ..., -0.7916, -0.8666, -0.8816],</span><br><span class=\"line\">                         [-0.8967, -0.9117, -0.9117,  ..., -0.8366, -0.8666, -0.8666],</span><br><span class=\"line\">                         ...,</span><br><span class=\"line\">                         [-0.6715, -0.7016, -0.6715,  ..., -0.6415, -0.6565, -0.6565],</span><br><span class=\"line\">                         [-0.6415, -0.7016, -0.7016,  ..., -0.6115, -0.6565, -0.5965],</span><br><span class=\"line\">                         [-0.6415, -0.6565, -0.6865,  ..., -0.5815, -0.6265, -0.5665]],</span><br><span class=\"line\">               </span><br><span class=\"line\">                        [[-0.6839, -0.6412, -0.6697,  ..., -0.6555, -0.6697, -0.6270],</span><br><span class=\"line\">                         [-0.6981, -0.6555, -0.6555,  ..., -0.5701, -0.6412, -0.6555],</span><br><span class=\"line\">                         [-0.6697, -0.6839, -0.6839,  ..., -0.6128, -0.6412, -0.6412],</span><br><span class=\"line\">                         ...,</span><br><span class=\"line\">                         [-0.4564, -0.4848, -0.4564,  ..., -0.4279, -0.4422, -0.4422],</span><br><span class=\"line\">                         [-0.4279, -0.4848, -0.4848,  ..., -0.3995, -0.4422, -0.3853],</span><br><span class=\"line\">                         [-0.4279, -0.4422, -0.4706,  ..., -0.3711, -0.4137, -0.3568]]]]),</span><br><span class=\"line\">                             &#x27;text_input&#x27;: [&#x27;[UNUSED_TOKEN_146]user</span><br><span class=\"line\">               &#x27;</span><br><span class=\"line\">                                            &#x27;&lt;ImageHere&gt;&lt;ImageHere&gt;图中是什么[UNUSED_TOKEN_145]</span><br><span class=\"line\">               &#x27;</span><br><span class=\"line\">                                            &#x27;[UNUSED_TOKEN_146]assistant</span><br><span class=\"line\">               &#x27;</span><br><span class=\"line\">                                            &#x27;这张图中包含了......[UNUSED_TOKEN_145]</span><br><span class=\"line\">               &#x27;</span><br><span class=\"line\">                                            &#x27;&lt;/s&gt;&#x27;]&#125;&#125;]</span><br></pre></td></tr></table></figure>\n\n<p>经过collator以后！！！！</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">instances: [&#123;&#x27;samples&#x27;: &#123;&#x27;data_type&#x27;: &#x27;multi&#x27;,</span><br><span class=\"line\">                            &#x27;image&#x27;: tensor([[[[-0.9748, -0.9310, -0.9602,  ..., -0.9456, -0.9602, -0.9164],</span><br><span class=\"line\">                        [-0.9893, -0.9456, -0.9456,  ..., -0.8580, -0.9310, -0.9456],</span><br><span class=\"line\">                        [-0.9602, -0.9748, -0.9748,  ..., -0.9018, -0.9310, -0.9310],</span><br><span class=\"line\">                        ...,</span><br><span class=\"line\">                        [-0.7412, -0.7704, -0.7412,  ..., -0.7120, -0.7266, -0.7266],</span><br><span class=\"line\">                        [-0.7120, -0.7704, -0.7704,  ..., -0.6828, -0.7266, -0.6682],</span><br><span class=\"line\">                        [-0.7120, -0.7266, -0.7558,  ..., -0.6536, -0.6974, -0.6390]],</span><br><span class=\"line\">              </span><br><span class=\"line\">                       [[-0.9117, -0.8666, -0.8967,  ..., -0.8816, -0.8967, -0.8516],</span><br><span class=\"line\">                        [-0.9267, -0.8816, -0.8816,  ..., -0.7916, -0.8666, -0.8816],</span><br><span class=\"line\">                        [-0.8967, -0.9117, -0.9117,  ..., -0.8366, -0.8666, -0.8666],</span><br><span class=\"line\">                        ...,</span><br><span class=\"line\">                        [-0.6715, -0.7016, -0.6715,  ..., -0.6415, -0.6565, -0.6565],</span><br><span class=\"line\">                        [-0.6415, -0.7016, -0.7016,  ..., -0.6115, -0.6565, -0.5965],</span><br><span class=\"line\">                        [-0.6415, -0.6565, -0.6865,  ..., -0.5815, -0.6265, -0.5665]],</span><br><span class=\"line\">              </span><br><span class=\"line\">                       [[-0.6839, -0.6412, -0.6697,  ..., -0.6555, -0.6697, -0.6270],</span><br><span class=\"line\">                        [-0.6981, -0.6555, -0.6555,  ..., -0.5701, -0.6412, -0.6555],</span><br><span class=\"line\">                        [-0.6697, -0.6839, -0.6839,  ..., -0.6128, -0.6412, -0.6412],</span><br><span class=\"line\">                        ...,</span><br><span class=\"line\">                        [-0.4564, -0.4848, -0.4564,  ..., -0.4279, -0.4422, -0.4422],</span><br><span class=\"line\">                        [-0.4279, -0.4848, -0.4848,  ..., -0.3995, -0.4422, -0.3853],</span><br><span class=\"line\">                        [-0.4279, -0.4422, -0.4706,  ..., -0.3711, -0.4137, -0.3568]]],</span><br><span class=\"line\">              </span><br><span class=\"line\">              </span><br><span class=\"line\">                      [[[-0.9748, -0.9310, -0.9602,  ..., -0.9456, -0.9602, -0.9164],</span><br><span class=\"line\">                        [-0.9893, -0.9456, -0.9456,  ..., -0.8580, -0.9310, -0.9456],</span><br><span class=\"line\">                        [-0.9602, -0.9748, -0.9748,  ..., -0.9018, -0.9310, -0.9310],</span><br><span class=\"line\">                        ...,</span><br><span class=\"line\">                        [-0.7412, -0.7704, -0.7412,  ..., -0.7120, -0.7266, -0.7266],</span><br><span class=\"line\">                        [-0.7120, -0.7704, -0.7704,  ..., -0.6828, -0.7266, -0.6682],</span><br><span class=\"line\">                        [-0.7120, -0.7266, -0.7558,  ..., -0.6536, -0.6974, -0.6390]],</span><br><span class=\"line\">              </span><br><span class=\"line\">                       [[-0.9117, -0.8666, -0.8967,  ..., -0.8816, -0.8967, -0.8516],</span><br><span class=\"line\">                        [-0.9267, -0.8816, -0.8816,  ..., -0.7916, -0.8666, -0.8816],</span><br><span class=\"line\">                        [-0.8967, -0.9117, -0.9117,  ..., -0.8366, -0.8666, -0.8666],</span><br><span class=\"line\">                        ...,</span><br><span class=\"line\">                        [-0.6715, -0.7016, -0.6715,  ..., -0.6415, -0.6565, -0.6565],</span><br><span class=\"line\">                        [-0.6415, -0.7016, -0.7016,  ..., -0.6115, -0.6565, -0.5965],</span><br><span class=\"line\">                        [-0.6415, -0.6565, -0.6865,  ..., -0.5815, -0.6265, -0.5665]],</span><br><span class=\"line\">              </span><br><span class=\"line\">                       [[-0.6839, -0.6412, -0.6697,  ..., -0.6555, -0.6697, -0.6270],</span><br><span class=\"line\">                        [-0.6981, -0.6555, -0.6555,  ..., -0.5701, -0.6412, -0.6555],</span><br><span class=\"line\">                        [-0.6697, -0.6839, -0.6839,  ..., -0.6128, -0.6412, -0.6412],</span><br><span class=\"line\">                        ...,</span><br><span class=\"line\">                        [-0.4564, -0.4848, -0.4564,  ..., -0.4279, -0.4422, -0.4422],</span><br><span class=\"line\">                        [-0.4279, -0.4848, -0.4848,  ..., -0.3995, -0.4422, -0.3853],</span><br><span class=\"line\">                        [-0.4279, -0.4422, -0.4706,  ..., -0.3711, -0.4137, -0.3568]]]]),</span><br><span class=\"line\">                            &#x27;text_input&#x27;: [&#x27;[UNUSED_TOKEN_146]user</span><br><span class=\"line\">              &#x27;</span><br><span class=\"line\">                                           &#x27;&lt;ImageHere&gt;&lt;ImageHere&gt;图中是什么[UNUSED_TOKEN_145]</span><br><span class=\"line\">              &#x27;</span><br><span class=\"line\">                                           &#x27;[UNUSED_TOKEN_146]assistant</span><br><span class=\"line\">              &#x27;</span><br><span class=\"line\">                                           &#x27;这张图中包含了......[UNUSED_TOKEN_145]</span><br><span class=\"line\">              &#x27;</span><br><span class=\"line\">                                           &#x27;&lt;/s&gt;&#x27;]&#125;&#125;]</span><br></pre></td></tr></table></figure>\n\n\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">img2emb</span>(<span class=\"params\">self, image</span>):</span><br><span class=\"line\">        img_embeds = self.vision_proj(self.vit(image.to(self.device)))</span><br><span class=\"line\">        ic(img_embeds.shape) <span class=\"comment\">#   img_embeds.shape: torch.Size([2, 1225, 2048])</span></span><br><span class=\"line\">        atts_img = torch.ones( </span><br><span class=\"line\">            img_embeds.size()[:-<span class=\"number\">1</span>], dtype=torch.long).to(img_embeds.device)</span><br><span class=\"line\">        ic(atts_img.shape) <span class=\"comment\">#    atts_img.shape: torch.Size([2, 1225])</span></span><br><span class=\"line\">        img_target = torch.ones(</span><br><span class=\"line\">            img_embeds.size()[:<span class=\"number\">2</span>], dtype=torch.long).to(</span><br><span class=\"line\">                img_embeds.device) * -<span class=\"number\">100</span></span><br><span class=\"line\">\t\t<span class=\"comment\">#  img_target.shape: torch.Size([2, 1225])</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> img_embeds, atts_img, img_target</span><br></pre></td></tr></table></figure>\n\n"},{"title":"Z-Image深度解析：60亿参数模型如何以效率挑战巨头Nano Banana Pro","url":"/2025/12/08/Z-Image/","content":"<p><img src=\"/2025/12/08/Z-Image/1.PNG\"></p>\n<blockquote>\n<p>打破“唯规模论”，Z-Image的高效之道</p>\n</blockquote>\n<h2 id=\"唯规模论的挑战\"><a href=\"#唯规模论的挑战\" class=\"headerlink\" title=\"唯规模论的挑战\"></a>唯规模论的挑战</h2><p>在当前高性能文生图（Text-to-Image）领域，一场关于模型规模的“军备竞赛”正愈演愈烈。无论是顶级的闭源商业模型，还是如Qwen-Image（20B）、FLUX.2（32B）和Hunyuan-Image-3.0（80B）等领先的开源项目，其发展路径似乎都遵循着一条不成文的法则——“唯规模论”。然而，性能的提升往往伴随着高昂得令人望而却步的训练与推理成本，这已成为阻碍技术普及与创新的巨大障碍。</p>\n<p>与此同时，业界浮现出另一条看似高效的捷径：依赖从强大的闭源模型中蒸馏出的合成数据进行训练。尽管这种方法能在资源有限的情况下快速提升性能，但它也带来了潜在的风险——形成一个封闭的反馈循环，可能导致错误累积和数据同质化，从而扼杀了超越教师模型范畴的创新能力。</p>\n<p>在这一背景下，<strong>Z-Image</strong>应运而生。它不仅挑战了“不惜一切代价扩大规模”的范式，更摒弃了对合成数据蒸馏的依赖。Z-Image的核心论点清晰而有力：通过对<strong>数据</strong>、<strong>架构</strong>、<strong>训练</strong>和<strong>推理</strong>全生命周期的系统性优化，完全可以用更精简的资源，在纯粹的真实世界数据上，实现与巨头相媲美甚至超越的顶尖性能。</p>\n<p>Z-Image的成就，通过一系列惊人的数据得以体现：</p>\n<p>• <strong>高效的成本控制</strong>：仅用 <strong>31.4万 H800 GPU小时</strong>（约合63万美元）便完成了从零到一的完整训练，将顶尖模型的训练成本拉至前所未有的亲民水平。</p>\n<p>• <strong>精简的模型规模</strong>：模型参数仅为 <strong>60亿</strong>，远小于动辄数百亿的竞争对手，为在消费级硬件上的部署与微调打开了大门。</p>\n<p>• <strong>卓越的性能表现</strong>：在写实图像生成和极具挑战性的双语（中&#x2F;英）文本渲染方面，其性能媲美甚至超越了部分顶级的商业闭源模型。</p>\n<p>• <strong>开放的社区贡献</strong>：为推动技术民主化，Z-Image的模型代码、权重和在线演示均已公开发布，赋能更广泛的开发者与研究者。</p>\n<p>本文将深入剖析Z-Image实现这一系列突破背后的四大技术支柱，为读者系统性地揭示其如何在资源效率与模型性能之间找到最优解，为高效能AIGC（AI-Generated Content）的发展开辟一条全新的路径。</p>\n<h2 id=\"Z-Image理念：系统性优化的四大支柱\"><a href=\"#Z-Image理念：系统性优化的四大支柱\" class=\"headerlink\" title=\"Z-Image理念：系统性优化的四大支柱\"></a>Z-Image理念：系统性优化的四大支柱</h2><ol>\n<li><p><strong>高效的数据基础设施</strong> 这不仅是对数据进行简单的清洗与筛选，而是构建了一个动态的、以“最大化单位时间知识获取率”为目标的智能数据引擎。它确保每一单位的计算资源都能从数据中汲取最大化的信息增益，为模型的性能上限奠定了坚实基础。</p>\n</li>\n<li><p><strong>高效的S3-DiT架构</strong> 借鉴大语言模型（LLM）的成功经验，Z-Image采用了一种创新的单流（Single-Stream）扩散变换器架构。这种设计实现了极致的参数效率，并将文本、图像生成与图像编辑等多模态任务在统一框架下无缝集成，显著提升了模型的通用性与可扩展性。</p>\n</li>\n<li><p><strong>高效的训练策略</strong> Z-Image的训练过程被设计成一套循序渐进的“智能课程”。它通过分阶段、有重点地引导模型从掌握基础知识到精通高级技能，再到对齐人类偏好，系统性地构建模型能力，从而避免了在无效或低效的学习路径上浪费宝贵的计算资源。</p>\n</li>\n<li><p><strong>高效的推理方案</strong> 通过引入先进的知识蒸馏技术，Z-Image衍生出Turbo版本。该方案在几乎不牺牲生成质量的前提下，将推理速度提升了十倍以上，使其能够在企业级GPU上实现亚秒级响应，并成功适配内存小于16GB的消费级硬件。</p>\n</li>\n</ol>\n<p>接下来，对这四大支柱逐一进行深度解析，揭示Z-Image高效能背后的技术秘诀。</p>\n<h3 id=\"（1）数据为先-——-构建智能高效的数据基础设施\"><a href=\"#（1）数据为先-——-构建智能高效的数据基础设施\" class=\"headerlink\" title=\"（1）数据为先 —— 构建智能高效的数据基础设施\"></a>（1）数据为先 —— 构建智能高效的数据基础设施</h3><p>在计算资源受限的前提下，<strong>数据策略必须从追求“数量”转向追求“效率”</strong>。Z-Image的数据基础设施并非一个静态的数据集，而是一个旨在<strong>最大化每单位算力信息增益</strong>的动态引擎，它直接决定了模型能力的上限。该基础设施由四大协同模块构成，共同确保了训练数据在概念上广博而不冗余，在质量上精纯且对齐。</p>\n<p><strong>数据分析引擎 (Data Profiling Engine)</strong> 。该引擎是数据策略的量化基石，它从多个维度为海量原始数据建立档案。技术层面，它评估图像的物理属性，如<strong>感知哈希（pHash）</strong>、清晰度、<strong>压缩伪影</strong>、<strong>色偏</strong>和<strong>模糊度</strong>；审美层面，它利用经过专业标注训练的模型为图像的视觉吸引力打分；语义层面，它通过专门的视觉语言模型（VLM）为图像生成丰富的语义标签。这些多维度的元信息，使得构建程序化的、分阶段的训练课程成为可能，确保在模型发展的“正确阶段”使用“正确的数据”。</p>\n<p> <strong>跨模态向量引擎 (Cross-modal Vector Engine)</strong> 。该引擎是保障数据多样性与非冗余性的核心。它通过大规模语义去重，有效清除了概念上重复的数据。为解决主流方法中<code>range_search</code>函数的严重可扩展性瓶颈，Z-Image团队创新性地将其替换为基于GPU加速流水线的<strong>高效k近邻（k-NN）搜索函数</strong>。这一工程上的巨大突破，使得处理速度达到了惊人的<strong>“在8块H800上每10亿条目仅需8小时”</strong>，极大地提升了去重效率。此外，其强大的跨模态检索能力成为诊断和修复模型缺陷的关键工具，能够快速定位并剔除导致问题的低质量数据簇，同时主动补充数据集中缺失的概念。</p>\n<p> <strong>世界知识拓扑图 (World Knowledge Topological Graph)</strong> 。该知识图谱如同一座“语义罗盘”，为数据策展提供了结构化的指引。它通过层级化的方式组织世界知识，确保了数据在概念覆盖上的广度。在实际应用中，它可以识别出数据池中代表性不足的概念（例如，特定文化符号或稀有物体），并指导数据引擎进行靶向补充。此外，在训练过程中，它还支持精细化的“概念平衡采样”，通过动态调整不同概念数据的采样权重，确保模型能够均衡地学习，避免偏向常见概念。</p>\n<p><strong>主动主动数据治理引擎(Active Curation Engine)</strong> 这个闭环系统将数据基础设施从一个被动的数据源转变为一个自我完善的动态系统。它通过“人机协同”（Human-in-the-Loop）的主动学习循环，持续提升数据质量。系统首先自动采样模型表现不佳或缺乏知识的案例，随后这些案例进入一个由AI和人类专家共同参与的标注与审核流程。高质量的标注数据不仅被用于优化数据策展模型本身，也反哺了知识图谱，形成了一个不断发现问题、解决问题并自我提升的良性循环，尤其在解决长尾分布问题上发挥了关键作用。</p>\n<p>一个强大的数据基础设施不仅为文生图模型提供了高质量的“燃料”，更为其衍生能力（如图像编辑）的构建奠定了坚实基础。正是基于此，Z-Image才得以高效地拓展其功能边界。</p>\n<h3 id=\"（2）精简强大-——-可扩展的单流S3-DiT架构\"><a href=\"#（2）精简强大-——-可扩展的单流S3-DiT架构\" class=\"headerlink\" title=\"（2）精简强大 —— 可扩展的单流S3-DiT架构\"></a>（2）精简强大 —— 可扩展的单流S3-DiT架构</h3><p><img src=\"/2025/12/08/Z-Image/2.PNG\"></p>\n<p>Z-Image架构设计的核心目标是效率与稳定性。为了实现这一目标，它借鉴了大语言模型中“decoder-only”架构的卓越可扩展性，选择了一条与主流双流（dual-stream）架构截然不同的技术路径，即<strong>可扩展的单流扩散变换器（Scalable Single-Stream Diffusion Transformer, S3-DiT）</strong>。</p>\n<p>S3-DiT架构的核心特点可以总结如下：</p>\n<p>• <strong>统一的单流设计</strong> 。该架构的最大创新在于将来自不同模态的tokens——包括文本、图像VAE编码和图像语义——在序列层面直接拼接，形成一个统一的输入流。与分别处理文本和图像的双流架构相比，这种“早期融合”的设计极大地促进了不同模态信息在模型每一层的深度交互与融合，从而显著提升了参数的利用效率。</p>\n<p>• <strong>核心组件选择</strong>。 为了在性能与效率间取得平衡，模型选用了一系列轻量级但性能卓越的组件。文本编码器采用了仅40亿参数且具备强大双语能力的<code>Qwen3-4B</code>，而图像词元化则依赖于以高质量重建著称的<code>Flux VAE</code>。这些精挑细选的组件共同服务于整体架构的高效与高质量目标。</p>\n<p>• <strong>训练稳定性保障</strong> 。为了确保60亿参数规模的模型在长时间的训练过程中保持稳定，S3-DiT集成了一系列先进的归一化技术。例如，通过引入<code>QK-Norm</code>和<code>Sandwich-Norm</code>，有效调节了注意力模块内部的激活值和信号幅度，从而避免了训练过程中可能出现的梯度爆炸或消失问题。</p>\n<p>• <strong>对编辑任务的天然扩展性</strong>。 S3-DiT的统一序列处理方式使其天然具备处理复杂多模态任务的能力。在处理图像编辑等任务时，架构通过一种精巧的机制来区分源图像和目标图像：<strong>为它们的tokens分配对齐的空间RoPE坐标，但在时间维度上通过一个单位间隔偏移来区分。此外，模型还应用了不同的时间条件值</strong>，以区分干净的参考图像和带噪声的目标图像。这种设计无需修改核心架构，展现了极强的灵活性和可扩展性。</p>\n<h3 id=\"（3）智能训练-——-从基础到卓越的渐进式课程\"><a href=\"#（3）智能训练-——-从基础到卓越的渐进式课程\" class=\"headerlink\" title=\"（3）智能训练 —— 从基础到卓越的渐进式课程\"></a>（3）智能训练 —— 从基础到卓越的渐进式课程</h3><p><img src=\"/2025/12/08/Z-Image/4.PNG\"></p>\n<p>Z-Image的训练过程并非简单的“数据投喂”，而是一个精心设计的、分阶段的“课程”，旨在系统性地构建和优化模型能力。整个流程好比一个人的成长与教育过程：从通识教育打下基础，到专业深造拓展能力，再到步入社会实践以对齐现实需求。</p>\n<ol>\n<li><strong>阶段一：低分辨率预训练 (Low-resolution Pre-training)</strong></li>\n</ol>\n<p> <strong>目标</strong>：此阶段是模型的“义务教育”，在<code>256x256</code>的固定低分辨率下进行。其核心目标是高效地建立基础的视觉-语义对齐能力和图像合成知识，为后续更高阶的学习打下坚实的基础。由于分辨率较低，计算成本也相对可控，使得模型能在此阶段用有限的资源学习到最广泛的概念。</p>\n<ol start=\"2\">\n<li><strong>阶段二：全能预训练 (Omni-pre-training)</strong></li>\n</ol>\n<p> <strong>目标：进入“大学深造”阶段，模型开始接触更复杂的任务。</strong></p>\n<p>这里的“全能 (Omni)”体现在三个方面：首先是<strong>任意分辨率训练</strong>，使模型具备处理不同尺寸和宽高比图像的能力；其次是<strong>文生图与图生图联合训练</strong>，将图像编辑能力与生成能力一并培养；最后是<strong>多层次双语文本描述训练</strong>，确保模型深刻理解不同粒度的文本指令。这种多任务联合训练的策略，通过“任务摊销”的方式，极大地提升了预训练预算的利用效率。</p>\n<ol start=\"3\">\n<li><strong>阶段三：监督微调 (SFT)</strong></li>\n</ol>\n<p><strong>目标：此阶段好比“职业技能培训”，其核心目标从“最大化生成多样性”转向“最大化生成质量”，即将模型的生成分布收窄至一个专注、高保真的子流形上。</strong></p>\n<p>为实现此目标，训练数据以经过严格筛选的高质量图文对为主。同时，为防止在追求高质量时遗忘掉不常见的概念，该阶段特别采用了“<strong>概念平衡</strong>”采样策略，动态调整训练数据中长尾概念的权重，有效避免了<strong>灾难性遗忘</strong>。</p>\n<ol start=\"4\">\n<li><strong>阶段四：人类反馈强化学习 (RLHF)</strong></li>\n</ol>\n<p><strong>目标：这是模型“步入社会”的最后一步，旨在弥合模型生成结果与真实人类偏好之间的细微差距。</strong></p>\n<p><strong>Z-Image采用了一种精妙的两步策略：首先，使用</strong>直接偏好优化 (DPO)** 进行离线对齐，重点解决如<strong>文本渲染、对象计数</strong>等具有<strong>客观</strong>评判标准的问题；随后，采用<strong>组相对策略优化 (GRPO)</strong> 进行在线微调，进一步优化<strong>审美、照片真实感</strong>等更<strong>主观</strong>的质量维度。</p>\n<p>从训练过程的中间结果可以直观地看到，模型在每个阶段都取得了显著的进步。预训练阶段建立了基本的图像结构和语义关联，SFT阶段大幅提升了图像的精细度和美学质量，而RLHF阶段则进一步增强了图像的真实感和对复杂指令的遵循能力。</p>\n<p>经过这套多阶段的精心训练，Z-Image已具备强大的生成能力。然而，为了满足实际应用中对实时性的严苛要求，还必须攻克最后一个难题——推理效率。</p>\n<h3 id=\"（4）极速推理-——-Z-Image-Turbo的诞生\"><a href=\"#（4）极速推理-——-Z-Image-Turbo的诞生\" class=\"headerlink\" title=\"（4）极速推理 —— Z-Image-Turbo的诞生\"></a>（4）极速推理 —— Z-Image-Turbo的诞生</h3><p>尽管基础模型已足够强大，但其生成一张高质量图像通常需要约100次函数评估（Number of Function Evaluations，NFE），这对于追求实时交互的应用场景而言是无法接受的。因此，如何在不牺牲甚至提升视觉质量的前提下，将推理步骤从近百步大幅压缩至个位数，成为Z-Image走向实用化的关键。Z-Image-Turbo的诞生，正是为了解决这一核心矛盾。</p>\n<p>Z-Image-Turbo的高效推理能力主要源于两项在知识蒸馏领域的关键技术创新：</p>\n<p>• <strong>解耦分布匹配蒸馏 (Decoupled DMD)</strong> 。传统的分布匹配蒸馏（DMD）方法在实践中常常导致生成图像的细节丢失和色彩偏移。Z-Image团队深入研究后洞察到，DMD的成功实际上源于两种可分离的机制：作为主要驱动力的<strong>“CFG增强（CFG-Augmentation, CA）”</strong>负责提升生成能力，而作为稳定器的<strong>“分布匹配（Distribution Matching, DM）”</strong>则确保训练过程稳定。基于此，他们提出了Decoupled DMD技术，通过解耦并分别为这两个机制设计最优的训练策略，从根本上解决了传统DMD方法中常见的细节模糊和色彩失真问题。</p>\n<p>• <strong>结合强化学习的DMDR</strong>。 为了进一步提升模型的审美和对人类偏好的对齐度，团队巧妙地将强化学习（RL）与蒸馏过程相结合，提出了DMDR（Distribution Matching Distillation meets Reinforcement Learning）框架。该方法的核心思想是，利用分布匹配（DM）项作为一个强大的<strong>“内在正则化器”</strong>，与强化学习的奖励目标协同优化。这使得模型在努力对齐人类偏好的同时，能够有效避免“刷分”（reward hacking）——即模型为了获得高分而生成视觉上不合理或扭曲的图像——从而确保了生成结果的稳定与高质量。</p>\n<p>一系列优化的效果是显著的。原始的SFT模型质量高但速度慢；标准的DMD虽然快，但出现了明显的细节模糊；Decoupled DMD成功恢复了细节和色彩保真度；而最终的Z-Image-Turbo，即D-DMD与DMDR的结合体，在仅需8步推理的情况下，其感知质量甚至超越了需要100步推理的教师模型。</p>\n<p>至此，Z-Image-Turbo成功实现了在企业级GPU上的亚秒级推理，并能兼容显存小于16GB的消费级硬件，完美解决了推理速度与视觉保真度之间的长期矛盾。</p>\n<p><img src=\"/2025/12/08/Z-Image/6.PNG\"></p>\n<h2 id=\"性能评估：Z-Image在公开基准上的表现\"><a href=\"#性能评估：Z-Image在公开基准上的表现\" class=\"headerlink\" title=\"性能评估：Z-Image在公开基准上的表现\"></a>性能评估：Z-Image在公开基准上的表现</h2><h3 id=\"人类偏好评估：AI-Arena的胜利\"><a href=\"#人类偏好评估：AI-Arena的胜利\" class=\"headerlink\" title=\"人类偏好评估：AI Arena的胜利\"></a>人类偏好评估：AI Arena的胜利</h3><p>在所有评估中，由大规模真人进行双盲投票的偏好评估最具说服力。在公开、独立的基准测试平台 <strong>AI Arena</strong> 上，Z-Image-Turbo与全球顶尖的开源及闭源模型进行了直接对决。结果显示，Z-Image-Turbo取得了 <strong>全球第四、开源模型第一</strong> 的优异成绩。其Elo分数（一种衡量相对技能水平的评级系统）不仅超越了所有其他开源模型，甚至超过了多个知名的闭源商业模型，充分证明了其生成结果获得了广泛的人类认可。</p>\n<p><img src=\"/2025/12/08/Z-Image/7.PNG\"></p>\n<h3 id=\"自动化基准测试：全面领先\"><a href=\"#自动化基准测试：全面领先\" class=\"headerlink\" title=\"自动化基准测试：全面领先\"></a>自动化基准测试：全面领先</h3><p>在一系列自动化评测基准上，Z-Image同样展现出全面的领先优势：</p>\n<p>• <strong>双语文本渲染</strong> 综合<code>CVTG-2K</code>和<code>LongText-Bench</code>两大权威文本渲染基准测试的结果，Z-Image在处理复杂的中英文长文本和多区域文本渲染任务上，准确率均达到了<strong>SOTA（State-of-the-Art）</strong> 水平。这一结果凸显了模型在文本渲染这一公认的生成模型高难度领域中的顶尖实力。</p>\n<p>• <strong>细粒度指令遵循</strong> 在<code>OneIG</code>和<code>TIIF</code>等专注于评估模型对复杂、多维度指令理解能力的基准上，Z-Image的表现同样出色。它在对齐度、推理、风格等多项指标上均名列前茅，证明了其能够精准地理解和执行包含多个约束条件的复杂用户提示。</p>\n<p>• <strong>图像编辑能力</strong> 根据<code>ImgEdit</code>和<code>GEdit</code>的评测数据，衍生模型Z-Image-Edit在物体添加&#x2F;移除、风格转换、背景替换等多种编辑任务上，均展现出与顶尖编辑模型相媲美的强大竞争力，特别是在双语指令编辑方面表现突出。</p>\n<p><img src=\"/2025/12/08/Z-Image/9.PNG\"></p>\n<h2 id=\"结论与展望：迈向高效能AIGC的新范式\"><a href=\"#结论与展望：迈向高效能AIGC的新范式\" class=\"headerlink\" title=\"结论与展望：迈向高效能AIGC的新范式\"></a>结论与展望：迈向高效能AIGC的新范式</h2><ul>\n<li><p>Z-Image系列模型的推出，为整个AIGC社区提供了一套完整的、可复现的<strong>“蓝图”</strong>。这套蓝图雄辩地证明了，在不依赖于海量参数和近乎无限计算资源的前提下，通过对数据、架构、训练和推理等环节进行系统性的端到端优化，同样可以打造出世界一流的生成模型。</p>\n</li>\n<li><p>Z-Image的成功，预示着AIGC领域可能正迎来一个新的发展范式。它有望推动行业从“唯规模论”的单一路径，转向更加注重效率和创新的多元化发展。这不仅将催生出更多易于访问、成本友好且性能卓越的生成模型，更将极大地促进技术的民主化，让更多中小型企业、研究机构和个人开发者能够参与到这场AI创新的浪潮中。</p>\n</li>\n<li><p>论文未提及训练数据集的规模，感觉至少在亿的规模吧。</p>\n</li>\n</ul>\n"},{"title":"r1","url":"/2025/02/09/r1/","content":""}]